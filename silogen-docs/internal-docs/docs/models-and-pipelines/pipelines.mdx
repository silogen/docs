---
title: Pipelines
description: How to configure and deploy pipelines in the SiloGen platform.
---

## Deploying pipelines

Pipelines are deployed through model-service, which uses the orchestrator internally. Pipelines are easiest to deploy with silodev, after which they are shown in the v1/models response list (if you have access to the pipeline):

```
silodev auth login
silodev models deploy --prefix '' "<<full pipeline name with organisation and version>>"
cd core/tooling
python inference models
# or directly with
# curl https://model-service.services.silogen.ai/v1/models -H "Authorization: Bearer $(silodev auth token)"
```

## Creating a new pipeline

A pipeline can created either with the command `model_catalog create pipeline`, the `./core/tooling/create-default-pipeline` helper script, or by editing an existing `status: ready` pipeline. See also the instructions under [Adding a new backend model](adding-a-new-backend-model.mdx) (steps 4-9) on how to test deployments.

- With `model_catalog create pipeline`
  - Follow `model_catalog create pipeline --help`
- With `./core/tooling/create-default-pipeline`
  - First set up the model catalog CLI
  - The script will find the latest main-tag for the `llm_pipeline` image automatically
- By editing an existing pipeline with `model_catalog manage pipeline `
  - Editing a pipeline that is already marked as `status: ready` will make a new version of that pipeline. If you edit the name or organization of that pipeline, it will be considered a fork of the original pipeline.

After creating a pipeline, edit and double-check with `model_catalog manage pipeline`. When you are satisfied that the pipeline works, mark it as `status: ready`. Optionally, also mark the pipeline as `stable: true`. This will allow prompting the pipeline without version information in its name. In the future, `stable: true` should only be reserved for pipelines that have been evaluated.

<Admonition type="tip" title="tip">
  There are both autoscaling and non-autoscaling inference models in the model
  catalog. To know which is which, look at the parameter
  `autoscale_retention_period` of the inference model. If it is set to `inf`,
  the model will not scale to zero.
</Admonition>

### Pipeline Parameters

RAG and sampling parameters can be set for the pipeline. It’s recommended to first experiment in the [Developer console](https://chat.dev.silogen.ai/console/playground/chat) to find well-performing parameters for the LLM system and use case in question.

#### RAG

- **certainty**
  - Default value: 0.0
  - Any value between 0 and 1 where 0 means RAG retrieval can match anything and 1 means RAG retrieval only matches verbatim statements. The higher the value, the less chunks will match when searching RAG. A good starting value is either 0.8 or 0.0. If the value is 0.0, the results will not be filtered by certainty. They are still in order by most certain first, returning top_k of the most relevant results.
- **alpha**
  - Default value: None
  - A number between 0 and 1 which signifies the balance between RAG matching on keywords and semantic content during hybrid search. 0 means pure keyword search and 1 means pure semantic search. If unsure, 0.5 is a good starting point.
  - If alpha is set, the certainty parameter has no effect.
- **top_k**
  - Default value: 4
  - The number of results that should be retrieved by RAG.
- **grounding_msg**
  - Default value: “No sources found.”
  - A message that is displayed to the user, without interacting with the LLM, if the RAG certainty feature resulted in 0 chunks being retrieved from the vector DB. Intuitively this implies that there was no suitable information in the RAG collection, which in turn implies that the LLM should not attempt to answer
- **enabled**
  - Default value: true
  - Controls whether RAG is enabled for the pipeline

#### other

- **sampling_params**
  - Default: \{“temperature”: 0.0\}
  - Sampling-related parameters which are given when prompting unless overridden. Allowed values depend on the used inference engine, but contain at least the OpenAI text completion API parameters.
  - For VLLM-based models, see [Sampling Parameters — vLLM](https://docs.vllm.ai/en/latest/dev/sampling_params.html)
- Leave **URL** parameters as default. They control where the LLM pipeline contacts services that could potentially be inside the pipeline itself.

### Adding pipelines for a customer

Customers can either use pipelines under the Silogen organization, or a new pipeline can be made for an existing model.

- Using pipelines from the Silogen organization (recommended for very short demos)
  - Add the pipeline to the customer’s Keycloak group, see [SiloGen Developer Documentation | Configuring a Customer for details](../keycloak-configuration#configuring-a-customer)
  - Pipeline will show as “Silogen/pipeline_name” to the customer
- Making a new pipeline for an existing model (recommended for deployments which will stay up for longer than a demo, to clearly mark that it’s used by that customer)
  - Create a pipeline with the above instructions
  - Set the pipeline’s organization to be exactly the same as the organization value in your customer’s Keycloak group
  - Optional: add the pipeline for the Silogen group in Keycloak to allow testing it as a Silogen member
