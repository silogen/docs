{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SiloGen - AMD Enterprise AI Platform documentation","text":"<p>Welcome to the documentation for SiloGen - AMD Enterprise AI Platform. Enterprises striving to scale AI often encounter roadblocks that increase costs, slow innovation, and limit impact. The AMD Enterprise AI stack is built to overcome these challenges and unlock the full potential of AI across the enterprise.</p> <p>This is your comprehensive handbook designed to help data scientists and platform engineers successfully deploy, manage, and run AI workloads on AMD compute. The guide provides step-by-step instructions for installing and configuring the necessary software, as well as practical tutorials and use cases to help you run AI workloads efficiently on a scalable Kubernetes platform.</p> Introduction <ul> <li> Platform components overview </li> <li> Who is SiloGen for? </li> </ul> Platform installation <ul> <li> Platform installation basics </li> <li> Setup demo environmenta </li> </ul> AI development <ul> <li> AI development overview </li> <li> Tutorials </li> <li> Working with AI workloads </li> <li> Run AI workloads on Developer Center </li> </ul> AI resource management <ul> <li> AI resource overview </li> <li> Managing clusters </li> <li> Managing users </li> </ul>"},{"location":"login-to-silogen/","title":"Login to SiloGen platform","text":"<p>Users log in to SiloGen platform in their dedicated URL address, for example <code>your-silogen.ai</code>. Writing the address into browser's address bar takes you to the login page. The page has a button called \"Sign in with Keycloak\", that takes you to the authentication service.</p> <p></p> <p>Input your email address to the specified field and click \"Sign In\" button to continue with the login. Inputting wrong credentials will end up with an error screen.</p> <p>The next screen requires users to input their password. There is also a link to request a new password in case of loosing the previous one. Users can also restart the login process by clicking the refresh icon. Clicking the \"Sign In\" button will take the user to application or end up with an error message in case of a wrong password.</p> <p></p>","tags":["login","keycloak"]},{"location":"login-to-silogen/#keycloak-user-authentication","title":"Keycloak user authentication","text":"<p>SiloGen platform uses Keycloak for authenticating users. Keycloak is tightly integrated with all of the SiloGen services, and setting it up is straightforward. It can be federated so that users' credentials can be centrally managed in an enterprise solution, and integrated with LDAP or Active Directory.</p>","tags":["login","keycloak"]},{"location":"login-to-silogen/#other-login-measures","title":"Other login measures","text":"<p>SiloGen platform can be modified to utilize other login measures, such as working in an internal network through VPN.</p>","tags":["login","keycloak"]},{"location":"personal-preferences/","title":"Personal preferences","text":"<p>SiloGen platform has several settings the users can adjust to change their user experience.</p>","tags":["color theme","preferences"]},{"location":"personal-preferences/#color-theme","title":"Color theme","text":"<p>The SiloGen platform supports light and dark color modes for different tastes.</p> <p>Users can access the color theme toggler by clicking the username in top right corner of the user interface. A single click in the area activates a dropdown menu containing the color theme switcher. User can change the color theme by clicking the toggle button indicated by a sun or moon icon.</p> <p></p>","tags":["color theme","preferences"]},{"location":"platform-overview/","title":"Overview of the SiloGen Enterprise AI Platform","text":"<p>SiloGen Enterprise AI Platform offers a full-stack AI platform for developing, deploying and running AI workloads on a Kubernetes platform optimized for AMD compute. The platform can utilized by system administrators to resource managers and AI researchers to AI solution developers.</p>","tags":["platform","introduction","components"]},{"location":"platform-overview/#key-features-of-silogen-platform","title":"Key features of SiloGen platform","text":"<p>Silogen platform offers a variety of useful features for utilising AI compute.</p> <p>Optimized GPU utilization &amp; lower operational costs SiloGen ensures high GPU efficiency through intelligent workload placement and dynamic resource sharing. This eliminates waste, reduces costs, and guarantees fair access to compute power \u2014 empowering teams to innovate without delay.</p> <p>Unified AI infrastructure Our platform consolidates fragmented environments into a cohesive AI ecosystem. With standardized governance, tools, and processes, SiloGen simplifies operations and enables seamless collaboration across teams and business units.</p> <p>Accelerated AI delivery SiloGen streamlines the setup of AI resources and automates dependency management. By enabling reuse of AI assets and prebuilt templates, teams can move faster from experimentation to production \u2014 shortening time-to-value.</p> <p>AI-native workload orchestration Unlike traditional schedulers, SiloGen is optimized for AI. It intelligently prioritizes jobs, dynamically allocates resources, and ensures consistent performance, maximizing the utilization of compute infrastructure.</p> <p>With SiloGen, enterprises reduce complexity, accelerate AI deployment, and maximize ROI\u2014turning AI from a cost center into a strategic advantage.</p>","tags":["platform","introduction","components"]},{"location":"platform-overview/#key-components-of-silogen-platform","title":"Key components of SiloGen platform","text":"<p>The SiloGen platform is designed to provide a solid end-to-end model development, evaluation and inference experience optimized for AMD DC GPUs.</p> <p></p> <p>Developer Center (Dev Center) Enables researchers to manage AI workloads end-to-end. The Developer Center enables the usage of the SiloGen AI workloads and focuses on the user experience, offering low code approaches for developing AI applications by simplifying the execution of fine-tuning, inference and other jobs.</p> <p>Focusing on the user experience the Developer Center also exposes a number of catalogues enabling AI researchers to have a better understanding on which models, data and workloads can use and download for the purposes of their work. Finally, the Develop Center offers (and aims to expand the) integrations to well established MLOps tools such as MLFlow, Tensorboards and Kubeflow allowing researchers to use the AI developing tool that feels more natural to them.</p> <p>AI Resouce Manager (Airman) Using Airman enterprises can manage the resource utilization on the R&amp;D compute cluster by mapping user groups to compute, data (and image) resources. With Airman, enterprises can maximize the usage of GPUs by allowing projects and user groups to share GPUs and by configuring the compute clusters with policies that enable fair and smart scheduling. Administrators can monitor the GPU utilization at a project, department, cluster and Enterprise level using Airman dashboards.</p> <p>Kaiwo (Kubernetes - AI Workload Orchestrator) Kaiwo optimizes GPU resource utilization for AI workloads by minimizing GPU idleness. Workload Scheduler increases resource efficiency through intelligent job queueing, fair sharing of resources, guaranteed quotas and opportunistic gang scheduling. It controls the deployment of AI workloads by implementing a Kubernetes operator that watches for the deployment of AI workloads. Main functions: - Decides where and when workloads will be executed based on compute policies. - Supports multiple queues, fair GPU resource sharing, and topology-aware scheduling and other features. Open sourced at https://github.com/silogen/kaiwo.</p> <p>Kubernetes platform The core orchestration platform for managing containerized applications. Kubernetes is the industry standard for orchestrating containerized applications at scale. Kubernetes provides the flexibility, scalability, and reliability needed to support enterprise AI workloads, from training machine learning models to serving predictions in production.</p> <p>Cloud Forge A tool built to help enterprises easily deploy a scalable, production-ready AI platform using open-source technologies. Cloud Forge automates the deployment of the control and compute planes onto Kubernetes clusters, integrates the prepackaged SiloGen AI Workloads, and enables organizations running on AMD hardware to start training and deploying models within just a few hours\u2014essentially offering a streamlined, \"one-click\" setup for enterprise-grade AI infrastructure.</p> <p>AI Workloads Reference AI workloads optimized for AMD compute. This includes model fine-tuning, pretraining, evaluation and inference workloads. The AI workloads have been tested and executed on top of AMD GPUs and they have been open sourced (https://github.com/silogen/ai-workloads). SiloGen AI workloads provide building blocks for broader collaboration across the AI ecosystem and accelerate the development of AI use cases. The catalogue of pre developed AI workloads is continuously updated.</p>","tags":["platform","introduction","components"]},{"location":"target-audience/","title":"The target audience of SiloGen platform","text":"<p>The SiloGen platform and this documentation is designed for various personas covering multiple angles to the AI development.</p> <ul> <li>Data scientists: Professionals who develop and train AI/ML models and need a reliable platform to deploy and scale their workloads.</li> <li>AI practitioners: Anyone involved in building, deploying, or managing AI solutions in an enterprise environment.</li> <li>Team leaders: Managerial people responsible for allowing access to compute, maintaining quotas, and following analytics.</li> <li>Platform engineers: Engineers responsible for setting up, maintaining, and optimizing the infrastructure that supports AI workloads.</li> <li>Field and application engineers: Specialists who support customers in deploying and operating the platform.</li> </ul> <p>While some familiarity with Kubernetes, containerization, and AI/ML concepts is helpful, this handbook provides detailed instructions and explanations to accommodate users with varying levels of expertise.</p> <p>By the end of this handbook, you will have a fully functional Kubernetes platform optimized for AI workloads, along with the knowledge to run and manage AI use cases effectively.</p>","tags":["introduction","target audience"]},{"location":"ai-development/ai-catalogue/","title":"AI workloads catalogue","text":"<p>The AI Catalogue contains reference AI workloads optimized for AMD compute. This includes model fine-tuning, pretraining, evaluation and inference workloads. The AI workloads have been tested and executed on top of AMD GPUs and they have been open sourced (https://github.com/silogen/ai-workloads).</p>","tags":["developer center","ai catalogue","workload"]},{"location":"ai-development/ai-catalogue/#select-a-workload","title":"Select a workload","text":"<p>You can select a workload from the catalogue by selecting \"View and Deploy\".</p> <p></p>","tags":["developer center","ai catalogue","workload"]},{"location":"ai-development/ai-catalogue/#deploy-a-workload","title":"Deploy a workload","text":"<p>You can deploy a workload by clicking \"Quick deploy\".</p> <p></p>","tags":["developer center","ai catalogue","workload"]},{"location":"ai-development/fine-tuning/","title":"Fine-tuning","text":"<p>Fine-tuning a model allows you to customize it to your specific use case with your data. We provide a certified list of base models which you can fine-tune and we allow you to customize certain hyperparameters to get the best results.</p> <p>Fine-tuned models can be deployed and subsequently be used for evaluation and inferencing once weights for the model have been computed.</p>","tags":["developer center","fine-tuning"]},{"location":"ai-development/fine-tuning/#getting-ready-to-fine-tune-your-model","title":"Getting ready to fine-tune your model","text":"","tags":["developer center","fine-tuning"]},{"location":"ai-development/fine-tuning/#uploading-training-data","title":"Uploading training data","text":"<p>One of the first steps to take towards fine-tuning your model is to upload training data to the platform. The training data should represent a wide range of indicative conversations that you would like your model to respond to as part of inference.</p> <p>Once you have procured training data for your model, navigate to the \"Datasets\" page of the developer console to upload the dataset. We currently support uploading datasets in a JSONL format, where each row represents a separate chat conversation.</p> <p>Subsequently, click the \"Upload\" button and drop your JSONL file in with a name and description</p> <p></p>","tags":["developer center","fine-tuning"]},{"location":"ai-development/fine-tuning/#creating-a-fine-tuned-model","title":"Creating a fine-tuned model","text":"<p>Now navigate to Models page. You can trigger the creation of your fine-tuned model by clicking the \"Fine-Tune Model\" button and selecting appropriate entries.</p> <p>You must provide your model a name and can optionally specify a description and any of the three hyperparameters: batch-size, learning-rate multiplier and number of epochs. If you are unsure of the values to use, leave the fields empty, to auto-select the default certified values.</p> <p></p> <p>Once the fine-tuning process has successfully been triggered, you will be able to see your model in the \"Not-deployed\" tab of the page and the fine-tuning run itself in the \"Run status\" section.</p> <p>The fine-tuning run may take several hours to complete, so we recommend visiting the Developer console occasionally when the run is in progress. If the run fails mid-way, please reach out to our customer success team, and we will help triage your issue.</p>","tags":["developer center","fine-tuning"]},{"location":"ai-development/fine-tuning/#deploying-a-fine-tuned-model","title":"Deploying a fine-tuned model","text":"<p>Once your model has been successfully trained, the model status will reflect as \"Ready\": this means that the weights have been successfully computed for your model and it can be used for inferencing.</p> <p>Click the \"Deploy button\" corresponding the model you would like to deploy, and have it usable for inferencing. Please note that a model once deployed can take upto 5 minutes before it can serve requests.</p> <p>Once deployed, you can navigate either to the \"Chat and Compare\" page to converse with the model.</p>","tags":["developer center","fine-tuning"]},{"location":"ai-development/fine-tuning/#examples","title":"Examples","text":"<p>Run model fine-tuning workload through Kubernetes CLI</p> <p>Scalable fine-tuning</p>","tags":["developer center","fine-tuning"]},{"location":"ai-development/inference/","title":"Model inference","text":"","tags":["developer center","inference","chat"]},{"location":"ai-development/inference/#developer-center-playground-chat","title":"Developer Center Playground - Chat","text":"<p>The Playground Chat page allows you to experiment with models you have access to - you will be able to modify several Retrieval and Generation parameters to judge how they affect the model's response.</p>","tags":["developer center","inference","chat"]},{"location":"ai-development/inference/#accessing-the-playground-chat","title":"Accessing the Playground Chat","text":"<p>Navigate to the \"Chat and Compare\" to access the Playground Chat.</p> <p>You will be able to choose from the list of models you have access to. Expand the settings toggle to view and modify the retrieval and model generation parameters.</p> <p></p>","tags":["developer center","inference","chat"]},{"location":"ai-development/inference/#inspecting-the-debug-output-of-the-model","title":"Inspecting the debug output of the model","text":"<p>Once you have a response from the model, you can inspect the messages that were sent to the model, the context retrieved as part of RAG and the consumed tokens by clicking the \"bug\" icon next to the response.</p> <p></p> <p></p>","tags":["developer center","inference","chat"]},{"location":"ai-development/inference/#tutorials-and-examples","title":"Tutorials and examples","text":"<p>This section contains examples and reference workloads for running model inference on SiloGen platform.</p> <p>Model inferencing using SGLang</p> <p>Model inferencing using vLLM</p> <p>Using your own model and data</p>","tags":["developer center","inference","chat"]},{"location":"ai-development/overview/","title":"AI development overview","text":"<p>This article explains how to run and manage AI workloads on SiloGen platform.</p>","tags":["developer center","ai catalogue","workload"]},{"location":"ai-development/overview/#developer-center","title":"Developer Center","text":"<p>At the core of the SiloGen platform is the Developer Center which enables researchers to manage AI workloads end-to-end. Developer Center enables the usage of the SiloGen AI workloads and focuses on the user experience, offering low code approaches for developing AI applications by simplifying the execution of fine-tuning, inference and other jobs. Developer Center includes following capabilities:</p> <ul> <li>AI catalogue with pre-configured AI workloads: SiloGen platform provides researchers with a vast catalog of AI workloads and models optimized for AMD compute. The workloads include the most common developer tooling and frameworks: Jupyter Notebooks, Visual Studio Code and popular frameworks like Pytorch and Tensorflow.</li> <li>GPU-as-a-Service: Self-service access to AMD compute and GPUs for running smaller fine-tuning jobs to large scale distributed pretraining workloads. Access to GPUs and quotas are predefined in the Airman module so your team always has the right amount of resources available.</li> </ul> <p></p>","tags":["developer center","ai catalogue","workload"]},{"location":"ai-development/overview/#silogen-ai-workloads","title":"SiloGen AI Workloads","text":"<p>SiloGen platform offers common AI workloads to AI researchers covering fine-tuning, evaluation and deployment of LLMs. Those AI workloads have been tested and executed on AMD GPUs and they have been open sourced (https://github.com/silogen/ai-workloads). SiloGen AI workloads provide building blocks for broader collaboration across the AI ecosystem and accelerate the development of AI use cases. The catalogue of predeveloped AI workloads is continuously updated.</p>","tags":["developer center","ai catalogue","workload"]},{"location":"ai-development/overview/#running-ai-workloads-on-kubernetes-command-line","title":"Running AI workloads on Kubernetes command-line","text":"<p>You can also run AI workloads directly through a command-line interface using Helm charts and kubectl. You can select a suitable workload from a list of reference workloads in order to get started quickly. The reference workloads offer intelligent workload scheduling based on policies to optimize GPU utilization across workloads.</p> <p>See how to run workloads on Kubernetes.</p>","tags":["developer center","ai catalogue","workload"]},{"location":"ai-development/tutorials/","title":"Tutorials","text":"<p>Tutorials are learning-oriented articles that take the reader by the hand through a series of steps to complete a project of some kind. </p> <p>Tutorials include practical examples how to run AI model training, fine-tuning and inference workloads using SiloGen platform.</p> <ul> <li>Tutorial 0: Prerequisites for running the tutorials: Learn how to fine-tune a continued pretraining basemodel to make it an instruction following model, then deploy models and compare.</li> <li>Deliver resources and fine-tune: Learn how to download models and data to cluster MinIO storage, run fine-tuning jobs, and deploy inference services.</li> <li>Language extension: Odia-fine-tuning: Learn how to fine-tune a continued pretraining basemodel to make it an instruction following model, then deploy models and compare.</li> </ul>","tags":["developer center","tutorial","model training"]},{"location":"airman/overview/","title":"Overview","text":"<p>SiloGen platform provides AI resource management features through a module called Airman. Its key capabilities lie in cluster management and maintaining teams' access to computational resources.</p>","tags":["airman","overview","features"]},{"location":"airman/overview/#airman-features","title":"Airman features","text":"<p>Airman is built around the basic usage pattern of maintaining compute resources, setting up teams and projects, adding quotas, and individual users using the resources for their compute needs.</p> <ul> <li>Cluster: This is the physical part of the platform installation which can be managed in the Airman user interface.</li> <li>Organization: Organization is built from teams. Each team can have multiple users and multiple quotas.</li> <li>Quota: Quota is a usage limit reserved for a team to accomplish their tasks. Quotas are useful for ensuring everyone gets their fair share of compute resourses.</li> <li>User: Users are individuals who require compute access for work purposes.</li> </ul>","tags":["airman","overview","features"]},{"location":"airman/clusters/add-clusters-ui/","title":"Create a new cluster in the UI","text":"<p>This article explains how to create a new cluster in Airman user interface.</p> <p>To add a new cluster:</p> <ol> <li> <p>Click the \"Add Cluster\" button.</p> </li> <li> <p>Cluster identification - Set a unique identifier for your cluster.</p> </li> <li> <p>Set connection parameters and click \"Verify Connection\" button.</p> </li> <li> <p>Verify connection - wait for connection verification to complete.</p> </li> <li> <p>Complete next steps phase in the process.</p> </li> </ol> <p></p>","tags":["airman","cluster","add cluster"]},{"location":"airman/clusters/add-clusters-ui/#delete-a-cluster","title":"Delete a cluster","text":"<ol> <li> <p>Select the cluster you want to delete.</p> </li> <li> <p>Click \"Delete\" on the right menu of the cluster you want to delete.</p> </li> </ol>","tags":["airman","cluster","add cluster"]},{"location":"airman/clusters/overview/","title":"Clusters overview","text":"<p>The Clusters page provides a quick and easy way to see the status of all your clusters.</p>","tags":["airman","cluster"]},{"location":"airman/clusters/overview/#what-is-a-cluster","title":"What is a cluster?","text":"<p>A cluster is a set of nodes (interconnected computers) that work together to run containerized applications, including AI/ML models, data pipelines, and inference services.</p> <p>Cluster is the physical part of the platform installation which can be managed in the Airman user interface.</p>","tags":["airman","cluster"]},{"location":"airman/clusters/overview/#clusters-overview-page","title":"Clusters overview page","text":"<p>The clusters overview provides a list of the clusters added to SiloGen platform, along with their status. You can see the following information about your clusters:</p> <p>Clusters - Clusters for your organization that have been onboarded to AI Resource Manager. Onboarding clusters allows you to manage resources, quotas, and workloads on them.</p> <p>Available nodes - Total number of nodes available across all clusters. AI Resource Manager routinely pulls node information from clusters including resources and health</p> <p>Allocated GPUs - Total number of GPUs allocated to quotas, across all clusters. Users belonging to user groups with GPU quotas are guaranteed allocated GPU resources for their workloads, in the corresponding clusters.</p> <p>Running workloads - Total number of active workloads across all clusters. Workloads submitted via AI Resource Manager optimally scheduled, and tracked and monitored for resource usage and health.</p> <p>Clusters table</p> Column Description Name The name of the cluster Status Status of the cluster Nodes The number of nodes associated with this cluster. Shows the number of healthy nodes and total nodes. GPU allocation The share of the total number of GPUs allocated to quotas in the cluster. Users belonging to user groups with GPU quotas are guaranteed allocated GPU resources for their workloads. CPU allocation The share of the total number of CPUs allocated to quotas in the cluster. Users belonging to user groups with CPU quotas are guaranteed allocated CPU resources for their workloads. Memory allocation The share of the total amount of memory allocated to quotas in the cluster. Users belonging to user groups with GPU quotas are guaranteed allocated GPU resources for their workloads. <p></p>","tags":["airman","cluster"]},{"location":"airman/clusters/overview/#single-cluster-status","title":"Single cluster status","text":"<p>The Cluster page provides a quick and easy way to see the status of your cluster.</p> <p>Available nodes - The total number of nodes available in the cluster. AI Resource Manager routinely pulls node information from the cluster including resources and health.</p> <p>Assigned quotas - Quotas assigned to the cluster resources. User groups can be assigned quotas to limit the amount of resources they are guaranteed on the cluster.</p> <p>Allocated GPUs - The total number of GPUs allocated to quotas in the cluster. Users belonging to user groups with GPU quotas are guaranteed allocated GPU resources for their workloads.</p> <p>Running workloads - Total number of active workloads running on the cluster. Workloads submitted via AI Resource manager optimally scheduled, and tracked and monitored for resource usage and health.</p> <p>ASSIGNED QUOTAS table</p> Column Description IDENTIFIER Unique identifier for the quota STATUS Status of the quota ASSIGNED TO The user group that this quota applies to GPU ALLOCATION Total number of GPUs allocated to this quota <p>NODES table</p> Column Description NAME Name for the node STATUS Status of the node CPU CORES Number of CPU cores available on the node CPU MEMORY Total number of GPUs allocated to this quota GPU TYPE Type of GPU GPU DEVICES Number of GPUs on the node <p></p>","tags":["airman","cluster"]},{"location":"airman/quotas/overview/","title":"Quotas overview","text":"<p>The Quotas page provides a quick and easy way to see the status of all your clusters. </p>","tags":["airman","quota"]},{"location":"airman/quotas/overview/#what-is-a-quota","title":"What is a quota?","text":"<p>Quota is a usage limit reserved for a team to accomplish their tasks. Quotas are useful for ensuring everyone gets their fair share of compute resourses. In Airman you can manage GPU and CPUs quotas for your user groups.</p> <p>Quotas table</p> Column Description Identifier The name of the quota Status Status of the quota Assigned to The user group that this quota applies to. GPU allocation Number of GPUs allocated for this user group. Users belonging to this user group are guaranteed the allocated GPU quota for their workloads. CPU allocation Number of CPUs allocated for this user group. Users belonging to this user group are guaranteed the allocated CPU quota for their workloads. <p></p>","tags":["airman","quota"]},{"location":"airman/quotas/set-quotas/","title":"Manage quotas","text":"<p>This article explains how to define quotas for your user groups. In Airman you can set quotas for GPU and CPUs for your user groups. Users belonging to a user group are guaranteed the allocated quota for their AI workloads.</p>","tags":["airman","quota","add quota"]},{"location":"airman/quotas/set-quotas/#create-quota","title":"Create quota","text":"<p>Cluster - The cluster to which the quota applies to.</p> <p>Assignee - The user group for the quota. A quota can be assigned to only one user group.</p> <p>Identifier - Unique identifier for the quota.</p> <p>Description - A optional description that describes the quota.</p> <p></p>","tags":["airman","quota","add quota"]},{"location":"airman/quotas/set-quotas/#set-the-guaranteed-quota-values","title":"Set the guaranteed quota values","text":"<p>Define a quota below to gurarantee a fixed amount of compute resources for the assigned user group. Setting a quota category to 0 means no resources are guraranteed exclusively for that user group \u2014 its workloads will still use any available resource. Even with a quota in place, workloads may temporarily exceed their quota if additional resources are available.</p> <p>GPUs - The guaranteed allocation for the number of GPUs. \"Available to allocate\" shows the total amount available.</p> <p>CPU Cores - The guaranteed allocation for the number of CPUs. \"Available to allocate\" shows the total amount available.</p> <p>System Memory - The guaranteed allocation for the amount of memory (GB). \"Available to allocate\" shows the total amount available.</p> <p>Ephemeral Disk - The guaranteed allocation for the amount of disk space (GB). \"Available to allocate\" shows the total amount available.</p> <p></p>","tags":["airman","quota","add quota"]},{"location":"airman/users/manage-user-groups/","title":"Manage user groups","text":"<p>A user group is a collection of users bundled together to do AI work.</p> <p>The main view of user group management lists available user groups, the amount of assigned users and a short description about the groups.</p> <p></p> <p>Clicking the user group's name opens the edit view, that is similar to the new user group creation page. Adding new user groups happens by clicking the \"Create user group\" button. The view has a form for editing user group details. Click \"Save changes\" button to accept the update.</p> <p>Below the form is an area called Danger zone. It contains a button for deleting the user group. Deleting a group is a permanent action, and shouldn't be done without proper consideration.</p> <p>The right pane lists users assigned to the user group and option to add or invite new users.</p> <p></p> <p>Only platform administrators can create new user groups and assign users to them.</p>","tags":["user management","user groups"]},{"location":"airman/users/manage-users/","title":"Manage users","text":"<p>Users enter SiloGen platform with their email address that has been granted access. User management happens in Access control section found in the navigation.</p>","tags":["user management","invite users"]},{"location":"airman/users/manage-users/#main-view","title":"Main view","text":"<p>The main view of user management lists users with all relevant information related to them. Mainly the user's name and their email address inform who the user is and the role tells about their capabilities in the platform. Above the list is a search field capable of searching from name and address fields. Below the user list is a pagination used for navigating the user list.</p> <p></p>","tags":["user management","invite users"]},{"location":"airman/users/manage-users/#user-view","title":"User view","text":"<p>When clicking the user's name in the list, the view switches to the user view. It displays a form for updating user's name. The users' email address cannot be changed in the platform. Click \"Save changes\" button to accept the update.</p> <p>Below the form is an area called Danger zone. It contains a button for deleting the user account. Deleting an account is a permanent action, and shouldn't be done without proper consideration.</p> <p>The right pane lists users assignments to different user groups and their dedicated user role. User with proper user role can adjust individual users' user groups or change their role.</p> <p></p>","tags":["user management","invite users"]},{"location":"airman/users/manage-users/#invite-users","title":"Invite users","text":"<p>Inviting new users happens in a special page. Only users within allowed domains are accepted on the platform. Platform administrators can update organization's allowed domains. The main view in Invited users view lists all of the invited users before they have accepted and activated their accounts.</p>","tags":["user management","invite users"]},{"location":"airman/users/overview/","title":"Users overview","text":"<p>Silogen platform has a robust user management system bundled with the authtentication system Keycloak.</p>","tags":["user management","keycloak","roles"]},{"location":"airman/users/overview/#manage-organization","title":"Manage organization","text":"<p>Tooling for managing the organization and user access are found in the navigation under titles Organization and Access control. The first section covers user groups management, while as the second section goes through user management.</p>","tags":["user management","keycloak","roles"]},{"location":"airman/users/overview/#roles","title":"Roles","text":"<p>Airman and Developer Center have two different user roles: platform admin and team member.</p> <p>The platform admin has access to everything happening in the platform. When submitting workloads, they follow the regular quota rules, meaning they can submit workloads only to clusters where they have a quota.</p> <p>The team member has several limitations in their workflows. Team member can:</p> <ul> <li>view clusters if any of their quotas is reserved for the said cluster</li> <li>submit workloads if they have a quota</li> <li>view workloads their user group submitted</li> <li>delete workloads their user group submitted.</li> </ul> <p>Roles are currently closed and they cannot be adjusted in individual SiloGen platform installations.</p>","tags":["user management","keycloak","roles"]},{"location":"platform-installation/demo-environment/","title":"Installing a demo environment","text":"<p>This guide covers the process for installing and configuring a Kubernetes cluster from metal to model in a streamlined manner.</p> <p>The demo environment runs on top of Kubernetes platform and includes some necessary Kubernetes platform components for monitoring, secrets management and certificate management.</p> <p>The installation process leverages helper tools called Cluster Bloom and Cluster Forge that deploy and configure all necessary platform components, preparing a Kubernetes cluster for executing AI workloads.</p>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-installation/demo-environment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ubuntu (supported versions checked at runtime)</li> <li>Sufficient disk space (500GB+ recommended for root partition, 2TB+ for workloads)</li> <li>NVMe drives for optimal storage configuration</li> <li>ROCm-compatible AMD GPUs (for GPU nodes)</li> <li>Root/sudo access</li> </ul>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-installation/demo-environment/#setting-up-the-kubernetes-cluster","title":"Setting up the Kubernetes cluster","text":"<p>Please choose the appropriate install scenario based on your infrastructure preferences.</p>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-installation/demo-environment/#how-to-set-up-a-kubernetes-cluster-with-cluster-bloom-no-existing-kubernetes-cluster","title":"How to set up a Kubernetes cluster with Cluster Bloom (no existing Kubernetes cluster)","text":"<p>Use Cluster Bloom to install and configure a Kubernetes cluster. It installs the following features to prepare a (primarily AMD GPU) node to be part of a Kubernetes cluster:</p> <ul> <li>Automated RKE2 Kubernetes cluster deployment</li> <li>ROCm setup and configuration for AMD GPU nodes</li> <li>Disk management and Longhorn storage integration</li> <li>Multi-node cluster support with easy node joining</li> <li>1Password integration for secrets management</li> <li>Cluster Forge integration</li> </ul>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-installation/demo-environment/#installation-procedure","title":"Installation procedure","text":"<p>Download the latest Cluster Bloom release (adjust the URL to the release of your preference): <pre><code>wget https://github.com/silogen/cluster-bloom/releases/latest/download/bloom\n</code></pre> and run: <pre><code>sudo ./bloom\n</code></pre></p> <p>The Cluster Bloom interface will appear:</p> <p></p> <p>For systems with unmounted physical disks, a selection prompt will appear:</p> <p></p> <p>After successful installation, Cluster Bloom generates <code>additional_node_command.txt</code> that contains the command for installing additional nodes into the cluster.</p>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-installation/demo-environment/#install-kubernetes-components-into-an-existing-kubernetes-cluster","title":"Install Kubernetes components into an existing Kubernetes cluster","text":"<p>To enable running AI workloads on SiloGen platform on an existing Kubernetes cluster, download a Cluster Forge release package and run <code>deploy.sh</code>. This assumes there is a working Kubernetes cluster to deploy into, and the current Kubeconfig context refers to that cluster.</p> <p>For Cluster Forge <code>deploy</code> release:</p> <pre><code>wget https://github.com/silogen/cluster-forge/releases/download/deploy/deploy-release.tar.gz\ntar -xzvf deploy-release.tar.gz\nsudo bash clusterforge/deploy.sh \n</code></pre>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-installation/demo-environment/#validating-the-installation","title":"Validating the installation","text":"<p>Verify successful installation by running TinyLlama workloads in here You can confirm that services are running in the cluster using K9s, a terminal-based UI for Kubernetes clusters, which is installed by Cluster Bloom.</p>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-installation/demo-environment/#appendix","title":"Appendix","text":"<ul> <li>Cluster Forge: https://github.com/silogen/cluster-forge</li> <li>Cluster Bloom: https://github.com/silogen/cluster-bloom</li> </ul>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-installation/installation/","title":"Platform installation and setup","text":"","tags":["platform","installation","demo environment"]},{"location":"platform-installation/installation/#overview","title":"Overview","text":"<p>This guide covers how to install and configure the SiloGen platform on different compute infrastructure. Following infrastructure platforms are supported:</p> <ul> <li>Demo environment (on-premise): This includes setting up a one-node demo cluster with built in storage and secrets management. Main use case is to quickly get started running AI workloads on SiloGen platform. See installation article here.</li> <li>Azure cloud with AKS: See instructions how to setup a virtual machine with Instinct MI300X and SiloGen platform on Azure cloud here.</li> </ul>","tags":["platform","installation","demo environment"]},{"location":"references/overview/","title":"References","text":"<ol> <li> <p>Cluster Bloom: https://github.com/silogen/cluster-bloom</p> </li> <li> <p>Cluster Forge: https://github.com/silogen/cluster-forge</p> </li> <li> <p>Efficient GPU utilization &amp; orchestration using Kaiwo.</p> </li> </ol>"},{"location":"references/tags/","title":"Tags","text":"<p>Here are the tags used in the platform documentation.</p>"},{"location":"references/tags/#tag:add-cluster","title":"add cluster","text":"<ul> <li>            Add new cluster          </li> </ul>"},{"location":"references/tags/#tag:add-quota","title":"add quota","text":"<ul> <li>            Set quotas          </li> </ul>"},{"location":"references/tags/#tag:ai-catalogue","title":"ai catalogue","text":"<ul> <li>            AI catalogue          </li> <li>            Overview          </li> </ul>"},{"location":"references/tags/#tag:airman","title":"airman","text":"<ul> <li>            Add new cluster          </li> <li>            Clusters overview          </li> <li>            Overview          </li> <li>            Quotas overview          </li> <li>            Set quotas          </li> </ul>"},{"location":"references/tags/#tag:chat","title":"chat","text":"<ul> <li>            Inference          </li> </ul>"},{"location":"references/tags/#tag:cluster","title":"cluster","text":"<ul> <li>            Add new cluster          </li> <li>            Clusters overview          </li> </ul>"},{"location":"references/tags/#tag:color-theme","title":"color theme","text":"<ul> <li>            Personal preferences          </li> </ul>"},{"location":"references/tags/#tag:components","title":"components","text":"<ul> <li>            Platform overview          </li> </ul>"},{"location":"references/tags/#tag:demo-environment","title":"demo environment","text":"<ul> <li>            Demo environment installation          </li> <li>            Installation overview          </li> </ul>"},{"location":"references/tags/#tag:developer-center","title":"developer center","text":"<ul> <li>            AI catalogue          </li> <li>            Fine-tuning          </li> <li>            Inference          </li> <li>            Overview          </li> <li>            Overview          </li> </ul>"},{"location":"references/tags/#tag:features","title":"features","text":"<ul> <li>            Overview          </li> </ul>"},{"location":"references/tags/#tag:fine-tuning","title":"fine-tuning","text":"<ul> <li>            Fine-tuning          </li> </ul>"},{"location":"references/tags/#tag:inference","title":"inference","text":"<ul> <li>            Inference          </li> </ul>"},{"location":"references/tags/#tag:installation","title":"installation","text":"<ul> <li>            Demo environment installation          </li> <li>            Installation overview          </li> </ul>"},{"location":"references/tags/#tag:introduction","title":"introduction","text":"<ul> <li>            Platform overview          </li> <li>            Target audience          </li> </ul>"},{"location":"references/tags/#tag:invite-users","title":"invite users","text":"<ul> <li>            Manage users          </li> </ul>"},{"location":"references/tags/#tag:keycloak","title":"keycloak","text":"<ul> <li>            Login to SiloGen          </li> <li>            Users overview          </li> </ul>"},{"location":"references/tags/#tag:kubernetes","title":"kubernetes","text":"<ul> <li>            Demo environment installation          </li> </ul>"},{"location":"references/tags/#tag:login","title":"login","text":"<ul> <li>            Login to SiloGen          </li> </ul>"},{"location":"references/tags/#tag:model-training","title":"model training","text":"<ul> <li>            Overview          </li> </ul>"},{"location":"references/tags/#tag:overview","title":"overview","text":"<ul> <li>            Overview          </li> </ul>"},{"location":"references/tags/#tag:platform","title":"platform","text":"<ul> <li>            Demo environment installation          </li> <li>            Installation overview          </li> <li>            Platform overview          </li> </ul>"},{"location":"references/tags/#tag:preferences","title":"preferences","text":"<ul> <li>            Personal preferences          </li> </ul>"},{"location":"references/tags/#tag:quota","title":"quota","text":"<ul> <li>            Quotas overview          </li> <li>            Set quotas          </li> </ul>"},{"location":"references/tags/#tag:roles","title":"roles","text":"<ul> <li>            Users overview          </li> </ul>"},{"location":"references/tags/#tag:target-audience","title":"target audience","text":"<ul> <li>            Target audience          </li> </ul>"},{"location":"references/tags/#tag:tutorial","title":"tutorial","text":"<ul> <li>            Overview          </li> </ul>"},{"location":"references/tags/#tag:user-groups","title":"user groups","text":"<ul> <li>            Manage user groups          </li> </ul>"},{"location":"references/tags/#tag:user-management","title":"user management","text":"<ul> <li>            Manage user groups          </li> <li>            Manage users          </li> <li>            Users overview          </li> </ul>"},{"location":"references/tags/#tag:workload","title":"workload","text":"<ul> <li>            AI catalogue          </li> <li>            Overview          </li> </ul>"},{"location":"ai-workloads/docs/tutorials/tutorial-01-deliver-resources-and-finetune/","title":"Tutorial 01: Deliver model and data to cluster MinIO, then run finetune","text":"<p>This tutorial shows how to download a model and some data from HuggingFace Hub to a cluster-internal MinIO storage server, and then launch finetuning Jobs that use those resources. The checkpoints are also synced into the same cluster-internal MinIO storage. Finally, an inference workload is spawned to make it possible to discuss with the newly finetuned model. At the end of the tutorial, there are some instructions on changing the model and the data.</p> <p>The finetuning work in this tutorial is meant for demonstration purposes, small enough to be run live. We're starting from Tiny-Llama 1.1B Chat, a small LLM. This is already a chat-finetuned model. We're training it with some additional instruction data in the form of single prompt-and-answer pairs. The prompts in this data were gathered from real human prompts to LLMs, mostly ones that were shared on the now deprecated sharegpt.com site. The answers to those human prompts were generated with the Mistral Large model. So in essence, training on this data makes our model respond more like Mistral Large. And there's another thing that this training accomplishes, which is to change the chat template, meaning the way the input to the model is formatted. More specifically, this adds special tokens that signal the start and end of message. Our experience is that such special tokens make the inference time message end signaling and message formatting a bit more robust.</p>"},{"location":"ai-workloads/docs/tutorials/tutorial-01-deliver-resources-and-finetune/#1-setup","title":"1. Setup","text":"<p>Follow the setup in the tutorial pre-requisites section.</p>"},{"location":"ai-workloads/docs/tutorials/tutorial-01-deliver-resources-and-finetune/#2-run-workloads-to-deliver-data-and-a-model","title":"2. Run workloads to deliver data and a model","text":"<p>We will use the helm charts in <code>workloads/download-huggingface-model-to-bucket/helm</code> and <code>workloads/download-data-to-bucket/helm</code>. We will use them to deliver a Tiny-Llama 1.1B parameter model, and an Argilla single-turn response supervised finetuning dataset, respectively.</p> <p>Our user input files are in <code>workloads/download-huggingface-model-to-bucket/helm/overrides/tutorial-01-tiny-llama-to-minio.yaml</code>, and <code>workloads/download-data-to-bucket/helm/overrides/tutorial-01-argilla-to-minio.yaml</code>. <pre><code>helm template workloads/download-huggingface-model-to-bucket/helm \\\n  --values workloads/download-huggingface-model-to-bucket/helm/overrides/tutorial-01-tiny-llama-to-minio.yaml \\\n  --name-template \"deliver-tiny-llama-model\" \\\n  | kubectl apply -f -\nhelm template workloads/download-data-to-bucket/helm \\\n  --values workloads/download-data-to-bucket/helm/overrides/tutorial-01-argilla-to-minio.yaml \\\n  --name-template \"deliver-argilla-data\" \\\n  | kubectl apply -f -\n</code></pre></p> <p>The logs will show a model staging download and upload for the model delivery workload, and data download, preprocessing, and upload for the data delivery.</p>"},{"location":"ai-workloads/docs/tutorials/tutorial-01-deliver-resources-and-finetune/#3-scaling-finetuning-hyperparameter-tuning-with-parallel-jobs","title":"3. Scaling finetuning: Hyperparameter tuning with parallel Jobs","text":"<p>At the hyperparameter tuning stage, we run many parallel Jobs while varying a hyperparameter to find the best configuration. Here we are going to look for the best rank parameter <code>r</code> for LoRA.</p> <p>To define the finetuning workload, we will use the helm chart in <code>workloads/llm-finetune-silogen-engine/helm</code> . Our user input file is <code>workloads/llm-finetune-silogen-engine/overrides/tutorial-01-finetune-lora.yaml</code> . This also includes the finetuning hyperparameters - you can change them in the file to experiment, or use <code>--set</code> with helm templating to change an individual value.</p> <p>Let's create ten different finetuning jobs to try out different LoRA ranks:</p> <pre><code>run_id=alpha\nfor r in 4 6 8 10 12 16 20 24 32 64; do\n  name=\"tiny-llama-argilla-r-sweep-$run_id-$r\"\n  helm template workloads/llm-finetune-silogen-engine/helm \\\n    --values workloads/llm-finetune-silogen-engine/helm/overrides/tutorial-01-finetune-lora.yaml \\\n    --name-template $name \\\n    --set finetuning_config.peft_conf.peft_kwargs.r=$r \\\n    --set \"checkpointsRemote=default-bucket/experiments/$name\" \\\n    | kubectl apply -f -\ndone\n</code></pre> <p>For each Job we can see logs, a progress bar, and that Job's GPU utilization following the instructions above. If these Jobs get relaunched, they are setup to continue from the existing checkpoints. If we instead want to re-run from scratch, we can just change the <code>run_id</code> variable that is defined before the for loop.</p>"},{"location":"ai-workloads/docs/tutorials/tutorial-01-deliver-resources-and-finetune/#4-scaling-finetuning-multi-gpu-training","title":"4. Scaling finetuning: Multi-GPU training","text":"<p>Beside parallel Jobs, we can also take advantage of multiple GPUs by using them for parallel compute. This can be helpful for more compute demanding Jobs, and necessary with larger models.</p> <p>Let's launch an 8GPU run of full-parameter finetuning:</p> <pre><code>name=\"tiny-llama-argilla-v1\"\nhelm template workloads/llm-finetune-silogen-engine/helm \\\n  --values workloads/llm-finetune-silogen-engine/helm/overrides/tutorial-01-finetune-full-param.yaml \\\n  --name-template $name \\\n  --set \"checkpointsRemote=default-bucket/experiments/$name\" \\\n  --set \"finetuningGpus=8\" \\\n  | kubectl apply -f -\n</code></pre> <p>We can see logs, a progress bar, and the full 8-GPU compute utilization following the instructions above. The training steps of this multi-gpu training run take merely 75 seconds, which reflects the nature of finetuning: fast, iterative, with a focus on flexible experimentation.</p> <p>If we want to compare to an equivalent single-GPU run, we can run:</p> <pre><code>name=\"tiny-llama-argilla-v1-singlegpu\"\nhelm template workloads/llm-finetune-silogen-engine/helm \\\n  --values workloads/llm-finetune-silogen-engine/helm/overrides/tutorial-01-finetune-full-param.yaml \\\n  --name-template $name \\\n  --set \"checkpointsRemote=default-bucket/experiments/$name\" \\\n  --set \"finetuningGpus=1\" \\\n  | kubectl apply -f -\n</code></pre> <p>The training steps for this single-GPU run take around 340 seconds. Thus the full-node training yields a speedup ratio of around 0.22 (4.5x speed). Even higher speedups are achieved in pretraining, which benefits hugely from optimizations.</p>"},{"location":"ai-workloads/docs/tutorials/tutorial-01-deliver-resources-and-finetune/#5-inference-with-a-finetuned-model","title":"5. Inference with a finetuned model","text":"<p>After training the model, we'll want to discuss with it. For this we will use the helm chart in <code>workloads/llm-inference-vllm/helm</code> .</p> <p>Let's deploy the full-parameter finetuned model:</p> <pre><code>name=\"tiny-llama-argilla-v1\"\nhelm template workloads/llm-inference-vllm/helm \\\n  --set \"model=s3://default-bucket/experiments/$name/checkpoint-final\" \\\n  --set \"vllm_engine_args.served_model_name=$name\" \\\n  --name-template \"$name\" \\\n  | kubectl apply -f -\n</code></pre> <p>We can change the <code>name</code> to different experiment names to deploy other models. Note that discussing with the LoRA adapter models with these workloads requires us to merge the final adapter. This can be achieved during finetuning by adding <code>--set mergeAdapter=true</code> and additionally in the deploy command, we have to refer to the merged model, changing the path to <code>--set \"model=s3://default-bucket/experiments/$name/checkpoint-final-merged\"</code> .</p> <p>To discuss with the model, we first need to setup a connection to it. Since this is not a public-internet deployment, we'll do this simply by starting a background port-forwarding process:</p> <pre><code>name=\"tiny-llama-argilla-v1\"\nkubectl port-forward services/llm-inference-vllm-$name 8080:80 &gt;/dev/null &amp;\nportforwardPID=$!\n</code></pre> <p>Now we can discuss with the model, using curl:</p> <pre><code>name=\"tiny-llama-argilla-v1\"\nquestion=\"What are the top five benefits of eating a large breakfast?\"\ncurl http://localhost:8080/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"'$name'\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"'\"$question\"'\"}\n        ]\n    }' | jq \".choices[0].message.content\" --raw-output\n</code></pre> <p>We can test the limits of the model with our own questions. Since this is a model with a relatively limited capacity, its answers are often delightful nonsense.</p> <p>When we want to stop port-forwarding, we can just run: <pre><code>kill $portforwardPID\n</code></pre> and to stop the deployment, we run: <pre><code>name=\"tiny-llama-argilla-v1\"\nkubectl delete deployments/llm-inference-vllm-$name\n</code></pre></p>"},{"location":"ai-workloads/docs/tutorials/tutorial-01-deliver-resources-and-finetune/#next-steps-how-to-use-your-own-model-and-data","title":"Next Steps: How to use your own model and data","text":"<p>This tutorial has shown the basic steps in running finetuning and chatting with the resulting model. For many, the next step may be to use our own models and data. This section should get us started, but ultimately, this opens the whole topic of how to do finetuning, which is too large to cover here. One more comprehensive view point is provided by the T\u00fcl\u00fc 3 paper.</p>"},{"location":"ai-workloads/docs/tutorials/tutorial-01-deliver-resources-and-finetune/#preparing-your-own-model-and-data","title":"Preparing your own model and data","text":"<p>The workload <code>workloads/download-huggingface-model-to-bucket/helm</code> delivers HuggingFace Hub models. To get models from elsewhere, we may for instance do it manually by downloading them to our own computers and uploading to our bucket storage from there. The data delivery workload <code>workloads/download-data-to-bucket/helm</code> uses a free script to download and preprocess the data, so it is more flexible in this regard.</p> <p>The bucket storage used in this tutorial is a MinIO server hosted inside the cluster itself. To use some other S3-compatible bucket storage, we need to change the <code>bucketStorageHost</code> field, add our credentials (HMAC keys) as a Secret in our namespace (this is generally achieved via an External Secret that in turn fetches the info from some secret store that we have access to), and then refer to that bucket storage credentials Secret in the <code>bucketCredentialsSecret</code> nested fields.</p> <p>To prepare our own model, we create a values file that is similar to <code>workloads/download-huggingface-model-to-bucket/helm/overrides/tutorial-01-tiny-llama-to-minio.yaml</code>. . The key field is <code>modelID</code>, which defins which model is downloaded. The field <code>bucketModelPath</code> determines where the model is stored in the bucket storage.</p> <p>To prepare our own data, we structure our values file like <code>workloads/download-data-to-bucket/helm/overrides/tutorial-01-argilla-to-minio.yaml</code>. It may be easiest to write a Python script separately, potentially test it locally, and then put the script as a block text value for <code>dataScript</code>. The dataset upload location is set with the <code>bucketDataDir</code> field.</p>"},{"location":"ai-workloads/docs/tutorials/tutorial-01-deliver-resources-and-finetune/#data","title":"Data","text":"<p>The <code>dataScript</code> is a script instead of just a dataset identifier, because the datasets on HuggingFace hub don't have a standard format that can be always directly passed to our finetuning engine. The data script should format the data into the format that the silogen finetuning engine expects. For supervised finetuning, this is JSON lines, where each line has a JSON dictionary formatted as follows: <pre><code>{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"This is a user message\"},\n    {\"role\": \"assistant\", \"content\": \"The is an assistant answer\"}\n  ]\n}\n</code></pre> There can be an arbitrary number of messages. Additionally, each dictionary can contain a <code>dataset</code> field that has the dataset identifier, and an <code>id</code> field that identifies the data point uniquely. For Direct Preference Optimization, the data format is as follows: <pre><code>{\n  \"prompt_messages\": [\n    {\"role\": \"user\", \"content\": \"This is a user message\"},\n  ],\n  \"chosen_messages\": [\n    {\"role\": \"assistant\", \"content\": \"This is a preferred answer\"}\n  ],\n  \"rejected_messages\": [\n    {\"role\": \"assistant\", \"content\": \"This is a rejected answer\"}\n  ]\n}\n</code></pre></p> <p>The JSON lines output of the data script should be saved to the <code>/downloads/datasets/</code>. This is easy with the approach taken in the tutorial file: <pre><code>dataset.to_json(\"/downloads/datasets/&lt;name of your dataset file.jsonl&gt;\")\n</code></pre> The dataset is uploaded to the directory pointed to by <code>bucketDataDir</code>, with the same filename as it had under <code>/downloads/datasets</code>.</p>"},{"location":"ai-workloads/docs/tutorials/tutorial-01-deliver-resources-and-finetune/#model","title":"Model","text":"<p>Preparing a model is simple than data. We simply set the <code>modelID</code> to the HuggingFace Hub ID of the model (in the <code>Organization/ModelName</code> format). The model is the uploaded to the path pointed to by <code>bucketModelPath</code>.</p>"},{"location":"ai-workloads/docs/tutorials/tutorial-01-deliver-resources-and-finetune/#setting-finetuning-parameters","title":"Setting finetuning parameters","text":"<p>For finetuning, we create a values file that is similar to <code>workloads/llm-finetune-silogen-engine/helm/overrides/tutorial-01-finetune-lora.yaml</code> (for LoRA adapter training) or <code>workloads/llm-finetune-silogen-engine/overrides/tutorial-01-finetune-full-param.yaml</code> (for full parameter training). We'll want to inject our data in the field: <pre><code>finetuning_config:\n  data_conf:\n    training_data:\n      datasets:\n        - path: \"bucketName/path/to/file.jsonl\n</code></pre> The datasets array can contain any number of datasets, and they're concatenated in training.</p> <p>The model is set in the top level field <code>basemodel</code>, where the value should be a name of a bucket followed by the path to the model directory in the bucket, formatted like: <pre><code>basemodel: bucketName/path/to/modelDir\n</code></pre></p> <p>All finetuning configurations are not sensible with all models, and some settings might even fail for unsupported models. Ultimately we need to understand the particular model we're using to set the parameters correctly. Suitable hyperparameters also depend on the data.</p> <p>One key model compatibility parameter to look at is the chat template, which is set by <pre><code>finetuning_config:\n  data_conf:\n    chat_template_name: \"&lt;name of template&gt;\"\n</code></pre> If the model we start from already has a chat template, we should usually set this to <code>\"keep-original\"</code>. Otherwise, <code>\"chat-ml\"</code> is usually a reasonable choice. Another set of parameters that often needs to be changed between models is the set of PEFT target layers, if doing LoRA training. These are set in the following field: <pre><code>finetuning_config:\n  peft_conf:\n    peft_kwargs:\n      target_modules:\n        - q_proj\n        - k_proj\n        - v_proj\n        - o_proj\n        - up_proj\n        - down_proj\n</code></pre> One setting that can be used is <pre><code>finetuning_config:\n  peft_conf:\n    peft_kwargs:\n      target_modules: \"all-linear\"\n</code></pre> which targets all linear layers on the model and doesn't require knowing the names of the layers.</p>"},{"location":"ai-workloads/docs/tutorials/tutorial-02-language-extension-finetune/","title":"Finetuning: adding language-capabilities to a model","text":"<p>Adding a new language to model usually follows at least two stages: first, continued pretraining to build understanding and basic capacity to generate that language, and second, finetuning to make the model e.g. follow instructions. This tutorial handles that second stage of instruction-tuning for Odia language.</p> <p>The original basemodel is Qwen1.5-7B. The first stage of continued pretraining to add general Odia understanding and generation abilities to Qwen have already been done by the OdiaGenAI organization. That continued pretrained model is available openly here. A relevant comparison point for this tutorial is the chat-finetuned version of the Qwen1.5 basemodel, which should have the capability to follow instructions, but is not specifically meant for Odia.</p> <p>Note that access to the Odia continued pretraining version of the Qwen model requires signing the request on Huggingface. This also means that for downloading the model, we'll need to use a HuggingFace access token. See instructions here. \u26a0\ufe0f Note: This tutorial does not add the HF Token to the cluster yet. You need to add it yourself. \u26a0\ufe0f</p> <p>The tutorial includes cluster setup, model and data downloads, finetuning, and finally inference. We should start with a working cluster, setup by a cluster administrator using Cluster-forge. The access to that cluster is provided with a suitable Kubeconfig file.</p>"},{"location":"ai-workloads/docs/tutorials/tutorial-02-language-extension-finetune/#1-setup","title":"1: Setup","text":"<p>Follow the setup in the tutorial pre-requisites section.</p> <p>\u26a0\ufe0f WARNING: This tutorial does not handle adding the HF Token to the cluster yet. Coming soon. Before then, to run this tutorial, you are responsible for adding your HF Token as a secret called <code>hf-token</code> with the key <code>hf-token</code> in the <code>silo-tutorial</code> namespace. \u26a0\ufe0f</p>"},{"location":"ai-workloads/docs/tutorials/tutorial-02-language-extension-finetune/#2-fetch-data-and-models","title":"2. Fetch data and models","text":"<p>First we'll fetch the model and data for the finetuning. We will use the helm charts in <code>workloads/download-huggingface-model-to-bucket/helm</code> and <code>workloads/download-data-to-bucket/helm</code>. We will download a Qwen1.5 7B-parameter model, one English instruction dataset and five different Odia-language single-turn instruction finetuning datasets. These datasets cover slightly different areas, including open instructions, context-based question-answering, translation, and identity answers. The identity answers aim to make our model call itself Olive, and tell that it is from the OdiaGenAI project. Our user input files are in <code>workloads/download-huggingface-model-to-bucket/helm/overrides/tutorial-02-qwen-odia.yaml</code> and <code>workloads/download-data-to-bucket/helm/overrides/tutorial-02-odia-data.yaml</code></p> <pre><code>helm template workloads/download-huggingface-model-to-bucket/helm \\\n  --values workloads/download-huggingface-model-to-bucket/helm/overrides/tutorial-02-qwen-odia.yaml \\\n  --name-template \"download-odia-qwen-odia\" \\\n  | kubectl apply -f -\nhelm template workloads/download-data-to-bucket/helm \\\n  --values workloads/download-data-to-bucket/helm/overrides/tutorial-02-odia-data.yaml \\\n  --name-template \"download-odia-data\" \\\n  | kubectl apply -f -\n</code></pre> <p>The logs will show a model staging download and upload, then data download, preprocessing, and upload.</p>"},{"location":"ai-workloads/docs/tutorials/tutorial-02-language-extension-finetune/#3-finetune-an-odia-model-on-8gpus","title":"3. Finetune an Odia model on 8GPUs","text":"<p>We will run our Odia language finetuning using 8 GPUs in a data parallel setup. For finetuning, we'll use the helm chart in  <code>workloads/llm-finetune-silogen-engine/helm</code>, and the user input file in <code>workloads/llm-finetune-silogen-engine/helm/overrides/tutorial-02-qwen-odia-instruct-v1.yaml</code>.</p> <p>This training job takes around 13 hours, because the combination of 6 datasets is large and we go through that combination 6 times. We have found that a large number of training steps is necessary to enable the model to learn act reasonably in Odia language. We suspect this has something to do with the Odia script, which is not well covered by the Qwen tokenizer, and leads to very long sequences produced character or even diacritic at a time.</p> <p>Let's launch the finetuning Job: <pre><code>name=\"qwen-odia-instruct-v1\"\nhelm template workloads/llm-finetune-silogen-engine/helm \\\n  --values workloads/llm-finetune-silogen-engine/helm/overrides/tutorial-02-qwen-odia-instruct-v1.yaml \\\n  --name-template $name \\\n  --set \"checkpointsRemote=default-bucket/experiments/$name\" \\\n  --set \"finetuningGpus=8\" \\\n  | kubectl apply -f -\n</code></pre> can see logs, a progress bar, and the full 8-GPU compute utilization following the instructions above.</p>"},{"location":"ai-workloads/docs/tutorials/tutorial-02-language-extension-finetune/#4-compare-the-official-qwen15-7b-chat-model-the-odia-continued-pretraining-basemodel-and-the-odia-finetuned-model","title":"4. Compare the official Qwen1.5-7B-Chat model, the Odia continued pretraining basemodel, and the Odia-finetuned model:","text":"<p>The Qwen1.5-7B-Chat is a general chat-finetuned version of the same Qwen basemodel that our Odia continued pretraining started from. Thus it is a good point of comparison. Additionally we'll deploy the Odia continued pretraining basemodel, to see how the instruct-finetuning changed it.</p> <p>For inference deployments, we will use the helm chart in  <code>workloads/llm-inference-vllm/helm/chart</code>.</p> <p>Deploy the models with the following commands. Note that the Qwen1.5-7B-Chat model we're getting directly from HuggingFace, while the other two models we're fetching from the cluster internal bucket storage. <pre><code>name=\"qwen-base-chat\"\nhelm template workloads/llm-inference-vllm/helm \\\n  --set \"model=Qwen/Qwen1.5-7B-Chat\" \\\n  --set \"vllm_engine_args.served_model_name=$name\" \\\n  --name-template \"$name\" \\\n  | kubectl create -f -\nname=\"qwen-odia-base\"\nhelm template workloads/llm-inference-vllm/helm \\\n  --set \"model=s3://default-bucket/models/OdiaGenAI/LLM_qwen_1.5_odia_7b\" \\\n  --set \"vllm_engine_args.served_model_name=$name\" \\\n  --name-template \"$name\" \\\n  | kubectl create -f -\nname=\"qwen-odia-instruct-v1\"\nhelm template workloads/llm-inference-vllm/helm \\\n  --set \"model=s3://default-bucket/experiments/$name/checkpoint-final\" \\\n  --set \"vllm_engine_args.served_model_name=$name\" \\\n  --name-template \"$name\" \\\n  | kubectl create -f -\n</code></pre></p> <p>To discuss with the models, we need to setup connections to them. Since these are not public-internet deployments, we'll do this simply by starting background port-forwarding processes: <pre><code>name=\"qwen-base-chat\"\nkubectl port-forward services/llm-inference-vllm-$name 8080:80 &gt;/dev/null &amp;\nqwenchatPID=$!\nname=\"qwen-odia-base\"\nkubectl port-forward services/llm-inference-vllm-$name 8090:80 &gt;/dev/null &amp;\nodiabasePID=$!\nname=\"qwen-odia-instruct-v1\"\nkubectl port-forward services/llm-inference-vllm-$name 8100:80 &gt;/dev/null &amp;\nodiainstructPID=$!\n</code></pre></p> <p>Now we can talk to the models. We'll ask them about the difference between physics and chemistry, in Odia: <pre><code>question=\"\u0b2a\u0b26\u0b3e\u0b30\u0b4d\u0b25 \u0b2c\u0b3f\u0b1c\u0b4d\u0b1e\u0b3e\u0b28 \u0b0f\u0b2c\u0b02 \u0b30\u0b38\u0b3e\u0b5f\u0b28 \u0b2c\u0b3f\u0b1c\u0b4d\u0b1e\u0b3e\u0b28 \u0b2e\u0b27\u0b4d\u0b5f\u0b30\u0b47 \u0b15\u2019\u0b23 \u0b2a\u0b3e\u0b30\u0b4d\u0b25\u0b15\u0b4d\u0b5f \u0b05\u0b1b\u0b3f?\"\ntemperature=0.0\nmax_tokens=2048\npresence_penalty=1.2\nmin_p=0.05\necho -e \"\\n\\nQwen 1.5 7B Chat:\"\nname=\"qwen-base-chat\"\ncurl http://localhost:8080/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"'$name'\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"'\"$question\"'\"}\n        ],\n        \"temperature\": '$temperature',\n        \"max_tokens\": '$max_tokens',\n        \"presence_penalty\": '$presence_penalty',\n        \"min_p\": '$min_p'\n    }' 2&gt;/dev/null | jq \".choices[0].message.content\" --raw-output | fold -s | sed 's/^/  /'\necho -e \"\\n\\nQwen 1.5 7B Odia-CPT Base:\"\nname=\"qwen-odia-base\"\ncurl http://localhost:8090/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"'$name'\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"'\"$question\"'\"}\n        ],\n        \"temperature\": '$temperature',\n        \"max_tokens\": '$max_tokens',\n        \"presence_penalty\": '$presence_penalty',\n        \"min_p\": '$min_p'\n    }' 2&gt;/dev/null | jq \".choices[0].message.content\" --raw-output | fold -s | sed 's/^/  /'\necho -e \"\\n\\nQwen 1.5 Odia-CPT Instruction-tuned model v1:\"\nname=\"qwen-odia-instruct-v1\"\ncurl http://localhost:8100/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"'$name'\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"'\"$question\"'\"}\n        ],\n        \"temperature\": '$temperature',\n        \"max_tokens\": '$max_tokens',\n        \"presence_penalty\": '$presence_penalty',\n        \"min_p\": '$min_p'\n    }' 2&gt;/dev/null | jq \".choices[0].message.content\" --raw-output | fold -s | sed 's/^/  /'\n</code></pre> We'll find that the general Qwen chat-model does not keep to Odia, it easily switches to e.g. Hindi or Chinese. The Odia continued pretraining basemodel does answer in Odia, but since it is just a continuation model, the text it produces is not really answer to the question, but rather rambling. The finetuned model answers the question in Odia.</p> <p>When we're done chatting with the models, we can kill the port-forwards with: <pre><code>kill $qwenchatPID $odiabasePID $odiainstructPID\n</code></pre> and we can shut down the inference deployments with: <pre><code>for name in qwen-base-chat qwen-odia-base qwen-odia-instruct-v1; do\n  kubectl delete deployment llm-inference-vllm-$name\ndone\n</code></pre></p>"},{"location":"ai-workloads/docs/tutorials/tutorial-prereqs/","title":"Tutorial 0: Prerequisites for running the tutorials","text":""},{"location":"ai-workloads/docs/tutorials/tutorial-prereqs/#required-program-installs","title":"Required program installs","text":"<p>Programs that are used in this tutorial:</p> <ul> <li> <p>kubectl</p> </li> <li> <p>helm</p> </li> <li> <p>k9s</p> </li> <li> <p>jq</p> </li> <li> <p>curl</p> </li> </ul> <p>At least curl and often jq too are commonly installed in many distributions out of the box.</p>"},{"location":"ai-workloads/docs/tutorials/tutorial-prereqs/#namespace-setup","title":"Namespace setup","text":"<p>In order to run the workloads you need to have a Kubernetes cluster with sufficient resources configured. This includes storage, secrets, namespace and HuggingFace token. These should be installed and configured as part of the installation process, but if this is not the case you can use the following command to set up these. The command does the following:</p> <ul> <li> <p>Adds a namespace, where we will conduct all our work. We will setup the <code>silo-tutorial</code> namespace, but this also works in the default namespace in your cluster.</p> </li> <li> <p>Adds an External Secret to get the credentials to access the MinIO storage from our namespace.</p> <ul> <li>This depends on a ClusterSecretStore called <code>k8s-secret-store</code> being already setup by a cluster admin, and the MinIO API credentials being secret there. The cluster should have these by default.</li> </ul> </li> <li> <p>Adds a LocalQueue so that our Jobs schedule intelligently.</p> <ul> <li>This references the ClusterQueue <code>kaiwo</code> which should already be setup by a cluster admin.</li> </ul> </li> </ul> <p>We will use the helm chart in <code>workloads/k8s-namespace-setup/helm</code> and the overrides in <code>workloads/k8s-namespace-setup/helm/overrides/</code>.</p> <p>This first creates a new namespace and sets the current context to use it from now on: <pre><code>kubectl create namespace \"silo-tutorial\"\nkubectl config set-context --current --namespace silo-tutorial\nhelm template workloads/k8s-namespace-setup/helm \\\n  --values workloads/k8s-namespace-setup/helm/overrides/tutorial-prereqs-local-queue.yaml \\\n  --values workloads/k8s-namespace-setup/helm/overrides/tutorial-prereqs-storage-access-external-secret.yaml \\\n  | kubectl apply -f -\n</code></pre></p> <ul> <li>HuggingFace token: In addition to running the command above you also need to add your HF Token as a secret called <code>hf-token</code> with the key <code>hf-token</code> in your namespace.</li> </ul>"},{"location":"ai-workloads/docs/tutorials/tutorial-prereqs/#monitoring-progress-logs-and-gpu-utilization-with-k9s","title":"Monitoring progress, logs, and GPU utilization with k9s","text":"<p>We're interested to see a progress bar of the finetuning training, seeing any messages that a workload logs, and we also want to verify that our GPU Jobs are consuming our compute relatively effectively. This information can be fetched from our Kubernetes cluster in many ways, but one convenient and recommended way us using k9s. We recommend the official documentation for more thorough guidance, but this section shows some basic commands to get what we want here.</p> <p>To get right to the Jobs view in the namespace we're using in this walk-through, we can run:</p> <pre><code>k9s --command Jobs\n</code></pre> <p>Choose a Job using <code>arrow keys</code> and <code>Enter</code> to see the Pod that it spawned, then <code>Enter</code> again to see the Container in the Pod. From here, we can do three things:</p> <ul> <li> <p>Look at the logs by pressing <code>l</code>. The logs show any output messages produced during the workload runtime.</p> </li> <li> <p>Attach to the output of the container by pressing <code>a</code>. This is particularly useful to see the interactive progress bar of a finetuning run.</p> </li> <li> <p>Spawn a shell inside the container by pressing <code>s</code>. Inside the shell we can run <code>watch -n0.5 rocm-smi</code> to get a view of the GPU utilization that updates every 0.5s.</p> </li> </ul> <p>Return from any regular <code>k9s</code> view with <code>Esc</code> .</p>"},{"location":"ai-workloads/workloads/workloads-overview/","title":"Working with AI workloads on Kubernetes","text":""},{"location":"ai-workloads/workloads/workloads-overview/#overview","title":"Overview","text":"<p>This document provides a general introduction to deploying AI workloads on Kubernetes using Helm charts and Kubernetes manifests. While each specific AI workload in our solution has its own dedicated documentation, this guide explains the common concepts, structure, and deployment patterns that apply across all our AI workload Helm charts.</p> <p>Each AI workload is defined as a separate Helm chart that can be submitted to run on a Kubernetes platform.</p>"},{"location":"ai-workloads/workloads/workloads-overview/#what-is-helm","title":"What is Helm?","text":"<p>Helm is the package manager for Kubernetes, often referred to as \"the apt/yum for K8s.\" It simplifies the deployment and management of applications by:</p> <ul> <li>Packaging Kubernetes resources into reusable charts (pre-configured templates).</li> <li>Managing dependencies, versions, and configurations.</li> <li>Supporting easy upgrades, rollbacks, and customization.</li> </ul> <p>Think of it as a blueprint that defines how an application should be installed and run in a Kubernetes cluster.</p>"},{"location":"ai-workloads/workloads/workloads-overview/#how-does-helm-relate-to-kubernetes-manifests","title":"How does Helm relate to Kubernetes manifests?","text":"<p>Helm doesn't replace manifests - it generates them dynamically using: - Templates: Manifest files with variables (in templates/ directory) - Values: Configuration that fills the template variables</p>"},{"location":"ai-workloads/workloads/workloads-overview/#why-use-helm-for-ai-workloads","title":"Why Use Helm for AI Workloads?","text":"<p>Deploying AI/ML workloads (inference, training, batch processing) on Kubernetes can be complex due to:</p> <ul> <li>Dependencies (models, datasets, GPU drivers).</li> <li>Configuration variability (resource limits, scaling, model versions).</li> <li>Reproducibility (consistent deployments across dev/test/prod).</li> </ul> <p>Helm addresses these challenges by:</p> <ol> <li>Standardizing Deployments:</li> <li> <p>AI components (model servers, preprocessing, monitoring) are defined in a Helm chart (deployment.yaml, service.yaml, etc.).</p> </li> <li> <p>Managing Configurations:</p> </li> <li> <p>values.yaml centralizes tunable parameters (e.g., replicaCount, modelPath, GPU limits). Overrides allow specifying specific models, datasets or environment-specific setups (e.g., dev vs. prod):</p> </li> <li> <p>Supporting AI-Specific Needs</p> </li> <li>GPU/accelerator support: Define resource requests in values.yaml.</li> <li>Model storage: Mount PersistentVolumes or download models at runtime.</li> <li>Scaling: Pre-configure Horizontal Pod Autoscaler (HPA) for inference workloads.</li> </ol>"},{"location":"ai-workloads/workloads/workloads-overview/#structure-of-the-workloads","title":"Structure of the workloads","text":"<pre><code>&lt;workload-name&gt;/\n\u251c\u2500\u2500 Chart.yaml          # Metadata about the chart, such as the release name.\n\u251c\u2500\u2500 values.yaml         # Default configuration values for the workload.\n\u251c\u2500\u2500 templates/          # Kubernetes manifest templates that Helm uses to generate actual manifests\n\u2502   \u251c\u2500\u2500 deployment.yaml # Configuration of the AI workload deployment\n\u2502   \u251c\u2500\u2500 service.yaml    # Configuration of the Kubernetes service\n\u2514\u2500\u2500 overrides           # Customization of the AI workload without modifying the original chart\n</code></pre>"},{"location":"ai-workloads/workloads/workloads-overview/#understanding-the-valuesyaml-file","title":"Understanding the values.yaml file","text":"<p>The values.yaml file is a YAML-formatted configuration file that contains key-value pairs representing the parameters required for your model inference deployment. These parameters include: - Image name: name of the docker image to use</p> <ul> <li> <p>Resources: Define GPU, CPU and memory requirements for your deployment.</p> </li> <li> <p>Model: Provide the name of the model.</p> </li> <li> <p>Storage details</p> </li> <li> <p>Environment Variables: Set environment-specific configurations, such as secrets and storage host location.</p> </li> <li> <p>Service Configuration: Customize service settings like ports, timeouts, and logging levels.</p> </li> </ul>"},{"location":"ai-workloads/workloads/workloads-overview/#overrides","title":"Overrides","text":"<p>Overrides allow customization of the AI workload without modifying the original chart. This includes changing the model and data sets.</p>"},{"location":"ai-workloads/workloads/workloads-overview/#running-ai-workloads-on-k8s","title":"Running AI workloads on k8s","text":"<p>It is recommended to use <code>helm template</code> and pipe the result to <code>kubectl create</code> , rather than using <code>helm install</code>. Generally, a command looks as follows</p> <p><pre><code>helm template &lt;release-name&gt; . -f &lt;override.yaml&gt; | kubectl apply -f -\n</code></pre> or <pre><code>helm template &lt;release-name&gt; . --set &lt;parameter&gt;=&lt;value&gt; | kubectl apply -f -\n</code></pre></p>"},{"location":"ai-workloads/workloads/workloads-overview/#how-to-use-overrides-to-customize-the-workload","title":"How to use overrides to customize the workload","text":"<p>There are multiple options you can use to customize the workload by applying overrides.</p> <p>Alternative 1: Deploy a Specific Model Configuration</p> <p>To deploy a specific model along with its settings, use the following command from the <code>helm</code> directory:</p> <pre><code>helm template tiny-llama . -f overrides/models/tinyllama_tinyllama-1.1b-chat-v1.0.yaml | kubectl apply -f -\n</code></pre> <p>Alternative 2: Override the Model</p> <p>You can also override the model on the command line:</p> <pre><code>helm template qwen2-0-5b . --set model=Qwen/Qwen2-0.5B-Instruct | kubectl apply -f -\n</code></pre> <p>Helm merges overrides in this order (last takes precedence):</p> <ol> <li>values.yaml (in chart)</li> <li>-f files (in command order)</li> <li>--set arguments</li> </ol>"},{"location":"ai-workloads/workloads/workloads-overview/#verifying-the-deployment","title":"Verifying the Deployment","text":"<p>Check pods <pre><code>kubectl get pods -n &lt;namespace&gt;\n</code></pre> Check services <pre><code>kubectl get svc -n &lt;namespace&gt;\n</code></pre> View logs (for a specific pod) <pre><code>kubectl logs -f &lt;pod-name&gt; -n &lt;namespace&gt;\n</code></pre></p>"},{"location":"ai-workloads/workloads/workloads-overview/#monitoring-progress-logs-and-gpu-utilization-with-k9s","title":"Monitoring progress, logs, and GPU utilization with k9s","text":"<p>We're interested to see a progress bar of the finetuning training, seeing any messages that a workload logs, and we also want to verify that our GPU Jobs are consuming our compute relatively effectively. This information can be fetched from our Kubernetes cluster in many ways, but one convenient and recommended way us using k9s. We recommend the official documentation for more thorough guidance, but this section shows some basic commands to get what we want here.</p> <p>To get right to the Jobs view in the namespace <code>&lt;namespace&gt;</code>, we can run:</p> <pre><code>k9s --namespace &lt;namespace&gt; --command Jobs\n</code></pre> <p>Choose a Job using <code>arrow keys</code> and <code>Enter</code> to see the Pod that it spawned, then <code>Enter</code> again to see the Container in the Pod. From here, we can do three things:</p> <ul> <li> <p>Look at the logs by pressing <code>l</code>. The logs show any output messages produced during the workload runtime.</p> </li> <li> <p>Attach to the output of the container by pressing <code>a</code>. This is particularly useful to see the interactive progress bar of a finetuning run.</p> </li> <li> <p>Spawn a shell inside the container by pressing <code>s</code>. Inside the shell we can run <code>watch -n0.5 rocm-smi</code> to get a view of the GPU utilization that updates every 0.5s.</p> </li> </ul> <p>Return from any regular <code>k9s</code> view with <code>Esc</code> .</p>"},{"location":"ai-workloads/workloads/workloads-overview/#editing-reusing-workloads","title":"Editing &amp; reusing workloads","text":"<p>To create a new workload, you can duplicate an existing workload and adapt as needed.</p> <p>Example: Using your own model and data in the workloads</p>"},{"location":"ai-workloads/workloads/dev-chatui-openwebui/helm/","title":"Open WebUI for LLM Services","text":"<p>This Helm Chart deploys a WebUI for aggregating deployed LLM services within the cluster.</p>"},{"location":"ai-workloads/workloads/dev-chatui-openwebui/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>The basic configurations for deployment are defined in the <code>values.yaml</code> file.</p> <p>To deploy the service, execute the following command from the Helm folder:</p> <pre><code>helm template &lt;release-name&gt; . | kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads/workloads/dev-chatui-openwebui/helm/#automatic-discovery-and-health-checks-for-llm-services","title":"Automatic Discovery and Health Checks for LLM Services","text":"<p>OpenAI-compatible endpoints can by specified by the user through the <code>env_vars.OPENAI_API_BASE_URLS</code> environment variable. Additionally, service discovery is used to include all OpenAI-compatible LLM inference services running in the same namespace.</p>"},{"location":"ai-workloads/workloads/dev-chatui-openwebui/helm/#client-side-service-discovery-optional","title":"Client-Side Service Discovery (Optional)","text":"<p>Client-side discovery can be performed using the <code>--dry-run=server</code> flag:</p> <pre><code>helm template &lt;release-name&gt; . --dry-run=server | kubectl apply -f -\n</code></pre> <p>For a service to be included in <code>OPENAI_API_BASE_URLS_AUTODISCOVERY</code> during client-side discovery: - The service must be running in the same namespace. - The service name must start with <code>llm-inference-</code>.</p>"},{"location":"ai-workloads/workloads/dev-chatui-openwebui/helm/#server-side-service-discovery","title":"Server-Side Service Discovery","text":"<p>The system performs server-side discovery of LLM inference services automatically. For a service to be included, the following conditions must be met: - The service must be running in the same namespace. - The service name must start with <code>llm-inference-</code>. - The pod's service account must have the necessary permissions to check running services in the namespace (configured via role-binding).</p>"},{"location":"ai-workloads/workloads/dev-chatui-openwebui/helm/#health-checks-and-filtering","title":"Health Checks and Filtering","text":"<p>Before finalizing <code>OPENAI_API_BASE_URLS</code> and starting the service, the URLs specified by the user and the auto-discovered services are merged, and filtered based on a health check.</p> <p>For a service to be included in the final <code>OPENAI_API_BASE_URLS</code>: - The service must respond successfully to the <code>/v1/models</code> endpoint with an HTTP status code of 200.</p> <p>The final <code>OPENAI_API_BASE_URLS</code> determines what services/models are included in Open WebUI interface.</p>"},{"location":"ai-workloads/workloads/dev-text2image-comfyui/helm/","title":"ComfyUI Text-to-Image/Video Workload","text":"<p>This Helm Chart deploys a ComfyUI web app for text-to-image/video generation.</p>"},{"location":"ai-workloads/workloads/dev-text2image-comfyui/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>Basic configurations are defined in the <code>values.yaml</code> file.</p> <p>To deploy the service, run the following command within the <code>helm</code> folder:</p> <pre><code>helm template . | kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads/workloads/dev-text2image-comfyui/helm/#interacting-with-the-workload","title":"Interacting with the Workload","text":""},{"location":"ai-workloads/workloads/dev-text2image-comfyui/helm/#verify-deployment","title":"Verify Deployment","text":"<p>Check the deployment and service status:</p> <pre><code>kubectl get deployment\nkubectl get service\n</code></pre>"},{"location":"ai-workloads/workloads/dev-text2image-comfyui/helm/#port-forwarding","title":"Port Forwarding","text":"<p>To access the service locally on port <code>8188</code>, forward the port of the service/deployment using the following commands. This assumes the service name is <code>dev-text2image-comfyui</code>:</p> <p>The service exposes HTTP on port 80 (the deployment uses port 8188 by default).</p> <pre><code>kubectl port-forward services/dev-text2image-comfyui 8188:80\n</code></pre> <p>Now, you can access ComfyUI (Manager also included) at http://localhost:8188 using a web browser.</p>"},{"location":"ai-workloads/workloads/dev-workspace-vscode/helm/","title":"Visual Studio Code Workload","text":"<p>This workload deploys a basic Visual Studio Code environment on top of any image with Python (pip) pre-installed. It is ideal for interactive development sessions and experimentation with other workloads.</p>"},{"location":"ai-workloads/workloads/dev-workspace-vscode/helm/#configuration-parameters","title":"Configuration Parameters","text":"<p>You can configure the following parameters in the <code>values.yaml</code> file or override them via the command line:</p> Parameter Description Default <code>image</code> Container image repository and tag <code>rocm/pytorch:rocm6.4_ubuntu24.04_py3.12_pytorch_release_2.6.0</code> <code>imagePullPolicy</code> Image pull policy <code>Always</code> <code>gpus</code> Number of GPUs to allocate (set to 0 for CPU-only mode) <code>1</code> <code>memory_per_gpu</code> Memory allocated per GPU (in Gi) <code>128</code> <code>cpu_per_gpu</code> CPU cores allocated per GPU <code>4</code> <code>storage.ephemeral</code> Ephemeral storage configuration <code>128Gi</code>, <code>mlstorage</code>, <code>ReadWriteOnce</code> <code>deployment.ports.http</code> HTTP port exposed by the service <code>8080</code> <code>entrypoint</code> Custom entrypoint script See <code>values.yaml</code> for details <p>For more details see the <code>values.yaml</code> file.</p>"},{"location":"ai-workloads/workloads/dev-workspace-vscode/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>To deploy the chart with the release name <code>example</code>, run the following command from the <code>helm/</code> directory:</p> <pre><code>helm template example . | kubectl apply -f -\n</code></pre> <p>Note: If you set the <code>gpus</code> value greater than 0, ensure you specify a GPU-capable image to utilize the allocated resources properly.</p>"},{"location":"ai-workloads/workloads/dev-workspace-vscode/helm/#accessing-the-workload-locally","title":"Accessing the Workload Locally","text":"<p>To access Visual Studio Code locally, forward the service port to your machine:</p> <pre><code>kubectl port-forward services/dev-workspace-vscode-example 8080:80\n</code></pre> <p>Then, open your browser and navigate to <code>http://localhost:8080</code>.</p>"},{"location":"ai-workloads/workloads/dev-workspace-vscode/helm/#accessing-the-workload-via-url","title":"Accessing the Workload via URL","text":"<p>To access the workload through a URL, you can enable either an Ingress or HTTPRoute in the <code>values.yaml</code> file. The following parameters are available:</p> Parameter Description Default <code>ingress.enabled</code> Enable Ingress resource <code>false</code> <code>httproute.enabled</code> Enable HTTPRoute resource <code>false</code> <p>See the corresponding template files in the <code>templates/</code> directory. For more details on configuring Ingress or HTTPRoute, refer to the Ingress documentation and HTTPRoute documentation, or documentation of the particular gateway implementation you may use, like KGateway. Check with your cluster administrator for the correct configuration for your environment.</p>"},{"location":"ai-workloads/workloads/download-data-to-bucket/helm/","title":"workload helm template to download and preprocess data to bucket storage","text":"<p>This is an workload which downloads data, potentially preprocesses it, and uploads it to bucket storage. Since the <code>helm install</code> semantics are centered around on-going installs, not jobs that run once, it's best to just run <code>helm template</code> and pipe the result to <code>kubectl create</code> (<code>create</code> maybe more appropriate than apply for this Job as we don't expect to modify existing entities). Example: <pre><code>helm template workloads/download-data-to-bucket/helm \\\n    -f workloads/download-data-to-bucket/overrides/argilla-mistral-large-human-prompts.yaml \\\n    --name-template download-argilla \\\n    | kubectl create -f -\n</code></pre></p>"},{"location":"ai-workloads/workloads/download-data-to-bucket/helm/#user-input-values","title":"User input values","text":"<p>See the <code>values.yaml</code> file for all user input values that you can provide, with instructions. In values.yaml, the <code>dataScript</code> is a script instead of just a dataset identifier, because the datasets on HuggingFace hub don't have a standard format that can be always directly passed to any training framework. The data script should format the data into the format that the training framework expects.</p> <p>Any data files output of the data script should be saved to the <code>/downloads/datasets/</code>. The files are uploaded to the directory pointed to by <code>bucketDataDir</code>, with the same filename as they had under <code>/downloads/datasets</code>.</p>"},{"location":"ai-workloads/workloads/download-data-to-bucket/helm/#silogen-finetuning-engine-format","title":"Silogen finetuning engine format","text":"<p>For the silogen finetuning engine, the data format is JSON lines. For supervised finetuning, each line has a JSON dictionary formatted as follows: <pre><code>{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"This is a user message\"},\n    {\"role\": \"assistant\", \"content\": \"The is an assistant answer\"}\n  ]\n}\n</code></pre> There can be an arbitrary number of messages. Additionally, each dictionary can contain a <code>dataset</code> field that has the dataset identifier, and an <code>id</code> field that identifies the data point uniquely. For Direct Preference Optimization, the data format is as follows: <pre><code>{\n  \"prompt_messages\": [\n    {\"role\": \"user\", \"content\": \"This is a user message\"},\n  ],\n  \"chosen_messages\": [\n    {\"role\": \"assistant\", \"content\": \"This is a preferred answer\"}\n  ],\n  \"rejected_messages\": [\n    {\"role\": \"assistant\", \"content\": \"This is a rejected answer\"}\n  ]\n}\n</code></pre></p> <p>The HuggingFace datasets library can save the output in JSON lines format with the <code>to_json</code> function: <pre><code>dataset.to_json(\"/downloads/datasets/&lt;name of your dataset file.jsonl&gt;\")\n</code></pre></p>"},{"location":"ai-workloads/workloads/download-huggingface-model-to-bucket/helm/","title":"workload helm template to download a model to bucket storage","text":"<p>This is an workload which downloads a model and uploads it to bucket storage. Since the <code>helm install</code> semantics are centered around on-going installs, not jobs that run once, it's best to just run <code>helm template</code> and pipe the result to <code>kubectl create</code> (<code>create</code> maybe more appropriate than apply for this Job as we don't expect to modify existing entities). Example: <pre><code>helm template workloads/download-huggingface-model-to-bucket/helm \\\n    -f workloads/download-huggingface-model-to-bucket/helm/overrides/llama-3.1-tiny-random-to-google.yaml \\\n    --name-template download-huggingface \\\n    | kubectl create -f -\n</code></pre></p>"},{"location":"ai-workloads/workloads/download-huggingface-model-to-bucket/helm/#user-input-values","title":"User input values","text":"<p>See the <code>values.yaml</code> file for the user input values that you can provide, with instructions.</p>"},{"location":"ai-workloads/workloads/k8s-namespace-setup/helm/","title":"k8s-namespace-setup","text":"<p>A Helm chart for setting up Kubernetes namespaces. This chart allows you to configure the following components in a namespace:</p> <ul> <li>External Secrets: Pull in secrets from within or outside the cluster, such as bucket storage credentials or Hugging Face tokens.</li> <li>Kueue: Set up a local queue for Kueue in the namespace to submit jobs.</li> <li>Role / Role Binding: Configure permissions for service accounts, like the default service account, to access the Kubernetes API from within a container.</li> </ul> <p>If you are not sure if you need any of these, then this workload is probably not needed for you.</p>"},{"location":"ai-workloads/workloads/k8s-namespace-setup/helm/#installation","title":"Installation","text":"<p>To apply a configuration to the active namespace, use:</p> <pre><code>helm template . -f overrides/rename-secret-names.yaml | kubectl apply -f -\n</code></pre> <p>To specify a different namespace:</p> <pre><code>helm template . -f overrides/rename-secret-names.yaml | kubectl apply -n &lt;namespace&gt; -f -\n</code></pre> <p>Control which components to set up using command line parameters:</p> <pre><code>helm template . --set kueue.setup=true --set role.setup=true | kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads/workloads/k8s-namespace-setup/helm/#configuration","title":"Configuration","text":"<p>The following table lists the configurable parameters of the <code>k8s-namespace-setup</code> chart and their default values.</p>"},{"location":"ai-workloads/workloads/k8s-namespace-setup/helm/#external-secret","title":"External Secret","text":"Parameter Description Default <code>external_secret.setup</code> Enable external secret <code>false</code> <code>external_secret.external_secret_name</code> External secret name <code>minio-credentials-fetcher</code> <code>external_secret.src.secret_store_name</code> Secret store name <code>k8s-secret-store</code> <code>external_secret.src.remote_secret_name</code> Remote secret name <code>default-user</code> <code>external_secret.src.access_key_name</code> Access key name <code>API_ACCESS_KEY</code> <code>external_secret.src.secret_key_name</code> Secret key name <code>API_SECRET_KEY</code> <code>external_secret.dest.k8s_secret_name</code> Kubernetes secret name <code>minio-credentials</code> <code>external_secret.dest.access_key_name</code> Kubernetes access key name <code>minio-access-key</code> <code>external_secret.dest.secret_key_name</code> Kubernetes secret key name <code>minio-secret-key</code>"},{"location":"ai-workloads/workloads/k8s-namespace-setup/helm/#kueue","title":"Kueue","text":"Parameter Description Default <code>kueue.setup</code> Enable kueue <code>false</code> <code>kueue.cluster_queue_name</code> Cluster queue name <code>kaiwo</code>"},{"location":"ai-workloads/workloads/k8s-namespace-setup/helm/#roles","title":"Roles","text":"Parameter Description Default <code>role.setup</code> Enable roles setup <code>false</code> <code>role.name</code> Role name <code>default-role</code> <code>role.bindingName</code> Role binding name <code>default-role-binding</code> <code>role.rules</code> Role rules See <code>values.yaml</code>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/","title":"Finetuning with the SiloGen finetuning engine","text":"<p>This is a Helm Chart for finetuning Jobs based on the SiloGen finetuning engine. The chart integrates the finetuning config as part of the <code>values.yaml</code> input.</p> <p>See the <code>values.yaml</code> file for the general structure (more documentation coming soon).</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/#running-the-workload","title":"Running the workload","text":"<p>Since the <code>helm install</code> semantics are centered around on-going installs, not jobs that run once, it's best to just run <code>helm template</code> and pipe the result to <code>kubectl create</code>.</p> <p>Example command: <pre><code>helm template . \\\n  -f overrides/llama-31-tiny-random-deepspeed-values.yaml \\\n  --name-template llama-31-tiny-random-deepspeed-alpha \\\n  | kubectl create -f -\n</code></pre></p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/#multiple-overlays-simplified-interface","title":"Multiple overlays, simplified interface","text":"<p>This chart supports two ways of specifying certain inputs, one on the top level and one as part of the finetuning_config: - Training data can be provided as <code>trainingData</code> or <code>finetuning_config.data_conf.training_data.datasets</code> - The total batch size target can be provided as <code>batchSize</code> or <code>finetuning_config.batchsize_conf.total_train_batch_size</code> - The number of epochs to run for can be provided as <code>numberOfEpochs</code> or <code>finetuning_config.training_args.num_train_epochs</code></p> <p>The top level inputs provide a simpler interface to run finetuning. However, they're not enough alone to fully specify a sensible training setup. The expectation is that these top-level inputs are used in conjuction with a set of override files that specify most arguments. This is the expected way that the chart is used in conjuction with the so called Silogen developer console. An example of such use is: <pre><code>helm template . \\\n  -f overrides/models/meta-llama_llama-3.1-8b.yaml \\\n  -f overrides/dev-console/default.yaml \\\n  --name-template llama-31-8b-argilla-alpha \\\n  | kubectl create -f -\n</code></pre></p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/#multiple-overlays-general-case","title":"Multiple overlays, general case","text":"<p>Multiple overlays can be useful for a CLI user as well. Here's an example that reproduces the output of tutorial-01-finetune-full-param.yaml: <pre><code>helm template . \\\n  -f overrides/models/tiny-llama_tinyllama-1.1b-chat-v1.0.yaml \\\n  -f overrides/additional-example-files/repro-tutorial-01-user-inputs.yaml \\\n  --name-template tiny-llama-argilla-alpha \\\n  | kubectl create -f -\n</code></pre> To check that the manifests match, we can run a diff and see that it is empty: <pre><code>diff \\\n  &lt;( \\\n    helm template . \\\n    -f overrides/models/tiny-llama_tinyllama-1.1b-chat-v1.0.yaml \\\n    -f overrides/additional-example-files/repro-tutorial-01-user-inputs.yaml \\\n    --name-template tiny-llama-argilla-alpha \\\n  ) \\\n  &lt;( \\\n    helm template . \\\n    -f overrides/tutorial-01-finetune-full-param.yaml \\\n    --name-template tiny-llama-argilla-alpha \\\n  )\n</code></pre></p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/#tensorboard","title":"Tensorboard","text":"<p>Specifying <code>runTensorboard: true</code> and <code>finetuning_config.trainings_args.report_to: [\"tensorboard\"]</code> logs the training progress to tensorboard and serves the Tensorboard web UI from the training container. The tensorboard logs are also uploaded to the bucket storage for later use.</p> <p>To connect to the Tensorboard web UI on the container, start a port-forward: <pre><code>kubectl port-forward --namespace YOUR_NAMESPACE pods/YOUR_POD_NAME 6006:6006\n</code></pre> Then browse to localhost:6006.</p> <p>Note that the logging frequency is set by the HuggingFace Transformers logging options.</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/#best-known-configuration-model-overrides","title":"Best-known-configuration model overrides","text":"<p>The directory <code>overrides/models</code> hosts finetuning recipes for various models. The files are named according to model canonical names, which is the huggingface pattern of <code>organization/model-name</code> just changed into <code>organization_model-name</code>. These configurations have been shown to work well in experiments, but that does not guarantee that these exact parameters are always optimal. The best parameters still depend on the data, too.</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/","title":"Finetuning config structure and parameters","text":"<p>This document describes the structure of the finetuning configuration, and the parameters and values that can be defined there.</p> <p>See the finetuning config section this config file for an example of a valid configuration. See the various sub-configs for their options. Additional properties are not allowed.</p> <p>Top-level properties:</p> Property Type Required Possible values Default Description method <code>const</code> <code>sft</code> <code>\"sft\"</code> data_conf <code>object</code> \u2705 ChatTrainValidConfig The data input config training_args <code>object</code> \u2705 SilogenTrainingArguments Transformer TrainingArguments with some restrictions overrides <code>object</code> Overrides <code>{\"num_train_epochs\": null, \"lr_multiplier\": 1.0, \"lr_batch_size_scaling\": \"none\"}</code> Override options to simplify the config interface batchsize_conf <code>object</code> \u2705 BatchsizeConfig Batch size configuration peft_conf <code>object</code> \u2705 NoPeftConfig or PretrainedPeftConfig or GenericPeftConfig Adapter configuration run_conf <code>object</code> \u2705 RunConfig Model related configuration tracking <code>object</code> or <code>null</code> FinetuningTrackingConfig MLFlow tracking configuration quant_conf <code>object</code> BnBQuantizationConfig or NoQuantizationConfig <code>{\"quantization_type\": \"no-quantization\"}</code> Quantization configuration sft_args <code>object</code> \u2705 SFTArguments SFT specific arguments"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#definitions","title":"Definitions","text":""},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#autosplitdatainput","title":"AutoSplitDataInput","text":"<p>Automatic validation split from the training data</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description type <code>const</code> \u2705 <code>AUTO_SPLIT</code> data_type <code>string</code> string <code>\"ChatConversation\"</code> generally, the data_type is automatically set based on the experiment config method ratio <code>number</code> number <code>0.2</code> Ratio of the training data to use for validation seed <code>integer</code> integer <code>1289525893</code> Seed for the random number generator for splitting"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#batchsizeconfig","title":"BatchsizeConfig","text":"<p>Config for determining the total batch size</p> <p>Total batch size is the effective batch size for the complete training run. It is equal to number of processes * per-device batch size * accumulation.</p> <p>The maximum batch size per device is the maximum batch size that can be accommodated on a single device. This mostly limited by the memory capacity of the device.</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_1","title":"Type: <code>object</code>","text":"Property Type Required Possible values Description total_train_batch_size <code>integer</code> \u2705 integer The total batch size for the training run max_per_device_train_batch_size <code>integer</code> \u2705 integer The maximum training batch size per device per_device_eval_batch_size <code>integer</code> or <code>null</code> integer The maximum eval batch size per device, if not given, will use same as training batch size"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#bnbquantizationconfig","title":"BnBQuantizationConfig","text":"<p>Bits and Bytes configuration</p> <p>The options are from the BitsAndBytes config, see: https://huggingface.co/docs/transformers/en/main_classes/quantization#transformers.BitsAndBytesConfig</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_2","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description quantization_type <code>const</code> <code>bits-and-bytes</code> <code>\"bits-and-bytes\"</code> load_in_8bit <code>boolean</code> boolean <code>False</code> load_in_4bit <code>boolean</code> boolean <code>False</code> llm_int8_threshold <code>number</code> number <code>6.0</code> llm_int8_skip_modules <code>array</code> or <code>null</code> string llm_int8_enable_fp32_cpu_offload <code>boolean</code> boolean <code>False</code> llm_int8_has_fp16_weight <code>boolean</code> boolean <code>False</code> bnb_4bit_compute_dtype <code>string</code> or <code>null</code> string bnb_4bit_quant_type <code>const</code> <code>fp4</code> and/or <code>nf4</code> <code>\"fp4\"</code> bnb_4bit_use_double_quant <code>boolean</code> boolean <code>False</code> bnb_4bit_quant_storage <code>string</code> or <code>null</code> string"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#chattrainvalidconfig","title":"ChatTrainValidConfig","text":"<p>Training time data configuration.</p> <p>Always defines some DataInput for training data and can include validation DataInput, though a trivial NoneDataInput is also allowed for the validation side.</p> <p>Additionally includes chat template and padding configurations, as those are part of the data input pipeline.</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_3","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description training_data <code>object</code> \u2705 ConcatenationDataInput or WeightedMixDataInput validation_data <code>object</code> \u2705 AutoSplitDataInput or ConcatenationDataInput or NoneDataInput chat_template_name <code>string</code> <code>mistral-with-system</code> or <code>chat-ml</code> or <code>poro</code> or <code>keep-original</code> or <code>simplified-llama31</code> <code>\"mistral-with-system\"</code> padding_side <code>string</code> string <code>\"right\"</code> Padding side, right is usually right. missing_pad_token_strategy <code>string</code> MissingPadTokenStrategy <code>\"bos-repurpose\"</code> See the MissingPadTokenStrategys for descriptions of the options"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#concatenationdatainput","title":"ConcatenationDataInput","text":"<p>A simple list of datasets</p> <p>These are simply concatenated, the same as sampling all with equal weight.</p> <p>The datasets themselves need to be in the finetuning supported JSONL formats. For SFT this means lines:</p> <pre><code>{\"messages\": {\"content\": \"string\", \"role\": \"string\"}}\n</code></pre> <p>For DPO this means lines of:</p> <pre><code>{\"prompt_messages\": {\"content\": \"string\", \"role\": \"string\"}, \"chosen_messages\": {\"content\": \"string\", \"role\": \"string\"}, \"rejected_messages\": {\"content\": \"string\", \"role\": \"string\"}}\n</code></pre>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_4","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description type <code>const</code> \u2705 <code>CONCATENATION</code> datasets <code>array</code> \u2705 DatasetDefinition data_type <code>string</code> string <code>\"ChatConversation\"</code> generally, the data_type is automatically set based on the experiment config method"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#datasetdefinition","title":"DatasetDefinition","text":"<p>Define how to load a dataset</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_5","title":"Type: <code>object</code>","text":"Property Type Required Possible values Description path <code>string</code> \u2705 string Local path to a JSONL file in the finetuning data format"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#finetuningtrackingconfig","title":"FinetuningTrackingConfig","text":"<p>Settings that define how run details are logged</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_6","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description mlflow_server_uri <code>string</code> \u2705 string MLflow server URI. Can be local path experiment_name <code>string</code> \u2705 string Experiment name that is used for MLFlow tracking run_id <code>string</code> or <code>null</code> string Run id, to resume logging to previousely started run run_name <code>string</code> or <code>null</code> string Run name, to give meaningful name to the run to be displayed in MLFlow UI. Used only when run_id is unspecified hf_mlflow_log_artifacts <code>string</code> string <code>\"False\"</code> Whether to store model artifacts in MLFlow"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#genericpeftconfig","title":"GenericPeftConfig","text":"<p>Config for any new initialized PEFT Adapter</p> <p>See https://huggingface.co/docs/peft/tutorial/peft_model_config for the possible kwargs and https://github.com/huggingface/peft/blob/v0.7.1/src/peft/utils/peft_types.py for the types.</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#example","title":"Example","text":"<pre><code>&gt;&gt;&gt; loaded_data = {'peft_type':'LORA', 'task_type': 'CAUSAL_LM',\n...         'peft_kwargs': {'r': 32, 'target_modules': ['v_proj']}}\n&gt;&gt;&gt; generic_conf = GenericPeftConfig(**loaded_data)\n&gt;&gt;&gt; # Then later in the code something like:\n&gt;&gt;&gt; model = transformers.AutoModel.from_pretrained('hf-internal-testing/tiny-random-MistralModel')\n&gt;&gt;&gt; peft.get_peft_model(model, generic_conf.get_peft_config())\nPeftModelForCausalLM(\n  (base_model): LoraModel(\n    ...\n  )\n)\n</code></pre>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_7","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description peft_type <code>string</code> \u2705 PeftType task_type <code>string</code> TaskType <code>\"CAUSAL_LM\"</code> peft_kwargs <code>object</code> object"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#missingpadtokenstrategy","title":"MissingPadTokenStrategy","text":"<p>Specifies the available missing pad token strategies.</p> <p>We've shown in a small set of experiments that repurposing EOS can start to hurt performance while the other options seem to work equally well.</p> <p>Repurposing EOS is the default in many online sources, but it is actually a bad idea if we want to predict EOS, as all the pad_token_ids get ignored in loss computation, and thus the model does not learn to predict the end of the text. However, for models that have additional tokens for end of message, end of turn, etc. this is not so dangerous.</p> <p>Repurposing BOS is similar to repurposing EOS, but since we do not need to predict BOS, this may be more sensible.</p> <p>Repurposing UNK can work with tokenizers that never produce UNKs in normal data (e.g. Mistral tokenizers should have a byte fall-back so that everything can be tokenized).</p> <p>UNK_CONVERT_TO_EOS uses a hack where the unk_token_id is initially used for padding, but in the collation phase the input-side UNKs (padding) gets set to EOS, so that the input-side padding looks like EOS. On the output-side, the UNKs (padding) still gets ignored. NOTE: This will leave the tokenizer's pad_token_id set to the unk_token_id; so any subsequent use of the model where padding is involved should somehow explicitly set the pad_token_id again.</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-string","title":"Type: <code>string</code>","text":"<p>Possible Values: <code>eos-repurpose</code> or <code>bos-repurpose</code> or <code>unk-repurpose</code> or <code>unk-convert-to-eos</code></p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#modelarguments","title":"ModelArguments","text":"<p>These are passed to AutoModelForCausalLM.from_pretrained</p> <p>See parameter docstrings and help at: https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained See below in \"Parameters for big model inference\" too, it affects training too. Also note that this link takes you to the transformers main branch version - be sure to compare with the installed version of transformers (that keeps changing over time, and it is difficult to keep this doctstring up to date, so we wanted to link to the latest here).</p> <p>Some important parameters to consider are: - device_map :     A map that specifies where each submodule should go. It doesn\u2019t need to be refined to each parameter/buffer     name, once a given module name is inside, every submodule of it will be sent to the same device. If we only pass     the device (e.g., \"cpu\", \"cuda:1\", \"mps\", or a GPU ordinal rank like 1) on which the model will be allocated,     the device map will map the entire model to this device. Passing device_map = 0 means put the whole model on GPU     0. - attn_implementation :     The attention implementation to use in the model (if relevant). Can be any of \"eager\" (manual implementation of     the attention), \"sdpa\" (using F.scaled_dot_product_attention), or \"flash_attention_2\" (using     Dao-AILab/flash-attention). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is     otherwise the manual \"eager\" implementation.</p> <p>NOTE:     This does not include quantization_config. Quantization config is specified separately.</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_8","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description torch_dtype <code>const</code> <code>auto</code> <code>\"auto\"</code> device_map <code>object</code> or <code>string</code> or <code>null</code> object and/or string Custom device map so that you can manually override the choices that HuggingFace would make. This can also be a string to specify \"auto\", \"balanced_low_0\", or \"sequential\" max_memory <code>object</code> or <code>null</code> object low_cpu_mem_usage <code>boolean</code> boolean <code>False</code> attn_implementation <code>string</code> or <code>null</code> string Note: this can be set to \"sdpa\", \"flash_attention_2\", \"eager\" offload_folder <code>string</code> or <code>null</code> string offload_state_dict <code>boolean</code> or <code>null</code> boolean Default is True if offloading (otherwise no effect) offload_buffers <code>boolean</code> or <code>null</code> boolean use_cache <code>boolean</code> boolean <code>True</code> Saves generated hidden states to speed up generation. See: https://discuss.huggingface.co/t/what-is-the-purpose-of-use-cache-in-decoder/958 use_cache is mutually exclusive with gradient_checkpointing cache_dir <code>string</code> or <code>null</code> string force_download <code>boolean</code> boolean <code>False</code> local_files_only <code>boolean</code> boolean <code>False</code> proxies <code>object</code> or <code>null</code> object resume_download <code>boolean</code> boolean <code>False</code> revision <code>string</code> string <code>\"main\"</code> code_revision <code>string</code> string <code>\"main\"</code> subfolder <code>string</code> or <code>null</code> string token <code>string</code> or <code>null</code> string use_safetensors <code>boolean</code> or <code>null</code> boolean variant <code>string</code> or <code>null</code> string trust_remote_code <code>boolean</code> boolean <code>False</code> Warning: if set to <code>True</code>, allows execution of downloaded remote code"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#nopeftconfig","title":"NoPeftConfig","text":"<p>A trivial config specifying that no peft is used</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_9","title":"Type: <code>object</code>","text":"Property Type Required Possible values Description peft_type <code>const</code> \u2705 <code>NO_PEFT</code>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#noquantizationconfig","title":"NoQuantizationConfig","text":"<p>A marker not to use quantization</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_10","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description quantization_type <code>const</code> <code>no-quantization</code> <code>\"no-quantization\"</code>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#nonedatainput","title":"NoneDataInput","text":"<p>A special type for not using data e.g. in validation</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_11","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description type <code>const</code> \u2705 <code>NONE</code> data_type <code>string</code> string <code>\"ChatConversation\"</code> generally, the data_type is automatically set based on the experiment config method"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#overrides","title":"Overrides","text":"<p>Override options that allow simple interfaces for charts using these configs</p> <p>This is particularly useful for a helm chart interface where we include the finetuning package config as a part of the values.yaml file. These a more flexible helm interface with certain keys brought to the top level.</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_12","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description num_train_epochs <code>integer</code> or <code>number</code> or <code>null</code> number Overrides the number of epochs in the training_args lr_multiplier <code>number</code> number <code>1.0</code> Multiplier applied to the learning rate in the training_args lr_batch_size_scaling <code>string</code> <code>none</code> <code>sqrt</code> <code>linear</code> <code>\"none\"</code> Scales the learning rate in the training_args by a factor derived from the total training batch size. <code>none</code>: No scaling. <code>sqrt</code>: Multiplies learning rate by square root of batch size (a classic scaling rule). <code>linear</code>: Multiplies learning rate by the batch size (a more modern scaling rule)."},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#pefttype","title":"PeftType","text":"<p>Enum class for the different types of adapters in PEFT.</p> <p>Supported PEFT types: - PROMPT_TUNING - MULTITASK_PROMPT_TUNING - P_TUNING - PREFIX_TUNING - LORA - ADALORA - BOFT - ADAPTION_PROMPT - IA3 - LOHA - LOKR - OFT - XLORA - POLY - LN_TUNING - VERA - FOURIERFT - HRA</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-string_1","title":"Type: <code>string</code>","text":"<p>Possible Values: <code>PROMPT_TUNING</code> or <code>MULTITASK_PROMPT_TUNING</code> or <code>P_TUNING</code> or <code>PREFIX_TUNING</code> or <code>LORA</code> or <code>ADALORA</code> or <code>BOFT</code> or <code>ADAPTION_PROMPT</code> or <code>IA3</code> or <code>LOHA</code> or <code>LOKR</code> or <code>OFT</code> or <code>POLY</code> or <code>LN_TUNING</code> or <code>VERA</code> or <code>FOURIERFT</code> or <code>XLORA</code> or <code>HRA</code> or <code>VBLORA</code></p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#pretrainedpeftconfig","title":"PretrainedPeftConfig","text":"<p>PEFT adapter uses the config and initialisation from a pretrained adapter</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_13","title":"Type: <code>object</code>","text":"Property Type Required Possible values Description peft_type <code>const</code> \u2705 <code>PRETRAINED_PEFT</code> name_or_path <code>string</code> \u2705 string HF ID or path to the pretrained peft"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#runconfig","title":"RunConfig","text":"<p>Experiment running configuration</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_14","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description model <code>string</code> string <code>\"/local_resources/basemodel\"</code> Local path to model to be fine-tuned. Normally this should be <code>/local_resources/basemodel</code> model_args <code>object</code> ModelArguments <code>{\"torch_dtype\": \"auto\", \"device_map\": \"auto\", \"max_memory\": null, \"low_cpu_mem_usage\": false, \"attn_implementation\": null, \"offload_folder\": null, \"offload_state_dict\": null, \"offload_buffers\": null, \"use_cache\": true, \"cache_dir\": null, \"force_download\": false, \"local_files_only\": false, \"proxies\": null, \"resume_download\": false, \"revision\": \"main\", \"code_revision\": \"main\", \"subfolder\": null, \"token\": null, \"use_safetensors\": null, \"variant\": null, \"trust_remote_code\": false}</code> tokenizer <code>string</code> or <code>null</code> string Model HuggingFace ID, or path, or None to use the one associated with the model use_fast_tokenizer <code>boolean</code> boolean <code>True</code> Use the Fast version of the tokenizer. The 'slow' version may be compatible with more features. resume_from_checkpoint <code>boolean</code> or <code>string</code> boolean and/or string <code>False</code> Normally should be set to 'auto' to continue if a checkpoint exists. Can set to <code>True</code> to always try to continue, <code>False</code> to never try, or a path to load from a specific path. final_checkpoint_name <code>string</code> string <code>\"checkpoint-final\"</code> Name of final checkpoint. Should be left as default"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#sftarguments","title":"SFTArguments","text":"<p>Supervised fine-tuning arguments</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_15","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description max_seq_length <code>integer</code> integer <code>2048</code> Maximum length input sequence length. Longer sequences will be filtered out. save_name_if_new_basemodel <code>string</code> string <code>\"checkpoint-new-basemodel\"</code> If a new basemodel is saved, it will be saved with this name train_on_completions_only <code>boolean</code> boolean <code>False</code> Only compute loss on the assistant's turns."},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#silogentrainingarguments","title":"SilogenTrainingArguments","text":"<p>HuggingFace TrainingArguments as Config with additional SiloGen conventions</p> <p>The list of training arguments is best available online (the version might not be up-to-date here): https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments</p> <p>The TrainingArguments object does a lot of things besides specifying the training configuaration options (e.g. it has computed properties like true training batch size etc.)</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#tasktype","title":"TaskType","text":"<p>Enum class for the different types of tasks supported by PEFT.</p> <p>Overview of the supported task types: - SEQ_CLS: Text classification. - SEQ_2_SEQ_LM: Sequence-to-sequence language modeling. - CAUSAL_LM: Causal language modeling. - TOKEN_CLS: Token classification. - QUESTION_ANS: Question answering. - FEATURE_EXTRACTION: Feature extraction. Provides the hidden states which can be used as embeddings or features   for downstream tasks.</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-string_2","title":"Type: <code>string</code>","text":"<p>Possible Values: <code>SEQ_CLS</code> or <code>SEQ_2_SEQ_LM</code> or <code>CAUSAL_LM</code> or <code>TOKEN_CLS</code> or <code>QUESTION_ANS</code> or <code>FEATURE_EXTRACTION</code></p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#weighteddatasetdefinition","title":"WeightedDatasetDefinition","text":"<p>Define a dataset, with a weight for sampling</p>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_16","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description path <code>string</code> \u2705 string Local path to a JSONL file in the finetuning data format sampling_weight <code>number</code> number <code>1.0</code>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#weightedmixdatainput","title":"WeightedMixDataInput","text":"<p>A list of datasets where each is sampled by a certain weight</p> <p>These datasets are interleaved based on the sampling weights. The resulting dataset is fully precomputed, upto the point where every single sample in every dataset gets picked. This means that with small sampling weights, it can take a lot of draws to see every sample from a dataset and so the resulting dataset can be very large.</p> <p>The datasets themselves need to be in the finetuning supported JSONL formats. For SFT this means lines:</p> <pre><code>{\"messages\": {\"content\": \"string\", \"role\": \"string\"}}\n</code></pre> <p>For DPO this means lines of:</p> <pre><code>{\"prompt_messages\": {\"content\": \"string\", \"role\": \"string\"}, \"chosen_messages\": {\"content\": \"string\", \"role\": \"string\"}, \"rejected_messages\": {\"content\": \"string\", \"role\": \"string\"}}\n</code></pre>"},{"location":"ai-workloads/workloads/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_17","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description type <code>const</code> \u2705 <code>PRECOMPUTE_WEIGHTED_MIX</code> datasets <code>array</code> \u2705 WeightedDatasetDefinition data_type <code>string</code> string <code>\"ChatConversation\"</code> generally, the data_type is automatically set based on the experiment config method seed <code>integer</code> integer <code>19851243</code> Seed for the random number generator for interleaving draws"},{"location":"ai-workloads/workloads/llm-inference-llamacpp-mi300x/helm/","title":"LLM Inference Service with Llama.cpp","text":"<p>This Helm chart deploys a LLM inference service workload via llama.cpp</p>"},{"location":"ai-workloads/workloads/llm-inference-llamacpp-mi300x/helm/#prerequisites","title":"Prerequisites","text":"<ol> <li>Helm: Install <code>helm</code>. Refer to the Helm documentation for instructions.</li> <li>Secrets: (Optional) Create the secrets <code>minio-credentials</code> with keys <code>minio-access-key</code> and <code>minio-secret-key</code> in the namespace if you want to download pre-built executables and models from MinIO.</li> </ol>"},{"location":"ai-workloads/workloads/llm-inference-llamacpp-mi300x/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>Basic configurations are defined in the <code>values.yaml</code> file.</p> <p>The default model is 1.73-bit quantized DeepSeek-R1-UD-IQ1_M, which  fits in one MI300X GPU and can serve with a context length of 4K.</p> <p>For example: run the following command within the <code>helm/</code> folder to deploy the service:</p> <pre><code>helm template . --set env_vars.TEMP=\"0.8\" | kubectl apply -f -\n</code></pre> <p>Note: Compiling llama.cpp executables and downloading/merging the GGUF files of DeepSeek R1 (~200GB) from HuggingFace can take a significant amount of time. The deployment process may take over 30 minutes before the LLM inference service is ready.</p>"},{"location":"ai-workloads/workloads/llm-inference-llamacpp-mi300x/helm/#interacting-with-the-deployed-model","title":"Interacting with the Deployed Model","text":""},{"location":"ai-workloads/workloads/llm-inference-llamacpp-mi300x/helm/#verify-deployment","title":"Verify Deployment","text":"<p>Check the deployment and service status:</p> <pre><code>kubectl get deployment\nkubectl get service\n</code></pre>"},{"location":"ai-workloads/workloads/llm-inference-llamacpp-mi300x/helm/#port-forwarding","title":"Port Forwarding","text":"<p>To access the service locally, forward the port using the following commands. This assumes the service name is <code>llm-inference-llamacpp</code>:</p> <pre><code>kubectl port-forward services/llm-inference-llamacpp 8080:80\n</code></pre> <p>You can access the Llama.cpp server's WebUI at <code>http://localhost:8080</code> using a web browser.</p> <p>Additionally, an OpenAI-compatible API endpoint is available at <code>http://localhost:8080/v1</code></p>"},{"location":"ai-workloads/workloads/llm-inference-sglang/helm/","title":"LLM Inference with SGLang","text":"<p>This Helm Chart deploys the LLM Inference SGLang workload.</p>"},{"location":"ai-workloads/workloads/llm-inference-sglang/helm/#prerequisites","title":"Prerequisites","text":"<p>Ensure the following prerequisites are met before deploying any workloads:</p> <ol> <li>Helm: Install <code>helm</code>. Refer to the Helm documentation for instructions.</li> <li>Secrets: Create the following secrets in the namespace:<ul> <li><code>minio-credentials</code> with keys <code>minio-access-key</code> and <code>minio-secret-key</code>.</li> <li><code>hf-token</code> with key <code>hf-token</code>.</li> </ul> </li> </ol>"},{"location":"ai-workloads/workloads/llm-inference-sglang/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>It is recommended to use <code>helm template</code> and pipe the result to <code>kubectl apply</code> , rather than using <code>helm install</code>. Generally, a command looks as follows <pre><code>helm template [optional-release-name] &lt;helm-dir&gt; -f &lt;overrides/xyz.yaml&gt; --set &lt;name&gt;=&lt;value&gt; | kubectl apply -n &lt;namespace&gt; -f -\n</code></pre></p> <p>The chart provides three main ways to deploy models, detailed below.</p>"},{"location":"ai-workloads/workloads/llm-inference-sglang/helm/#alternative-1-deploy-a-specific-model-configuration","title":"Alternative 1: Deploy a Specific Model Configuration","text":"<p>To deploy a specific model along with its settings, use the following command from the <code>helm</code> directory:</p> <pre><code>helm template tiny-llama . -f overrides/models/tinyllama_tinyllama-1.1b-chat-v1.0.yaml | kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads/workloads/llm-inference-sglang/helm/#alternative-2-override-the-model","title":"Alternative 2: Override the Model","text":"<p>You can also override the model on the command line:</p> <pre><code>helm template qwen2-0-5b . --set model=Qwen/Qwen2-0.5B-Instruct | kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads/workloads/llm-inference-sglang/helm/#alternative-3-deploy-a-model-from-bucket-storage","title":"Alternative 3: Deploy a Model from Bucket Storage","text":"<p>If you have downloaded your model to bucket storage, use:</p> <pre><code>helm template qwen2-0-5b . --set model=s3://models/Qwen/Qwen2-0.5B-Instruct | kubectl apply -f -\n</code></pre> <p>The model will be automatically downloaded before starting the inference server.</p>"},{"location":"ai-workloads/workloads/llm-inference-sglang/helm/#user-input-values","title":"User Input Values","text":"<p>Refer to the <code>values.yaml</code> file for the user input values you can provide, along with instructions.</p>"},{"location":"ai-workloads/workloads/llm-inference-sglang/helm/#interacting-with-deployed-model","title":"Interacting with Deployed Model","text":""},{"location":"ai-workloads/workloads/llm-inference-sglang/helm/#verify-deployment","title":"Verify Deployment","text":"<p>Check the deployment status:</p> <pre><code>kubectl get deployment\n</code></pre>"},{"location":"ai-workloads/workloads/llm-inference-sglang/helm/#port-forwarding","title":"Port Forwarding","text":"<p>Forward the port to access the service (assuming the service is named <code>llm-inference-sglang-tiny-llama</code> ):</p> <pre><code>kubectl port-forward deployments/llm-inference-sglang-tiny-llama 8080:80\n</code></pre>"},{"location":"ai-workloads/workloads/llm-inference-sglang/helm/#test-the-deployment","title":"Test the Deployment","text":"<p>Send a test request to verify the service, assuming <code>TinyLlama/TinyLlama-1.1B-Chat-v1.0</code> model:</p> <pre><code>curl http://localhost:8080/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n        ]\n    }'\n</code></pre>"},{"location":"ai-workloads/workloads/llm-inference-vllm/helm/","title":"LLM Inference with vLLM","text":"<p>This Helm Chart deploys the LLM Inference vLLM workload.</p>"},{"location":"ai-workloads/workloads/llm-inference-vllm/helm/#prerequisites","title":"Prerequisites","text":"<p>Ensure the following prerequisites are met before deploying any workloads:</p> <ol> <li>Helm: Install <code>helm</code>. Refer to the Helm documentation for instructions.</li> <li>Secrets: Create the following secrets in the namespace:<ul> <li><code>minio-credentials</code> with keys <code>minio-access-key</code> and <code>minio-secret-key</code>.</li> <li><code>hf-token</code> with key <code>hf-token</code>.</li> </ul> </li> </ol>"},{"location":"ai-workloads/workloads/llm-inference-vllm/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>It is recommended to use <code>helm template</code> and pipe the result to <code>kubectl create</code> , rather than using <code>helm install</code>. Generally, a command looks as follows</p> <pre><code>helm template [optional-release-name] &lt;helm-dir&gt; -f &lt;overrides/xyz.yaml&gt; --set &lt;name&gt;=&lt;value&gt; | kubectl apply -f -\n</code></pre> <p>The chart provides three main ways to deploy models, detailed below.</p>"},{"location":"ai-workloads/workloads/llm-inference-vllm/helm/#alternative-1-deploy-a-specific-model-configuration","title":"Alternative 1: Deploy a Specific Model Configuration","text":"<p>To deploy a specific model along with its settings, use the following command from the <code>helm</code> directory:</p> <pre><code>helm template tiny-llama . -f overrides/models/tinyllama_tinyllama-1.1b-chat-v1.0.yaml | kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads/workloads/llm-inference-vllm/helm/#alternative-2-override-the-model","title":"Alternative 2: Override the Model","text":"<p>You can also override the model on the command line:</p> <pre><code>helm template qwen2-0-5b . --set model=Qwen/Qwen2-0.5B-Instruct | kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads/workloads/llm-inference-vllm/helm/#alternative-3-deploy-a-model-from-bucket-storage","title":"Alternative 3: Deploy a Model from Bucket Storage","text":"<p>If you have downloaded your model to bucket storage, use:</p> <pre><code>helm template qwen2-0-5b . --set model=s3://models/Qwen/Qwen2-0.5B-Instruct | kubectl apply -f -\n</code></pre> <p>The model will be automatically downloaded before starting the inference server.</p>"},{"location":"ai-workloads/workloads/llm-inference-vllm/helm/#user-input-values","title":"User Input Values","text":"<p>Refer to the <code>values.yaml</code> file for the user input values you can provide, along with instructions.</p>"},{"location":"ai-workloads/workloads/llm-inference-vllm/helm/#interacting-with-deployed-model","title":"Interacting with Deployed Model","text":""},{"location":"ai-workloads/workloads/llm-inference-vllm/helm/#verify-deployment","title":"Verify Deployment","text":"<p>Check the deployment status:</p> <pre><code>kubectl get deployment\n</code></pre>"},{"location":"ai-workloads/workloads/llm-inference-vllm/helm/#port-forwarding","title":"Port Forwarding","text":"<p>Forward the port to access the service (assuming the deployment is named <code>llm-inference-vllm-tiny-llama</code> ):</p> <pre><code>kubectl port-forward deployments/llm-inference-vllm-tiny-llama 8080:8080\n</code></pre>"},{"location":"ai-workloads/workloads/llm-inference-vllm/helm/#test-the-deployment","title":"Test the Deployment","text":"<p>Send a test request to verify the service, assuming <code>TinyLlama/TinyLlama-1.1B-Chat-v1.0</code> model:</p> <pre><code>curl http://localhost:8080/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n        ]\n    }'\n</code></pre>"},{"location":"ai-workloads/workloads/llm-inference-vllm-benchmark-mad/helm/","title":"LLM Inference Benchmarking Workload","text":"<p>This Helm chart submits a job to benchmark the performance of vLLM running a model in the same container.</p>"},{"location":"ai-workloads/workloads/llm-inference-vllm-benchmark-mad/helm/#prerequisites","title":"Prerequisites","text":"<ol> <li>Helm: Install <code>helm</code>. Refer to the Helm documentation for instructions.</li> <li> <p>MinIO Storage (optional): To use pre-downloaded model weights from MinIO storage, the following environment variables must be set, otherwise models will be downloaded from HuggingFace. MinIO storage is also used for saving benchmark results.</p> <ul> <li><code>BUCKET_STORAGE_HOST</code></li> <li><code>BUCKET_STORAGE_ACCESS_KEY</code></li> <li><code>BUCKET_STORAGE_SECRET_KEY</code></li> <li><code>BUCKET_MODEL_PATH</code></li> </ul> </li> <li> <p>HF Token (optional): If you need to download gated models from HuggingFace (e.g., Mistral and LLaMA 3.x) that are not available locally, ensure a secret named <code>hf-token</code> exists in the namespace.</p> </li> </ol>"},{"location":"ai-workloads/workloads/llm-inference-vllm-benchmark-mad/helm/#implementation","title":"Implementation","text":"<p>Basic configurations are defined in the <code>values.yaml</code> file, with key settings:</p> <ul> <li><code>env_vars.TESTOPT</code>: Must be set to either \"latency\" or \"throughput\"</li> <li><code>env_vars.USE_MAD</code>: Controls whether to apply the MAD approach (see below)</li> </ul> <p>Note: If the specified model cannot be found locally, the workload will attempt to download it from HuggingFace.</p>"},{"location":"ai-workloads/workloads/llm-inference-vllm-benchmark-mad/helm/#a-scenario-specific-approach","title":"A. Scenario-specific approach","text":"<p>In this approach (<code>env_vars.USE_SCENARIO</code> is not \"false\"), scenarios are defined in the <code>mount/scenarios_{$TESTOPT}.csv</code> file. Modify this file to specify models, parameters, and environment variables for benchmarking. Each column defines a parameter or variable, and each row represents a unique scenario to benchmark.</p> <p>The default configuration benchmarks latency using benchmark_latency.py from vLLM. Setting <code>env_vars.TESTOPT</code> to \"throughput\" will use benchmark_throughput.py instead.</p> <p>Example 1: Benchmark latency scenarios (default) <pre><code>helm template . | kubectl apply -f -\n</code></pre></p> <p>Example 2: Benchmark throughput scenarios <pre><code>helm template . --set env_vars.TESTOPT=\"throughput\" | kubectl apply -f -\n</code></pre></p>"},{"location":"ai-workloads/workloads/llm-inference-vllm-benchmark-mad/helm/#b-rocmmad-standalone-approach","title":"B. ROCm/MAD standalone approach","text":"<p>When <code>env_vars.USE_MAD</code> is not \"false\", the ROCm/MAD repository will be cloned. The specified model (<code>env_vars.MAD_MODEL</code>) will be benchmarked according to preset scripts.</p> <p>Example 3: Benchmark using MAD standalone approach with override settings <pre><code>helm template . -f overrides/methods/MAD-Qwen2.5_0.5B.yaml | kubectl apply -f -\n</code></pre></p>"}]}