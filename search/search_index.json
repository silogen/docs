{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SiloGen - AMD Enterprise AI Platform documentation","text":"<p>Welcome to the documentation for SiloGen - AMD Enterprise AI Platform. Enterprises striving to scale AI often encounter roadblocks that increase costs, slow innovation, and limit impact. The AMD Enterprise AI stack is built to overcome these challenges and unlock the full potential of AI across the enterprise.</p> <p>This is your comprehensive handbook designed to help Infrastructure administrators, AI practitioners and AI resource managers successfully deploy, manage, and run AI workloads on AMD compute. The guide provides step-by-step instructions for installing and configuring the necessary software, as well as practical tutorials and use cases to help you run AI workloads efficiently on a scalable Kubernetes platform.</p> Introduction <ul> <li> SiloGen platform overview </li> <li> Quick start guide </li> <li> Target audience </li> </ul> Infrastructure administrator <ul> <li> Install on-premises environment </li> <li> Single sign-on </li> </ul> AI practitioner <ul> <li> AI development overview </li> <li> Deploy model and run inference </li> <li> Model finetuning </li> <li> Access models </li> </ul> AI resource manager <ul> <li> AI resource overview </li> <li> Getting started </li> <li> Managing clusters </li> <li> Managing users </li> </ul>"},{"location":"cli-tutorials-overview/","title":"CLI tutorials overview","text":"<p>These tutorials are step-by-step guides with practical examples on how to run AI model training, fine-tuning, and inference workloads using the SiloGen platform.</p> <ul> <li>Tutorial 0: Prerequisites for running the tutorials: Shared setup needed for the tutorials, including platform configurations such as secrets and storage.</li> <li>Deliver resources and fine-tune: Learn how to download models and data to cluster MinIO storage, run fine-tuning jobs, and deploy inference services.</li> <li>Language extension: Odia fine-tuning: Learn how to fine-tune a continued pretraining base model to make it an instruction-following model, then deploy models and compare results.</li> </ul>","tags":["developer center","tutorial","model training"]},{"location":"login-to-silogen/","title":"Login to the SiloGen platform","text":"<p>Users log in to the SiloGen platform at their dedicated URL address, for example, <code>your-silogen.ai</code>. Entering the address in your browser's address bar takes you to the login page. The page has a button called \"Sign in with Keycloak,\" which takes you to the authentication service.</p> <p></p> <p>Enter your email address in the specified field and click the \"Sign In\" button to continue. Entering incorrect credentials will result in an error screen.</p> <p>The next screen requires you to enter your password. There is also a link to request a new password in case you have lost the previous one. You can restart the login process by clicking the refresh icon. Clicking the \"Sign In\" button will take you to the application or display an error message if the password is incorrect.</p> <p></p>","tags":["login","keycloak"]},{"location":"login-to-silogen/#keycloak-user-authentication","title":"Keycloak user authentication","text":"<p>The SiloGen platform uses Keycloak to authenticate users. Keycloak is tightly integrated with all SiloGen services, and setting it up is straightforward. It can be federated so that users' credentials can be centrally managed in an enterprise solution and integrated with LDAP or Active Directory.</p>","tags":["login","keycloak"]},{"location":"login-to-silogen/#other-login-measures","title":"Other login measures","text":"<p>The SiloGen platform can be modified to utilize other login methods, such as working in an internal network through a VPN.</p>","tags":["login","keycloak"]},{"location":"personal-preferences/","title":"Personal preferences","text":"<p>SiloGen platform has several settings the users can adjust to change their user experience.</p>","tags":["color theme","preferences"]},{"location":"personal-preferences/#color-theme","title":"Color theme","text":"<p>The SiloGen platform supports light and dark color modes for different tastes.</p> <p>Users can access the color theme toggler by clicking the username in the top right corner of the user interface. A single click in the area activates a dropdown menu containing the color theme switcher. User can change the color theme by clicking the toggle button indicated by a sun or moon icon.</p> <p></p>","tags":["color theme","preferences"]},{"location":"platform-overview/","title":"Overview of the SiloGen Enterprise AI platform","text":"<p>The SiloGen Enterprise AI platform is a full-stack solution for developing, deploying, and running AI workloads on a Kubernetes platform optimized for AMD compute. The platform can be utilized by system administrators, resource managers, AI researchers, and AI solution developers.</p>","tags":["platform","introduction","features"]},{"location":"platform-overview/#key-features-of-the-silogen-platform","title":"Key features of the SiloGen platform","text":"<p>The SiloGen platform offers a variety of features for AI compute use:</p> <p>Optimized GPU utilization &amp; lower operational costs SiloGen ensures high GPU efficiency through intelligent workload placement and dynamic resource sharing. This eliminates waste, reduces costs, and guarantees fair access to compute power\u2014empowering teams to innovate without delay.</p> <p>Unified AI infrastructure The platform consolidates fragmented environments into a cohesive AI ecosystem. With standardized governance, tools, and processes, SiloGen simplifies operations and enables seamless collaboration across teams and business units.</p> <p>Accelerated AI delivery SiloGen streamlines the setup of AI resources and automates dependency management. By enabling reuse of AI assets and prebuilt templates, teams can move faster from experimentation to production\u2014shortening time-to-value.</p> <p>AI-native workload orchestration Unlike traditional schedulers, SiloGen is optimized for AI. It intelligently prioritizes jobs, dynamically allocates resources, and ensures consistent performance, maximizing the utilization of compute infrastructure.</p> <p>With SiloGen, enterprises reduce complexity, accelerate AI deployment, and maximize ROI\u2014turning AI from a cost center into a strategic advantage.</p>","tags":["platform","introduction","features"]},{"location":"platform-overview/#key-components-of-the-silogen-platform","title":"Key components of the SiloGen platform","text":"<p>The SiloGen platform is designed to provide a solid end-to-end model development, evaluation, and inference experience optimized for AMD DC GPUs.</p> <p></p> <p>Developer Center (Dev Center) Enables researchers to manage AI workloads end-to-end. The Developer Center focuses on user experience, offering low-code approaches for developing AI applications by simplifying the execution of fine-tuning, inference, and other jobs.</p> <p>Focusing on user experience, the Developer Center has a comprehensive model catalog, enabling AI researchers to better understand which models, data, and workloads they can use and download for their work. Finally, the Developer Center offers (and aims to expand) integrations with well-established MLOps tools such as MLflow, TensorBoard, and Kubeflow, allowing researchers to use the AI development tools that feel most natural to them.</p> <p>AI Resource Manager (Airman) With Airman, enterprises can manage resource utilization on the R&amp;D compute cluster by mapping user groups to compute, data, and image resources. Airman maximizes GPU usage by allowing projects and user groups to share GPUs and by configuring compute clusters with policies that enable fair and smart scheduling. Administrators can monitor GPU utilization at the project, department, cluster, and enterprise levels using Airman dashboards.</p> <p>Kaiwo (Kubernetes - AI Workload Orchestrator) Kaiwo optimizes GPU resource utilization for AI workloads by minimizing GPU idleness. The workload scheduler increases resource efficiency through intelligent job queueing, fair sharing of resources, guaranteed quotas, and opportunistic gang scheduling. It controls the deployment of AI workloads by implementing a Kubernetes operator that watches for the deployment of AI workloads. Main functions:</p> <ul> <li>Decides where and when workloads will be executed based on compute policies.</li> <li>Supports multiple queues, fair GPU resource sharing, topology-aware scheduling, and other features. Open sourced at https://github.com/silogen/kaiwo.</li> </ul> <p>Kubernetes platform The core orchestration platform for managing containerized applications. Kubernetes is the industry standard for orchestrating containerized applications at scale. It provides the flexibility, scalability, and reliability needed to support enterprise AI workloads\u2014from training machine learning models to serving predictions in production.</p> <p>Cloud Forge A tool built to help enterprises easily deploy a scalable, production-ready AI platform using open-source technologies. Cloud Forge automates the deployment of the control and compute planes onto Kubernetes clusters, integrates prepackaged SiloGen AI workloads, and enables organizations running on AMD hardware to start training and deploying models within just a few hours\u2014essentially offering a streamlined, \"one-click\" setup for enterprise-grade AI infrastructure.</p> <p>AI workloads Reference AI workloads optimized for AMD compute, including model fine-tuning, pretraining, evaluation, and inference workloads. These workloads have been tested and executed on AMD GPUs and are open sourced (https://github.com/silogen/ai-workloads). SiloGen AI workloads provide building blocks for broader collaboration across the AI ecosystem and accelerate the development of AI use cases. The catalog of pre-developed AI workloads is continuously updated.</p>","tags":["platform","introduction","features"]},{"location":"quick-start/","title":"Quick start guide","text":"<p>This quick start guide will help you rapidly install, configure, and begin using the AMD SiloGen Enterprise AI platform for your AI workloads.</p> <p>SiloGen Enterprise AI Platform supports bare-metal, on-premises, and public cloud deployments of AI workloads. SiloGen Enterprise AI currently supports AMD Instinct MI250, MI300, and MI325 GPUs.</p> <p>Note</p> <p>These instructions are intended for installing SiloGen as a self-contained solution in an on-premises environment. For larger-scale, production-level deployments, refer to AMD SiloGen Enterprise AI support services.</p>","tags":["quick start","installation","introduction"]},{"location":"quick-start/#install-the-silogen-platform","title":"Install the SiloGen platform","text":"<p>The target audience for the installation section is infrastructure administrators.</p> <p>Follow the instructions in Install SiloGen platform to install SiloGen in an on-premises environment.</p>","tags":["quick start","installation","introduction"]},{"location":"quick-start/#onboard-organization-projects-and-users","title":"Onboard organization, projects and users","text":"<p>The target audience for this onboarding section is AI resource managers.</p> <p>Follow the instructions in Quick start for AI resource managers to onboard your organization, projects, and users.</p>","tags":["quick start","installation","introduction"]},{"location":"quick-start/#train-and-deploy-models-in-the-developer-center","title":"Train and deploy models in the Developer Center","text":"<p>The target audience for this section is AI practitioners.</p> <p>Follow the instructions in Quick start for AI practitioners to get started with fine-tuning and deploying AI models, and managing AI workloads.</p>","tags":["quick start","installation","introduction"]},{"location":"target-audience/","title":"Target audience of the SiloGen platform","text":"<p>The SiloGen platform and this documentation are designed for three user personas, each focusing on different layers and aspects of the AI technology stack and lifecycle.</p> <ul> <li>Infrastructure administrators: Professionals responsible for setting up, maintaining, and optimizing the infrastructure that supports AI workloads.</li> <li>AI resource managers: Professionals responsible for granting access to compute resources, maintaining projects, users, and optimizing AI resources.</li> <li>AI practitioners: Professionals who train, deploy, and manage AI models and applications in an enterprise environment.</li> </ul> <p>While some familiarity with Kubernetes, containerization, and AI/ML concepts is helpful, this handbook provides detailed instructions and explanations to accommodate users with varying levels of expertise. The handbook guides you to set up a fully deployed SiloGen Enterprise AI platform optimized for AI workloads on AMD compute and provides the knowledge needed to run and manage AI use cases effectively.</p>","tags":["introduction","target audience"]},{"location":"ai-workloads-docs/tutorials/tutorial-01-deliver-resources-and-finetune/","title":"Tutorial 01: Deliver model and data to cluster MinIO, then run finetune","text":"<p>This tutorial shows how to download a model and some data from HuggingFace Hub to a cluster-internal MinIO storage server, and then launch finetuning Jobs that use those resources. The checkpoints are also synced into the same cluster-internal MinIO storage. Finally, an inference workload is spawned to make it possible to discuss with the newly finetuned model. At the end of the tutorial, there are some instructions on changing the model and the data.</p> <p>The finetuning work in this tutorial is meant for demonstration purposes, small enough to be run live. We're starting from Tiny-Llama 1.1B Chat, a small LLM. This is already a chat-finetuned model. We're training it with some additional instruction data in the form of single prompt-and-answer pairs. The prompts in this data were gathered from real human prompts to LLMs, mostly ones that were shared on the now deprecated sharegpt.com site. The answers to those human prompts were generated with the Mistral Large model. So in essence, training on this data makes our model respond more like Mistral Large. And there's another thing that this training accomplishes, which is to change the chat template, meaning the way the input to the model is formatted. More specifically, this adds special tokens that signal the start and end of message. Our experience is that such special tokens make the inference time message end signaling and message formatting a bit more robust.</p>"},{"location":"ai-workloads-docs/tutorials/tutorial-01-deliver-resources-and-finetune/#1-setup","title":"1. Setup","text":"<p>Follow the setup in the tutorial pre-requisites section.</p>"},{"location":"ai-workloads-docs/tutorials/tutorial-01-deliver-resources-and-finetune/#2-run-workloads-to-deliver-data-and-a-model","title":"2. Run workloads to deliver data and a model","text":"<p>We will use the helm charts in <code>workloads/download-huggingface-model-to-bucket/helm</code> and <code>workloads/download-data-to-bucket/helm</code>. We will use them to deliver a Tiny-Llama 1.1B parameter model, and an Argilla single-turn response supervised finetuning dataset, respectively.</p> <p>Our user input files are in <code>workloads/download-huggingface-model-to-bucket/helm/overrides/tutorial-01-tiny-llama-to-minio.yaml</code>, and <code>workloads/download-data-to-bucket/helm/overrides/tutorial-01-argilla-to-minio.yaml</code>. <pre><code>helm template workloads/download-huggingface-model-to-bucket/helm \\\n  --values workloads/download-huggingface-model-to-bucket/helm/overrides/tutorial-01-tiny-llama-to-minio.yaml \\\n  --name-template \"deliver-tiny-llama-model\" \\\n  | kubectl apply -f -\nhelm template workloads/download-data-to-bucket/helm \\\n  --values workloads/download-data-to-bucket/helm/overrides/tutorial-01-argilla-to-minio.yaml \\\n  --name-template \"deliver-argilla-data\" \\\n  | kubectl apply -f -\n</code></pre></p> <p>The logs will show a model staging download and upload for the model delivery workload, and data download, preprocessing, and upload for the data delivery.</p>"},{"location":"ai-workloads-docs/tutorials/tutorial-01-deliver-resources-and-finetune/#3-scaling-finetuning-hyperparameter-tuning-with-parallel-jobs","title":"3. Scaling finetuning: Hyperparameter tuning with parallel Jobs","text":"<p>At the hyperparameter tuning stage, we run many parallel Jobs while varying a hyperparameter to find the best configuration. Here we are going to look for the best rank parameter <code>r</code> for LoRA.</p> <p>To define the finetuning workload, we will use the helm chart in <code>workloads/llm-finetune-silogen-engine/helm</code> . Our user input file is <code>workloads/llm-finetune-silogen-engine/overrides/tutorial-01-finetune-lora.yaml</code> . This also includes the finetuning hyperparameters - you can change them in the file to experiment, or use <code>--set</code> with helm templating to change an individual value.</p> <p>Let's create ten different finetuning jobs to try out different LoRA ranks:</p> <pre><code>run_id=alpha\nfor r in 4 6 8 10 12 16 20 24 32 64; do\n  name=\"tiny-llama-argilla-r-sweep-$run_id-$r\"\n  helm template workloads/llm-finetune-silogen-engine/helm \\\n    --values workloads/llm-finetune-silogen-engine/helm/overrides/tutorial-01-finetune-lora.yaml \\\n    --name-template $name \\\n    --set finetuning_config.peft_conf.peft_kwargs.r=$r \\\n    --set \"checkpointsRemote=default-bucket/experiments/$name\" \\\n    | kubectl apply -f -\ndone\n</code></pre> <p>For each Job we can see logs, a progress bar, and that Job's GPU utilization following the instructions above. If these Jobs get relaunched, they are setup to continue from the existing checkpoints. If we instead want to re-run from scratch, we can just change the <code>run_id</code> variable that is defined before the for loop.</p>"},{"location":"ai-workloads-docs/tutorials/tutorial-01-deliver-resources-and-finetune/#4-scaling-finetuning-multi-gpu-training","title":"4. Scaling finetuning: Multi-GPU training","text":"<p>Beside parallel Jobs, we can also take advantage of multiple GPUs by using them for parallel compute. This can be helpful for more compute demanding Jobs, and necessary with larger models.</p> <p>Let's launch an 8GPU run of full-parameter finetuning:</p> <pre><code>name=\"tiny-llama-argilla-v1\"\nhelm template workloads/llm-finetune-silogen-engine/helm \\\n  --values workloads/llm-finetune-silogen-engine/helm/overrides/tutorial-01-finetune-full-param.yaml \\\n  --name-template $name \\\n  --set \"checkpointsRemote=default-bucket/experiments/$name\" \\\n  --set \"finetuningGpus=8\" \\\n  | kubectl apply -f -\n</code></pre> <p>We can see logs, a progress bar, and the full 8-GPU compute utilization following the instructions above. The training steps of this multi-gpu training run take merely 75 seconds, which reflects the nature of finetuning: fast, iterative, with a focus on flexible experimentation.</p> <p>If we want to compare to an equivalent single-GPU run, we can run:</p> <pre><code>name=\"tiny-llama-argilla-v1-singlegpu\"\nhelm template workloads/llm-finetune-silogen-engine/helm \\\n  --values workloads/llm-finetune-silogen-engine/helm/overrides/tutorial-01-finetune-full-param.yaml \\\n  --name-template $name \\\n  --set \"checkpointsRemote=default-bucket/experiments/$name\" \\\n  --set \"finetuningGpus=1\" \\\n  | kubectl apply -f -\n</code></pre> <p>The training steps for this single-GPU run take around 340 seconds. Thus the full-node training yields a speedup ratio of around 0.22 (4.5x speed). Even higher speedups are achieved in pretraining, which benefits hugely from optimizations.</p>"},{"location":"ai-workloads-docs/tutorials/tutorial-01-deliver-resources-and-finetune/#5-inference-with-a-finetuned-model","title":"5. Inference with a finetuned model","text":"<p>After training the model, we'll want to discuss with it. For this we will use the helm chart in <code>workloads/llm-inference-vllm/helm</code> .</p> <p>Let's deploy the full-parameter finetuned model:</p> <pre><code>name=\"tiny-llama-argilla-v1\"\nhelm template workloads/llm-inference-vllm/helm \\\n  --set \"model=s3://default-bucket/experiments/$name/checkpoint-final\" \\\n  --set \"vllm_engine_args.served_model_name=$name\" \\\n  --name-template \"$name\" \\\n  | kubectl apply -f -\n</code></pre> <p>We can change the <code>name</code> to different experiment names to deploy other models. Note that discussing with the LoRA adapter models with these workloads requires us to merge the final adapter. This can be achieved during finetuning by adding <code>--set mergeAdapter=true</code> and additionally in the deploy command, we have to refer to the merged model, changing the path to <code>--set \"model=s3://default-bucket/experiments/$name/checkpoint-final-merged\"</code> .</p> <p>To discuss with the model, we first need to setup a connection to it. Since this is not a public-internet deployment, we'll do this simply by starting a background port-forwarding process:</p> <pre><code>name=\"tiny-llama-argilla-v1\"\nkubectl port-forward services/llm-inference-vllm-$name 8080:80 &gt;/dev/null &amp;\nportforwardPID=$!\n</code></pre> <p>Now we can discuss with the model, using curl:</p> <pre><code>name=\"tiny-llama-argilla-v1\"\nquestion=\"What are the top five benefits of eating a large breakfast?\"\ncurl http://localhost:8080/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"'$name'\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"'\"$question\"'\"}\n        ]\n    }' | jq \".choices[0].message.content\" --raw-output\n</code></pre> <p>We can test the limits of the model with our own questions. Since this is a model with a relatively limited capacity, its answers are often delightful nonsense.</p> <p>When we want to stop port-forwarding, we can just run: <pre><code>kill $portforwardPID\n</code></pre> and to stop the deployment, we run: <pre><code>name=\"tiny-llama-argilla-v1\"\nkubectl delete deployments/llm-inference-vllm-$name\n</code></pre></p>"},{"location":"ai-workloads-docs/tutorials/tutorial-01-deliver-resources-and-finetune/#next-steps-how-to-use-your-own-model-and-data","title":"Next Steps: How to use your own model and data","text":"<p>This tutorial has shown the basic steps in running finetuning and chatting with the resulting model. For many, the next step may be to use our own models and data. This section should get us started, but ultimately, this opens the whole topic of how to do finetuning, which is too large to cover here. One more comprehensive view point is provided by the T\u00fcl\u00fc 3 paper.</p>"},{"location":"ai-workloads-docs/tutorials/tutorial-01-deliver-resources-and-finetune/#preparing-your-own-model-and-data","title":"Preparing your own model and data","text":"<p>The workload <code>workloads/download-huggingface-model-to-bucket/helm</code> delivers HuggingFace Hub models. To get models from elsewhere, we may for instance do it manually by downloading them to our own computers and uploading to our bucket storage from there. The data delivery workload <code>workloads/download-data-to-bucket/helm</code> uses a free script to download and preprocess the data, so it is more flexible in this regard.</p> <p>The bucket storage used in this tutorial is a MinIO server hosted inside the cluster itself. To use some other S3-compatible bucket storage, we need to change the <code>bucketStorageHost</code> field, add our credentials (HMAC keys) as a Secret in our namespace (this is generally achieved via an External Secret that in turn fetches the info from some secret store that we have access to), and then refer to that bucket storage credentials Secret in the <code>bucketCredentialsSecret</code> nested fields.</p> <p>To prepare our own model, we create a values file that is similar to <code>workloads/download-huggingface-model-to-bucket/helm/overrides/tutorial-01-tiny-llama-to-minio.yaml</code>. . The key field is <code>modelID</code>, which defins which model is downloaded. The field <code>bucketModelPath</code> determines where the model is stored in the bucket storage.</p> <p>To prepare our own data, we structure our values file like <code>workloads/download-data-to-bucket/helm/overrides/tutorial-01-argilla-to-minio.yaml</code>. It may be easiest to write a Python script separately, potentially test it locally, and then put the script as a block text value for <code>dataScript</code>. The dataset upload location is set with the <code>bucketDataDir</code> field.</p>"},{"location":"ai-workloads-docs/tutorials/tutorial-01-deliver-resources-and-finetune/#data","title":"Data","text":"<p>The <code>dataScript</code> is a script instead of just a dataset identifier, because the datasets on HuggingFace hub don't have a standard format that can be always directly passed to our finetuning engine. The data script should format the data into the format that the silogen finetuning engine expects. For supervised finetuning, this is JSON lines, where each line has a JSON dictionary formatted as follows: <pre><code>{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"This is a user message\"},\n    {\"role\": \"assistant\", \"content\": \"The is an assistant answer\"}\n  ]\n}\n</code></pre> There can be an arbitrary number of messages. Additionally, each dictionary can contain a <code>dataset</code> field that has the dataset identifier, and an <code>id</code> field that identifies the data point uniquely. For Direct Preference Optimization, the data format is as follows: <pre><code>{\n  \"prompt_messages\": [\n    {\"role\": \"user\", \"content\": \"This is a user message\"},\n  ],\n  \"chosen_messages\": [\n    {\"role\": \"assistant\", \"content\": \"This is a preferred answer\"}\n  ],\n  \"rejected_messages\": [\n    {\"role\": \"assistant\", \"content\": \"This is a rejected answer\"}\n  ]\n}\n</code></pre></p> <p>The JSON lines output of the data script should be saved to the <code>/downloads/datasets/</code>. This is easy with the approach taken in the tutorial file: <pre><code>dataset.to_json(\"/downloads/datasets/&lt;name of your dataset file.jsonl&gt;\")\n</code></pre> The dataset is uploaded to the directory pointed to by <code>bucketDataDir</code>, with the same filename as it had under <code>/downloads/datasets</code>.</p>"},{"location":"ai-workloads-docs/tutorials/tutorial-01-deliver-resources-and-finetune/#model","title":"Model","text":"<p>Preparing a model is simple than data. We simply set the <code>modelID</code> to the HuggingFace Hub ID of the model (in the <code>Organization/ModelName</code> format). The model is the uploaded to the path pointed to by <code>bucketModelPath</code>.</p>"},{"location":"ai-workloads-docs/tutorials/tutorial-01-deliver-resources-and-finetune/#setting-finetuning-parameters","title":"Setting finetuning parameters","text":"<p>For finetuning, we create a values file that is similar to <code>workloads/llm-finetune-silogen-engine/helm/overrides/tutorial-01-finetune-lora.yaml</code> (for LoRA adapter training) or <code>workloads/llm-finetune-silogen-engine/overrides/tutorial-01-finetune-full-param.yaml</code> (for full parameter training). We'll want to inject our data in the field: <pre><code>finetuning_config:\n  data_conf:\n    training_data:\n      datasets:\n        - path: \"bucketName/path/to/file.jsonl\n</code></pre> The datasets array can contain any number of datasets, and they're concatenated in training.</p> <p>The model is set in the top level field <code>basemodel</code>, where the value should be a name of a bucket followed by the path to the model directory in the bucket, formatted like: <pre><code>basemodel: bucketName/path/to/modelDir\n</code></pre></p> <p>All finetuning configurations are not sensible with all models, and some settings might even fail for unsupported models. Ultimately we need to understand the particular model we're using to set the parameters correctly. Suitable hyperparameters also depend on the data.</p> <p>One key model compatibility parameter to look at is the chat template, which is set by <pre><code>finetuning_config:\n  data_conf:\n    chat_template_name: \"&lt;name of template&gt;\"\n</code></pre> If the model we start from already has a chat template, we should usually set this to <code>\"keep-original\"</code>. Otherwise, <code>\"chat-ml\"</code> is usually a reasonable choice. Another set of parameters that often needs to be changed between models is the set of PEFT target layers, if doing LoRA training. These are set in the following field: <pre><code>finetuning_config:\n  peft_conf:\n    peft_kwargs:\n      target_modules:\n        - q_proj\n        - k_proj\n        - v_proj\n        - o_proj\n        - up_proj\n        - down_proj\n</code></pre> One setting that can be used is <pre><code>finetuning_config:\n  peft_conf:\n    peft_kwargs:\n      target_modules: \"all-linear\"\n</code></pre> which targets all linear layers on the model and doesn't require knowing the names of the layers.</p>"},{"location":"ai-workloads-docs/tutorials/tutorial-02-language-extension-finetune/","title":"Finetuning: adding language-capabilities to a model","text":"<p>Adding a new language to model usually follows at least two stages: first, continued pretraining to build understanding and basic capacity to generate that language, and second, finetuning to make the model e.g. follow instructions. This tutorial handles that second stage of instruction-tuning for Odia language.</p> <p>The original basemodel is Qwen1.5-7B. The first stage of continued pretraining to add general Odia understanding and generation abilities to Qwen have already been done by the OdiaGenAI organization. That continued pretrained model is available openly here. A relevant comparison point for this tutorial is the chat-finetuned version of the Qwen1.5 basemodel, which should have the capability to follow instructions, but is not specifically meant for Odia.</p> <p>Note that access to the Odia continued pretraining version of the Qwen model requires signing the request on Huggingface. This also means that for downloading the model, we'll need to use a HuggingFace access token. See instructions here. \u26a0\ufe0f Note: This tutorial does not add the HF Token to the cluster yet. You need to add it yourself. \u26a0\ufe0f</p> <p>The tutorial includes cluster setup, model and data downloads, finetuning, and finally inference. We should start with a working cluster, setup by a cluster administrator using Cluster-forge. The access to that cluster is provided with a suitable Kubeconfig file.</p>"},{"location":"ai-workloads-docs/tutorials/tutorial-02-language-extension-finetune/#1-setup","title":"1: Setup","text":"<p>Follow the setup in the tutorial pre-requisites section.</p> <p>\u26a0\ufe0f WARNING: This tutorial does not handle adding the HF Token to the cluster yet. Coming soon. Before then, to run this tutorial, you are responsible for adding your HF Token as a secret called <code>hf-token</code> with the key <code>hf-token</code> in the <code>silo-tutorial</code> namespace. \u26a0\ufe0f</p>"},{"location":"ai-workloads-docs/tutorials/tutorial-02-language-extension-finetune/#2-fetch-data-and-models","title":"2. Fetch data and models","text":"<p>First we'll fetch the model and data for the finetuning. We will use the helm charts in <code>workloads/download-huggingface-model-to-bucket/helm</code> and <code>workloads/download-data-to-bucket/helm</code>. We will download a Qwen1.5 7B-parameter model, one English instruction dataset and five different Odia-language single-turn instruction finetuning datasets. These datasets cover slightly different areas, including open instructions, context-based question-answering, translation, and identity answers. The identity answers aim to make our model call itself Olive, and tell that it is from the OdiaGenAI project. Our user input files are in <code>workloads/download-huggingface-model-to-bucket/helm/overrides/tutorial-02-qwen-odia.yaml</code> and <code>workloads/download-data-to-bucket/helm/overrides/tutorial-02-odia-data.yaml</code></p> <pre><code>helm template workloads/download-huggingface-model-to-bucket/helm \\\n  --values workloads/download-huggingface-model-to-bucket/helm/overrides/tutorial-02-qwen-odia.yaml \\\n  --name-template \"download-odia-qwen-odia\" \\\n  | kubectl apply -f -\nhelm template workloads/download-data-to-bucket/helm \\\n  --values workloads/download-data-to-bucket/helm/overrides/tutorial-02-odia-data.yaml \\\n  --name-template \"download-odia-data\" \\\n  | kubectl apply -f -\n</code></pre> <p>The logs will show a model staging download and upload, then data download, preprocessing, and upload.</p>"},{"location":"ai-workloads-docs/tutorials/tutorial-02-language-extension-finetune/#3-finetune-an-odia-model-on-8gpus","title":"3. Finetune an Odia model on 8GPUs","text":"<p>We will run our Odia language finetuning using 8 GPUs in a data parallel setup. For finetuning, we'll use the helm chart in  <code>workloads/llm-finetune-silogen-engine/helm</code>, and the user input file in <code>workloads/llm-finetune-silogen-engine/helm/overrides/tutorial-02-qwen-odia-instruct-v1.yaml</code>.</p> <p>This training job takes around 13 hours, because the combination of 6 datasets is large and we go through that combination 6 times. We have found that a large number of training steps is necessary to enable the model to learn act reasonably in Odia language. We suspect this has something to do with the Odia script, which is not well covered by the Qwen tokenizer, and leads to very long sequences produced character or even diacritic at a time.</p> <p>Let's launch the finetuning Job: <pre><code>name=\"qwen-odia-instruct-v1\"\nhelm template workloads/llm-finetune-silogen-engine/helm \\\n  --values workloads/llm-finetune-silogen-engine/helm/overrides/tutorial-02-qwen-odia-instruct-v1.yaml \\\n  --name-template $name \\\n  --set \"checkpointsRemote=default-bucket/experiments/$name\" \\\n  --set \"finetuningGpus=8\" \\\n  | kubectl apply -f -\n</code></pre> can see logs, a progress bar, and the full 8-GPU compute utilization following the instructions above.</p>"},{"location":"ai-workloads-docs/tutorials/tutorial-02-language-extension-finetune/#4-compare-the-official-qwen15-7b-chat-model-the-odia-continued-pretraining-basemodel-and-the-odia-finetuned-model","title":"4. Compare the official Qwen1.5-7B-Chat model, the Odia continued pretraining basemodel, and the Odia-finetuned model:","text":"<p>The Qwen1.5-7B-Chat is a general chat-finetuned version of the same Qwen basemodel that our Odia continued pretraining started from. Thus it is a good point of comparison. Additionally we'll deploy the Odia continued pretraining basemodel, to see how the instruct-finetuning changed it.</p> <p>For inference deployments, we will use the helm chart in  <code>workloads/llm-inference-vllm/helm/chart</code>.</p> <p>Deploy the models with the following commands. Note that the Qwen1.5-7B-Chat model we're getting directly from HuggingFace, while the other two models we're fetching from the cluster internal bucket storage. <pre><code>name=\"qwen-base-chat\"\nhelm template workloads/llm-inference-vllm/helm \\\n  --set \"model=Qwen/Qwen1.5-7B-Chat\" \\\n  --set \"vllm_engine_args.served_model_name=$name\" \\\n  --name-template \"$name\" \\\n  | kubectl create -f -\nname=\"qwen-odia-base\"\nhelm template workloads/llm-inference-vllm/helm \\\n  --set \"model=s3://default-bucket/models/OdiaGenAI/LLM_qwen_1.5_odia_7b\" \\\n  --set \"vllm_engine_args.served_model_name=$name\" \\\n  --name-template \"$name\" \\\n  | kubectl create -f -\nname=\"qwen-odia-instruct-v1\"\nhelm template workloads/llm-inference-vllm/helm \\\n  --set \"model=s3://default-bucket/experiments/$name/checkpoint-final\" \\\n  --set \"vllm_engine_args.served_model_name=$name\" \\\n  --name-template \"$name\" \\\n  | kubectl create -f -\n</code></pre></p> <p>To discuss with the models, we need to setup connections to them. Since these are not public-internet deployments, we'll do this simply by starting background port-forwarding processes: <pre><code>name=\"qwen-base-chat\"\nkubectl port-forward services/llm-inference-vllm-$name 8080:80 &gt;/dev/null &amp;\nqwenchatPID=$!\nname=\"qwen-odia-base\"\nkubectl port-forward services/llm-inference-vllm-$name 8090:80 &gt;/dev/null &amp;\nodiabasePID=$!\nname=\"qwen-odia-instruct-v1\"\nkubectl port-forward services/llm-inference-vllm-$name 8100:80 &gt;/dev/null &amp;\nodiainstructPID=$!\n</code></pre></p> <p>Now we can talk to the models. We'll ask them about the difference between physics and chemistry, in Odia: <pre><code>question=\"\u0b2a\u0b26\u0b3e\u0b30\u0b4d\u0b25 \u0b2c\u0b3f\u0b1c\u0b4d\u0b1e\u0b3e\u0b28 \u0b0f\u0b2c\u0b02 \u0b30\u0b38\u0b3e\u0b5f\u0b28 \u0b2c\u0b3f\u0b1c\u0b4d\u0b1e\u0b3e\u0b28 \u0b2e\u0b27\u0b4d\u0b5f\u0b30\u0b47 \u0b15\u2019\u0b23 \u0b2a\u0b3e\u0b30\u0b4d\u0b25\u0b15\u0b4d\u0b5f \u0b05\u0b1b\u0b3f?\"\ntemperature=0.0\nmax_tokens=2048\npresence_penalty=1.2\nmin_p=0.05\necho -e \"\\n\\nQwen 1.5 7B Chat:\"\nname=\"qwen-base-chat\"\ncurl http://localhost:8080/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"'$name'\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"'\"$question\"'\"}\n        ],\n        \"temperature\": '$temperature',\n        \"max_tokens\": '$max_tokens',\n        \"presence_penalty\": '$presence_penalty',\n        \"min_p\": '$min_p'\n    }' 2&gt;/dev/null | jq \".choices[0].message.content\" --raw-output | fold -s | sed 's/^/  /'\necho -e \"\\n\\nQwen 1.5 7B Odia-CPT Base:\"\nname=\"qwen-odia-base\"\ncurl http://localhost:8090/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"'$name'\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"'\"$question\"'\"}\n        ],\n        \"temperature\": '$temperature',\n        \"max_tokens\": '$max_tokens',\n        \"presence_penalty\": '$presence_penalty',\n        \"min_p\": '$min_p'\n    }' 2&gt;/dev/null | jq \".choices[0].message.content\" --raw-output | fold -s | sed 's/^/  /'\necho -e \"\\n\\nQwen 1.5 Odia-CPT Instruction-tuned model v1:\"\nname=\"qwen-odia-instruct-v1\"\ncurl http://localhost:8100/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"'$name'\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"'\"$question\"'\"}\n        ],\n        \"temperature\": '$temperature',\n        \"max_tokens\": '$max_tokens',\n        \"presence_penalty\": '$presence_penalty',\n        \"min_p\": '$min_p'\n    }' 2&gt;/dev/null | jq \".choices[0].message.content\" --raw-output | fold -s | sed 's/^/  /'\n</code></pre> We'll find that the general Qwen chat-model does not keep to Odia, it easily switches to e.g. Hindi or Chinese. The Odia continued pretraining basemodel does answer in Odia, but since it is just a continuation model, the text it produces is not really answer to the question, but rather rambling. The finetuned model answers the question in Odia.</p> <p>When we're done chatting with the models, we can kill the port-forwards with: <pre><code>kill $qwenchatPID $odiabasePID $odiainstructPID\n</code></pre> and we can shut down the inference deployments with: <pre><code>for name in qwen-base-chat qwen-odia-base qwen-odia-instruct-v1; do\n  kubectl delete deployment llm-inference-vllm-$name\ndone\n</code></pre></p>"},{"location":"ai-workloads-docs/tutorials/tutorial-prereqs/","title":"Tutorial 0: Prerequisites for running the tutorials","text":""},{"location":"ai-workloads-docs/tutorials/tutorial-prereqs/#required-program-installs","title":"Required program installs","text":"<p>Programs that are used in this tutorial:</p> <ul> <li> <p>kubectl</p> </li> <li> <p>helm</p> </li> <li> <p>k9s</p> </li> <li> <p>jq</p> </li> <li> <p>curl</p> </li> </ul> <p>At least curl and often jq too are commonly installed in many distributions out of the box.</p>"},{"location":"ai-workloads-docs/tutorials/tutorial-prereqs/#namespace-setup","title":"Namespace setup","text":"<p>In order to run the workloads you need to have a Kubernetes cluster with sufficient resources configured. This includes storage, secrets, namespace and HuggingFace token. These should be installed and configured as part of the installation process, but if this is not the case you can use the following command to set up these. The command does the following:</p> <ul> <li> <p>Adds a namespace, where we will conduct all our work. We will setup the <code>silo-tutorial</code> namespace, but this also works in the default namespace in your cluster.</p> </li> <li> <p>Adds an External Secret to get the credentials to access the MinIO storage from our namespace.</p> <ul> <li>This depends on a ClusterSecretStore called <code>k8s-secret-store</code> being already setup by a cluster admin, and the MinIO API credentials being secret there. The cluster should have these by default.</li> </ul> </li> <li> <p>Adds a LocalQueue so that our Jobs schedule intelligently.</p> <ul> <li>This references the ClusterQueue <code>kaiwo</code> which should already be setup by a cluster admin.</li> </ul> </li> </ul> <p>We will use the helm chart in <code>workloads/k8s-namespace-setup/helm</code> and the overrides in <code>workloads/k8s-namespace-setup/helm/overrides/</code>.</p> <p>This first creates a new namespace and sets the current context to use it from now on: <pre><code>kubectl create namespace \"silo-tutorial\"\nkubectl config set-context --current --namespace silo-tutorial\nhelm template workloads/k8s-namespace-setup/helm \\\n  --values workloads/k8s-namespace-setup/helm/overrides/tutorial-prereqs-local-queue.yaml \\\n  --values workloads/k8s-namespace-setup/helm/overrides/tutorial-prereqs-storage-access-external-secret.yaml \\\n  | kubectl apply -f -\n</code></pre></p> <ul> <li>HuggingFace token: In addition to running the command above you also need to add your HF Token as a secret called <code>hf-token</code> with the key <code>hf-token</code> in your namespace.</li> </ul>"},{"location":"ai-workloads-docs/tutorials/tutorial-prereqs/#monitoring-progress-logs-and-gpu-utilization-with-k9s","title":"Monitoring progress, logs, and GPU utilization with k9s","text":"<p>We're interested to see a progress bar of the finetuning training, seeing any messages that a workload logs, and we also want to verify that our GPU Jobs are consuming our compute relatively effectively. This information can be fetched from our Kubernetes cluster in many ways, but one convenient and recommended way us using k9s. We recommend the official documentation for more thorough guidance, but this section shows some basic commands to get what we want here.</p> <p>To get right to the Jobs view in the namespace we're using in this walk-through, we can run:</p> <pre><code>k9s --command Jobs\n</code></pre> <p>Choose a Job using <code>arrow keys</code> and <code>Enter</code> to see the Pod that it spawned, then <code>Enter</code> again to see the Container in the Pod. From here, we can do three things:</p> <ul> <li> <p>Look at the logs by pressing <code>l</code>. The logs show any output messages produced during the workload runtime.</p> </li> <li> <p>Attach to the output of the container by pressing <code>a</code>. This is particularly useful to see the interactive progress bar of a finetuning run.</p> </li> <li> <p>Spawn a shell inside the container by pressing <code>s</code>. Inside the shell we can run <code>watch -n0.5 rocm-smi</code> to get a view of the GPU utilization that updates every 0.5s.</p> </li> </ul> <p>Return from any regular <code>k9s</code> view with <code>Esc</code> .</p>"},{"location":"ai-workloads-manifests/","title":"Workloads","text":"<p>This directory contains all the workloads. Each workload has its own directory, with different formats defined in subdirectories.</p> <p>To create a new workload, you can duplicate an existing workload and adapt as needed.</p> <p>The files needed to create different instantiations of a workload depends on the format. For reference, see the existing workloads, and the documentation of the respective formats.</p> <p>For an introduction to AI workloads see the Workloads overview</p>"},{"location":"ai-workloads-manifests/workloads-overview/","title":"Working with AI workloads on Kubernetes","text":""},{"location":"ai-workloads-manifests/workloads-overview/#overview","title":"Overview","text":"<p>This document provides a general introduction to deploying AI workloads on Kubernetes using Helm charts and Kubernetes manifests. While each specific AI workload in our solution has its own dedicated documentation, this guide explains the common concepts, structure, and deployment patterns that apply across all our AI workload Helm charts.</p> <p>Each AI workload is defined as a separate Helm chart that can be submitted to run on a Kubernetes platform.</p>"},{"location":"ai-workloads-manifests/workloads-overview/#what-is-helm","title":"What is Helm?","text":"<p>Helm is the package manager for Kubernetes, often referred to as \"the apt/yum for K8s.\" It simplifies the deployment and management of applications by:</p> <ul> <li>Packaging Kubernetes resources into reusable charts (pre-configured templates).</li> <li>Managing dependencies, versions, and configurations.</li> <li>Supporting easy upgrades, rollbacks, and customization.</li> </ul> <p>Think of it as a blueprint that defines how an application should be installed and run in a Kubernetes cluster.</p>"},{"location":"ai-workloads-manifests/workloads-overview/#how-does-helm-relate-to-kubernetes-manifests","title":"How does Helm relate to Kubernetes manifests?","text":"<p>Helm doesn't replace manifests - it generates them dynamically using: - Templates: Manifest files with variables (in templates/ directory) - Values: Configuration that fills the template variables</p>"},{"location":"ai-workloads-manifests/workloads-overview/#why-use-helm-for-ai-workloads","title":"Why Use Helm for AI Workloads?","text":"<p>Deploying AI/ML workloads (inference, training, batch processing) on Kubernetes can be complex due to:</p> <ul> <li>Dependencies (models, datasets, GPU drivers).</li> <li>Configuration variability (resource limits, scaling, model versions).</li> <li>Reproducibility (consistent deployments across dev/test/prod).</li> </ul> <p>Helm addresses these challenges by:</p> <ol> <li>Standardizing Deployments:</li> <li> <p>AI components (model servers, preprocessing, monitoring) are defined in a Helm chart (deployment.yaml, service.yaml, etc.).</p> </li> <li> <p>Managing Configurations:</p> </li> <li> <p>values.yaml centralizes tunable parameters (e.g., replicaCount, modelPath, GPU limits). Overrides allow specifying specific models, datasets or environment-specific setups (e.g., dev vs. prod):</p> </li> <li> <p>Supporting AI-Specific Needs</p> </li> <li>GPU/accelerator support: Define resource requests in values.yaml.</li> <li>Model storage: Mount PersistentVolumes or download models at runtime.</li> <li>Scaling: Pre-configure Horizontal Pod Autoscaler (HPA) for inference workloads.</li> </ol>"},{"location":"ai-workloads-manifests/workloads-overview/#structure-of-the-workloads","title":"Structure of the workloads","text":"<pre><code>&lt;workload-name&gt;/\n\u251c\u2500\u2500 Chart.yaml          # Metadata about the chart, such as the release name.\n\u251c\u2500\u2500 values.yaml         # Default configuration values for the workload.\n\u251c\u2500\u2500 templates/          # Kubernetes manifest templates that Helm uses to generate actual manifests\n\u2502   \u251c\u2500\u2500 deployment.yaml # Configuration of the AI workload deployment\n\u2502   \u251c\u2500\u2500 service.yaml    # Configuration of the Kubernetes service\n\u2514\u2500\u2500 overrides           # Customization of the AI workload without modifying the original chart\n</code></pre>"},{"location":"ai-workloads-manifests/workloads-overview/#understanding-the-valuesyaml-file","title":"Understanding the values.yaml file","text":"<p>The values.yaml file is a YAML-formatted configuration file that contains key-value pairs representing the parameters required for your model inference deployment. These parameters include: - Image name: name of the docker image to use</p> <ul> <li> <p>Resources: Define GPU, CPU and memory requirements for your deployment.</p> </li> <li> <p>Model: Provide the name of the model.</p> </li> <li> <p>Storage details</p> </li> <li> <p>Environment Variables: Set environment-specific configurations, such as secrets and storage host location.</p> </li> <li> <p>Service Configuration: Customize service settings like ports, timeouts, and logging levels.</p> </li> </ul>"},{"location":"ai-workloads-manifests/workloads-overview/#overrides","title":"Overrides","text":"<p>Overrides allow customization of the AI workload without modifying the original chart. This includes changing the model and data sets.</p>"},{"location":"ai-workloads-manifests/workloads-overview/#running-ai-workloads-on-k8s","title":"Running AI workloads on k8s","text":"<p>It is recommended to use <code>helm template</code> and pipe the result to <code>kubectl create</code> , rather than using <code>helm install</code>. Generally, a command looks as follows</p> <p><pre><code>helm template &lt;release-name&gt; . -f &lt;override.yaml&gt; | kubectl apply -f -\n</code></pre> or <pre><code>helm template &lt;release-name&gt; . --set &lt;parameter&gt;=&lt;value&gt; | kubectl apply -f -\n</code></pre></p>"},{"location":"ai-workloads-manifests/workloads-overview/#how-to-use-overrides-to-customize-the-workload","title":"How to use overrides to customize the workload","text":"<p>There are multiple options you can use to customize the workload by applying overrides.</p> <p>Alternative 1: Deploy a Specific Model Configuration</p> <p>To deploy a specific model along with its settings, use the following command from the <code>helm</code> directory:</p> <pre><code>helm template tiny-llama . -f overrides/models/tinyllama_tinyllama-1.1b-chat-v1.0.yaml | kubectl apply -f -\n</code></pre> <p>Alternative 2: Override the Model</p> <p>You can also override the model on the command line:</p> <pre><code>helm template qwen2-0-5b . --set model=Qwen/Qwen2-0.5B-Instruct | kubectl apply -f -\n</code></pre> <p>Helm merges overrides in this order (last takes precedence):</p> <ol> <li>values.yaml (in chart)</li> <li>-f files (in command order)</li> <li>--set arguments</li> </ol>"},{"location":"ai-workloads-manifests/workloads-overview/#verifying-the-deployment","title":"Verifying the Deployment","text":"<p>Check pods <pre><code>kubectl get pods -n &lt;namespace&gt;\n</code></pre> Check services <pre><code>kubectl get svc -n &lt;namespace&gt;\n</code></pre> View logs (for a specific pod) <pre><code>kubectl logs -f &lt;pod-name&gt; -n &lt;namespace&gt;\n</code></pre></p>"},{"location":"ai-workloads-manifests/workloads-overview/#monitoring-progress-logs-and-gpu-utilization-with-k9s","title":"Monitoring progress, logs, and GPU utilization with k9s","text":"<p>We're interested to see a progress bar of the finetuning training, seeing any messages that a workload logs, and we also want to verify that our GPU Jobs are consuming our compute relatively effectively. This information can be fetched from our Kubernetes cluster in many ways, but one convenient and recommended way us using k9s. We recommend the official documentation for more thorough guidance, but this section shows some basic commands to get what we want here.</p> <p>To get right to the Jobs view in the namespace <code>&lt;namespace&gt;</code>, we can run:</p> <pre><code>k9s --namespace &lt;namespace&gt; --command Jobs\n</code></pre> <p>Choose a Job using <code>arrow keys</code> and <code>Enter</code> to see the Pod that it spawned, then <code>Enter</code> again to see the Container in the Pod. From here, we can do three things:</p> <ul> <li> <p>Look at the logs by pressing <code>l</code>. The logs show any output messages produced during the workload runtime.</p> </li> <li> <p>Attach to the output of the container by pressing <code>a</code>. This is particularly useful to see the interactive progress bar of a finetuning run.</p> </li> <li> <p>Spawn a shell inside the container by pressing <code>s</code>. Inside the shell we can run <code>watch -n0.5 rocm-smi</code> to get a view of the GPU utilization that updates every 0.5s.</p> </li> </ul> <p>Return from any regular <code>k9s</code> view with <code>Esc</code> .</p>"},{"location":"ai-workloads-manifests/workloads-overview/#editing-reusing-workloads","title":"Editing &amp; reusing workloads","text":"<p>To create a new workload, you can duplicate an existing workload and adapt as needed.</p> <p>Example: Using your own model and data in the workloads</p>"},{"location":"ai-workloads-manifests/dev-chatui-aiaio/helm/","title":"dev-chatui-aiaio Helm Chart","text":"<p>This Helm chart is used to deploy dev-chatui-aiaio workload, i.e. the aiaio Chat UI. It is a simple application, making it a good example of how to package an application as a workload.</p>"},{"location":"ai-workloads-manifests/dev-chatui-aiaio/helm/#installation","title":"Installation","text":"<p>To install the chart with the release name <code>my-release</code>:</p> <pre><code>helm template my-release ./helm | kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads-manifests/dev-chatui-aiaio/helm/#configuration","title":"Configuration","text":"<p>The following table lists the configurable parameters of the <code>dev-chatui-aiaio</code> chart and their default values.</p> Parameter Description Default <code>image</code> Image repository <code>ghcr.io/silogen/aiaio:v20250221</code> <code>image.pullPolicy</code> Image pull policy <code>Always</code> <code>env_vars</code> Environment variables <code>{}</code> <code>storage.ephemeral.quantity</code> Ephemeral storage quantity <code>1Gi</code> <code>storage.ephemeral.storageClassName</code> Storage class name <code>mlstorage</code> <code>storage.ephemeral.accessModes</code> Access modes <code>[ ReadWriteOnce ]</code> <code>storage.dshm.sizeLimit</code> Shared memory size limit <code>1Gi</code> <code>deployment.port</code> Deployment port <code>9000</code> <code>entrypoint</code> Entrypoint command <code>\"\"</code>"},{"location":"ai-workloads-manifests/dev-chatui-aiaio/helm/#example","title":"Example","text":"<p>To deploy the chart with a custom values file <code>values_override.yaml</code>:</p> <pre><code>helm template my-release ./helm -f values_override.yaml | kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads-manifests/dev-chatui-openwebui/helm/","title":"Open WebUI for LLM Services","text":"<p>This Helm Chart deploys a WebUI for aggregating deployed LLM services within the cluster.</p>"},{"location":"ai-workloads-manifests/dev-chatui-openwebui/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>The basic configurations for deployment are defined in the <code>values.yaml</code> file.</p> <p>To deploy the service, execute the following command from the Helm folder:</p> <pre><code>helm template &lt;release-name&gt; . | kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads-manifests/dev-chatui-openwebui/helm/#automatic-discovery-and-health-checks-for-llm-services","title":"Automatic Discovery and Health Checks for LLM Services","text":"<p>OpenAI-compatible endpoints can by specified by the user through the <code>env_vars.OPENAI_API_BASE_URLS</code> environment variable. Additionally, service discovery is used to include all OpenAI-compatible LLM inference services running in the same namespace.</p>"},{"location":"ai-workloads-manifests/dev-chatui-openwebui/helm/#client-side-service-discovery-optional","title":"Client-Side Service Discovery (Optional)","text":"<p>Client-side discovery can be performed using the <code>--dry-run=server</code> flag:</p> <pre><code>helm template &lt;release-name&gt; . --dry-run=server | kubectl apply -f -\n</code></pre> <p>For a service to be included in <code>OPENAI_API_BASE_URLS_AUTODISCOVERY</code> during client-side discovery: - The service must be running in the same namespace. - The service name must start with <code>llm-inference-</code>.</p>"},{"location":"ai-workloads-manifests/dev-chatui-openwebui/helm/#server-side-service-discovery","title":"Server-Side Service Discovery","text":"<p>The system performs server-side discovery of LLM inference services automatically. For a service to be included, the following conditions must be met: - The service must be running in the same namespace. - The service name must start with <code>llm-inference-</code>. - The pod's service account must have the necessary permissions to check running services in the namespace (configured via role-binding).</p>"},{"location":"ai-workloads-manifests/dev-chatui-openwebui/helm/#health-checks-and-filtering","title":"Health Checks and Filtering","text":"<p>Before finalizing <code>OPENAI_API_BASE_URLS</code> and starting the service, the URLs specified by the user and the auto-discovered services are merged, and filtered based on a health check.</p> <p>For a service to be included in the final <code>OPENAI_API_BASE_URLS</code>: - The service must respond successfully to the <code>/v1/models</code> endpoint with an HTTP status code of 200.</p> <p>The final <code>OPENAI_API_BASE_URLS</code> determines what services/models are included in Open WebUI interface.</p>"},{"location":"ai-workloads-manifests/dev-text2image-comfyui/helm/","title":"ComfyUI Text-to-Image/Video Workload","text":"<p>This Helm Chart deploys a ComfyUI web app for text-to-image/video generation.</p>"},{"location":"ai-workloads-manifests/dev-text2image-comfyui/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>Basic configurations are defined in the <code>values.yaml</code> file.</p> <p>To deploy the service, run the following command within the <code>helm</code> folder:</p> <pre><code>helm template . | kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads-manifests/dev-text2image-comfyui/helm/#interacting-with-the-workload","title":"Interacting with the Workload","text":""},{"location":"ai-workloads-manifests/dev-text2image-comfyui/helm/#verify-deployment","title":"Verify Deployment","text":"<p>Check the deployment and service status:</p> <pre><code>kubectl get deployment\nkubectl get service\n</code></pre>"},{"location":"ai-workloads-manifests/dev-text2image-comfyui/helm/#port-forwarding","title":"Port Forwarding","text":"<p>To access the service locally on port <code>8188</code>, forward the port of the service/deployment using the following commands. This assumes the service name is <code>dev-text2image-comfyui</code>:</p> <p>The service exposes HTTP on port 80 (the deployment uses port 8188 by default).</p> <pre><code>kubectl port-forward services/dev-text2image-comfyui 8188:80\n</code></pre> <p>Now, you can access ComfyUI (Manager also included) at http://localhost:8188 using a web browser.</p>"},{"location":"ai-workloads-manifests/dev-workspace-jupyterlab/helm/","title":"JupyterLab Workload","text":"<p>This workload deploys a basic JupyterLab environment on top of any image with Python (pip) pre-installed. It is ideal for interactive development sessions and experimentation with other workloads.</p>"},{"location":"ai-workloads-manifests/dev-workspace-jupyterlab/helm/#configuration-parameters","title":"Configuration Parameters","text":"<p>You can configure the following parameters in the <code>values.yaml</code> file or override them via the command line:</p> Parameter Description Default <code>image</code> Container image repository and tag <code>rocm/pytorch:rocm6.4_ubuntu24.04_py3.12_pytorch_release_2.6.0</code> <code>imagePullPolicy</code> Image pull policy <code>Always</code> <code>gpus</code> Number of GPUs to allocate (set to 0 for CPU-only mode) <code>1</code> <code>memory_per_gpu</code> Memory allocated per GPU (in Gi) <code>128</code> <code>cpu_per_gpu</code> CPU cores allocated per GPU <code>4</code> <code>storage.ephemeral</code> Ephemeral storage configuration <code>128Gi</code>, <code>mlstorage</code>, <code>ReadWriteOnce</code> <code>deployment.ports.http</code> HTTP port exposed by the service <code>8080</code> <code>entrypoint</code> Custom entrypoint script See <code>values.yaml</code> for details <p>For more details see the <code>values.yaml</code> file.</p>"},{"location":"ai-workloads-manifests/dev-workspace-jupyterlab/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>To deploy the chart with the release name <code>example</code>, run the following command from the <code>helm/</code> directory:</p> <pre><code>helm template example . | kubectl apply -f -\n</code></pre> <p>Note: If you set the <code>gpus</code> value greater than 0, ensure you specify a GPU-capable image to utilize the allocated resources properly.</p>"},{"location":"ai-workloads-manifests/dev-workspace-jupyterlab/helm/#accessing-the-workload-locally","title":"Accessing the Workload Locally","text":"<p>To access JupyterLab locally, forward the service port to your machine:</p> <pre><code>kubectl port-forward services/dev-workspace-jupyterlab-example 8080:80\n</code></pre> <p>Then, open your browser and navigate to <code>http://localhost:8080</code>.</p>"},{"location":"ai-workloads-manifests/dev-workspace-jupyterlab/helm/#accessing-the-workload-via-url","title":"Accessing the Workload via URL","text":"<p>To access the workload through a URL, you can enable either an Ingress or HTTPRoute in the <code>values.yaml</code> file. The following parameters are available:</p> Parameter Description Default <code>ingress.enabled</code> Enable Ingress resource <code>false</code> <code>httproute.enabled</code> Enable HTTPRoute resource <code>false</code> <p>See the corresponding template files in the <code>templates/</code> directory. For more details on configuring Ingress or HTTPRoute, refer to the Ingress documentation and HTTPRoute documentation, or documentation of the particular gateway implementation you may use, like KGateway. Check with your cluster administrator for the correct configuration for your environment.</p>"},{"location":"ai-workloads-manifests/dev-workspace-vscode/helm/","title":"Visual Studio Code Workload","text":"<p>This workload deploys a basic Visual Studio Code environment on top of any image with Python (pip) pre-installed. It is ideal for interactive development sessions and experimentation with other workloads.</p>"},{"location":"ai-workloads-manifests/dev-workspace-vscode/helm/#configuration-parameters","title":"Configuration Parameters","text":"<p>You can configure the following parameters in the <code>values.yaml</code> file or override them via the command line:</p> Parameter Description Default <code>image</code> Container image repository and tag <code>rocm/pytorch:rocm6.4_ubuntu24.04_py3.12_pytorch_release_2.6.0</code> <code>imagePullPolicy</code> Image pull policy <code>Always</code> <code>gpus</code> Number of GPUs to allocate (set to 0 for CPU-only mode) <code>1</code> <code>memory_per_gpu</code> Memory allocated per GPU (in Gi) <code>128</code> <code>cpu_per_gpu</code> CPU cores allocated per GPU <code>4</code> <code>storage.ephemeral</code> Ephemeral storage configuration <code>128Gi</code>, <code>mlstorage</code>, <code>ReadWriteOnce</code> <code>deployment.ports.http</code> HTTP port exposed by the service <code>8080</code> <code>entrypoint</code> Custom entrypoint script See <code>values.yaml</code> for details <p>For more details see the <code>values.yaml</code> file.</p>"},{"location":"ai-workloads-manifests/dev-workspace-vscode/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>To deploy the chart with the release name <code>example</code>, run the following command from the <code>helm/</code> directory:</p> <pre><code>helm template example . | kubectl apply -f -\n</code></pre> <p>Note: If you set the <code>gpus</code> value greater than 0, ensure you specify a GPU-capable image to utilize the allocated resources properly.</p>"},{"location":"ai-workloads-manifests/dev-workspace-vscode/helm/#accessing-the-workload-locally","title":"Accessing the Workload Locally","text":"<p>To access Visual Studio Code locally, forward the service port to your machine:</p> <pre><code>kubectl port-forward services/dev-workspace-vscode-example 8080:80\n</code></pre> <p>Then, open your browser and navigate to <code>http://localhost:8080</code>.</p>"},{"location":"ai-workloads-manifests/dev-workspace-vscode/helm/#accessing-the-workload-via-url","title":"Accessing the Workload via URL","text":"<p>To access the workload through a URL, you can enable either an Ingress or HTTPRoute in the <code>values.yaml</code> file. The following parameters are available:</p> Parameter Description Default <code>ingress.enabled</code> Enable Ingress resource <code>false</code> <code>httproute.enabled</code> Enable HTTPRoute resource <code>false</code> <p>See the corresponding template files in the <code>templates/</code> directory. For more details on configuring Ingress or HTTPRoute, refer to the Ingress documentation and HTTPRoute documentation, or documentation of the particular gateway implementation you may use, like KGateway. Check with your cluster administrator for the correct configuration for your environment.</p>"},{"location":"ai-workloads-manifests/download-data-to-bucket/helm/","title":"workload helm template to download and preprocess data to bucket storage","text":"<p>This is an workload which downloads data, potentially preprocesses it, and uploads it to bucket storage. Since the <code>helm install</code> semantics are centered around on-going installs, not jobs that run once, it's best to just run <code>helm template</code> and pipe the result to <code>kubectl create</code> (<code>create</code> maybe more appropriate than apply for this Job as we don't expect to modify existing entities). Example: <pre><code>helm template workloads/download-data-to-bucket/helm \\\n    -f workloads/download-data-to-bucket/overrides/argilla-mistral-large-human-prompts.yaml \\\n    --name-template download-argilla \\\n    | kubectl create -f -\n</code></pre></p>"},{"location":"ai-workloads-manifests/download-data-to-bucket/helm/#user-input-values","title":"User input values","text":"<p>See the <code>values.yaml</code> file for all user input values that you can provide, with instructions. In values.yaml, the <code>dataScript</code> is a script instead of just a dataset identifier, because the datasets on HuggingFace hub don't have a standard format that can be always directly passed to any training framework. The data script should format the data into the format that the training framework expects.</p> <p>Any data files output of the data script should be saved to the <code>/downloads/datasets/</code>. The files are uploaded to the directory pointed to by <code>bucketDataDir</code>, with the same filename as they had under <code>/downloads/datasets</code>.</p>"},{"location":"ai-workloads-manifests/download-data-to-bucket/helm/#silogen-finetuning-engine-format","title":"Silogen finetuning engine format","text":"<p>For the silogen finetuning engine, the data format is JSON lines. For supervised finetuning, each line has a JSON dictionary formatted as follows: <pre><code>{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"This is a user message\"},\n    {\"role\": \"assistant\", \"content\": \"The is an assistant answer\"}\n  ]\n}\n</code></pre> There can be an arbitrary number of messages. Additionally, each dictionary can contain a <code>dataset</code> field that has the dataset identifier, and an <code>id</code> field that identifies the data point uniquely. For Direct Preference Optimization, the data format is as follows: <pre><code>{\n  \"prompt_messages\": [\n    {\"role\": \"user\", \"content\": \"This is a user message\"},\n  ],\n  \"chosen_messages\": [\n    {\"role\": \"assistant\", \"content\": \"This is a preferred answer\"}\n  ],\n  \"rejected_messages\": [\n    {\"role\": \"assistant\", \"content\": \"This is a rejected answer\"}\n  ]\n}\n</code></pre></p> <p>The HuggingFace datasets library can save the output in JSON lines format with the <code>to_json</code> function: <pre><code>dataset.to_json(\"/downloads/datasets/&lt;name of your dataset file.jsonl&gt;\")\n</code></pre></p>"},{"location":"ai-workloads-manifests/download-huggingface-model-to-bucket/helm/","title":"workload helm template to download a model to bucket storage","text":"<p>This is an workload which downloads a model and uploads it to bucket storage. Since the <code>helm install</code> semantics are centered around on-going installs, not jobs that run once, it's best to just run <code>helm template</code> and pipe the result to <code>kubectl create</code> (<code>create</code> maybe more appropriate than apply for this Job as we don't expect to modify existing entities). Example: <pre><code>helm template workloads/download-huggingface-model-to-bucket/helm \\\n    -f workloads/download-huggingface-model-to-bucket/helm/overrides/llama-3.1-tiny-random-to-google.yaml \\\n    --name-template download-huggingface \\\n    | kubectl create -f -\n</code></pre></p>"},{"location":"ai-workloads-manifests/download-huggingface-model-to-bucket/helm/#user-input-values","title":"User input values","text":"<p>See the <code>values.yaml</code> file for the user input values that you can provide, with instructions.</p>"},{"location":"ai-workloads-manifests/k8s-namespace-setup/helm/","title":"k8s-namespace-setup","text":"<p>A Helm chart for setting up Kubernetes namespaces. This chart allows you to configure the following components in a namespace:</p> <ul> <li>External Secrets: Pull in secrets from within or outside the cluster, such as bucket storage credentials or Hugging Face tokens.</li> <li>Kueue: Set up a local queue for Kueue in the namespace to submit jobs.</li> <li>Role / Role Binding: Configure permissions for service accounts, like the default service account, to access the Kubernetes API from within a container.</li> </ul> <p>If you are not sure if you need any of these, then this workload is probably not needed for you.</p>"},{"location":"ai-workloads-manifests/k8s-namespace-setup/helm/#installation","title":"Installation","text":"<p>To apply a configuration to the active namespace, use:</p> <pre><code>helm template . -f overrides/rename-secret-names.yaml | kubectl apply -f -\n</code></pre> <p>To specify a different namespace:</p> <pre><code>helm template . -f overrides/rename-secret-names.yaml | kubectl apply -n &lt;namespace&gt; -f -\n</code></pre> <p>Control which components to set up using command line parameters:</p> <pre><code>helm template . --set kueue.setup=true --set role.setup=true | kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads-manifests/k8s-namespace-setup/helm/#configuration","title":"Configuration","text":"<p>The following table lists the configurable parameters of the <code>k8s-namespace-setup</code> chart and their default values.</p>"},{"location":"ai-workloads-manifests/k8s-namespace-setup/helm/#external-secret","title":"External Secret","text":"Parameter Description Default <code>external_secret.setup</code> Enable external secret <code>false</code> <code>external_secret.external_secret_name</code> External secret name <code>minio-credentials-fetcher</code> <code>external_secret.src.secret_store_name</code> Secret store name <code>k8s-secret-store</code> <code>external_secret.src.remote_secret_name</code> Remote secret name <code>default-user</code> <code>external_secret.src.access_key_name</code> Access key name <code>API_ACCESS_KEY</code> <code>external_secret.src.secret_key_name</code> Secret key name <code>API_SECRET_KEY</code> <code>external_secret.dest.k8s_secret_name</code> Kubernetes secret name <code>minio-credentials</code> <code>external_secret.dest.access_key_name</code> Kubernetes access key name <code>minio-access-key</code> <code>external_secret.dest.secret_key_name</code> Kubernetes secret key name <code>minio-secret-key</code>"},{"location":"ai-workloads-manifests/k8s-namespace-setup/helm/#kueue","title":"Kueue","text":"Parameter Description Default <code>kueue.setup</code> Enable kueue <code>false</code> <code>kueue.cluster_queue_name</code> Cluster queue name <code>kaiwo</code>"},{"location":"ai-workloads-manifests/k8s-namespace-setup/helm/#roles","title":"Roles","text":"Parameter Description Default <code>role.setup</code> Enable roles setup <code>false</code> <code>role.name</code> Role name <code>default-role</code> <code>role.bindingName</code> Role binding name <code>default-role-binding</code> <code>role.rules</code> Role rules See <code>values.yaml</code>"},{"location":"ai-workloads-manifests/llm-deepspeed-estimate-ram-vram/helm/","title":"Deepspeed RAM and VRAM estimate","text":"<p>Deepspeed can require a large amount of RAM, and save on the VRAM, depending on the configuration. Deepspeed includes a memory estimation utility, and this chart exposes that utility in a convenient package.</p>"},{"location":"ai-workloads-manifests/llm-deepspeed-estimate-ram-vram/helm/#specifying-the-inputs","title":"Specifying the inputs","text":"<p>You can get estimates for a specifc model on bucket storage or on HuggingFace Hub. In that case, the model config is downloaded to the container and a model loaded onto a meta device, meaning that no parameters are actually constructed. The number of parameters is computed based on the loaded model, but note that any shared parameters will be counted multiple times, since the meta device cannot distinguish between shared layers and non-shared ones. That may slightly inflate the estimates. Specify the model in the <code>modelPath</code> field. If the model is on an S3-bucket, prefix the bucket path with <code>s3://</code>.</p> <p>You can also simply specify the number of parameters. Then, the estimates are based on your given number. Note that if you give both <code>modelPath</code> and <code>numParameters</code>, then <code>modelPath</code> is used. If you're using stage 3, you must also specify <code>largestLayerParameters</code>.</p> <p>The estimates depend on the number of nodes and GPUs per node. These are set with <code>nodes</code> and <code>gpusPerNode</code> respectively. Additionally, a buffer factor is used. It's set by <code>bufferFactor</code>.</p> <p>Finally, the estimates depend on the Deepspeed stage. Stages 1 and 2 have the same estimate, and stage 3 has its own. Specify the Deepspeed stage with <code>stage</code>.</p> <p>See <code>values.yaml</code> for other inputs, which determine the image, the bucket storage secret naming, the hf-token secret naming, and whether to use a KaiwoJob or a regular Job.</p>"},{"location":"ai-workloads-manifests/llm-deepspeed-estimate-ram-vram/helm/#getting-the-results","title":"Getting the results","text":"<p>The estimates are printed to the logs of the pod. You simply run the workload, e.g. <pre><code>helm template . --set modelPath=meta-llama/Llama-3.1-70B --values overrides/hf-token --name-template \"llama-70b-ds-estimate\" | kubectl apply -f -\n</code></pre> or <pre><code>helm template . --set modelPath=s3://default-bucket/models/meta-llama/Llama-3.1-70B --name-template \"llama-70b-ds-estimate\" | kubectl apply -f -\n</code></pre> or <pre><code>helm template . --set numParameters=70e9 --name-template \"generic-70b-ds-estimate\" | kubectl apply -f -\n</code></pre></p> <p>then let the workload finish and check the logs with <pre><code>kubectl logs llama-70b-ds-estimate-POD_UNIQUE_ID_PART_HERE\n</code></pre></p> <p>The logs end in something along the lines of: <code>Estimated memory needed for params, optim states and gradients for a: HW: Setup with 1 node, 8 GPUs per node. SW: Model with 69503M total params.   per CPU  |  per GPU |   Options  3107.03GB | 129.46GB | offload_optimizer=cpu  3107.03GB | 275.10GB | offload_optimizer=none</code></p>"},{"location":"ai-workloads-manifests/llm-evaluation-judge/helm/","title":"Documentation for LLM-as-a-Judge workloads","text":""},{"location":"ai-workloads-manifests/llm-evaluation-metrics/helm/","title":"Index","text":""},{"location":"ai-workloads-manifests/llm-evaluation-metrics/helm/#ai-evaluation-workloads","title":"AI Evaluation Workloads","text":"<p>This helm chart implements evaluation of LLMs using the BERTscore metric, comparing generated answers to a gold standard.</p> <p>The necessary Kubernetes and Helm files are stored here in <code>/workloads/llm-evaluation-metrics/helm</code>, while the docker image and associated evaluation package are stored in /docker/evaluation-metrics.</p>"},{"location":"ai-workloads-manifests/llm-evaluation-metrics/helm/#helm-and-kubernetes-files","title":"Helm and Kubernetes files","text":"<p>The Helm templates are stored in <code>/workloads/llm-evaluation-metrics/helm/templates</code>, the main template workload template being <code>metrics_evaluation_template_with_download.yaml</code>. Default values can be found in <code>values.yaml</code>, with user-defined configurations stored in <code>/overrides</code>.</p> <p>A few extra resources are defined in <code>templates/</code>. We use a <code>ConfigMap</code> (<code>templates/configmap.yaml</code>) to mount files directly to the cluster when running the workload. Anything stored in the <code>mount/</code> directory will be mounted.</p>"},{"location":"ai-workloads-manifests/llm-evaluation-metrics/helm/#docker-container","title":"Docker Container","text":"<p>We define an associated evaluation package in <code>/docker/evaluation-metrics</code>. This contains code to call the inference container, and subsequently run the BERTscore metric evaluation, writing results to MinIO storage.</p> <p>This package is installed into a docker image, which can be used to run the evaluation container in the helm template. We use a Makefile to push new images to our GitHub registry. (<code>&gt; make push</code>)</p>"},{"location":"ai-workloads-manifests/llm-evaluation-metrics/helm/#running","title":"Running","text":"<p>To run this evaluation workload with helm, use the template command and pipe it to kubectl apply:</p> <pre><code>cd workloads/llm-evaluation-metrics\n</code></pre> <pre><code>helm template helm -f examples/bertscore_llama-3.1-8B_cnn-dailymail_values.yaml | kubectl apply -f - -n &lt;your-namespace&gt;\n</code></pre> <p>To cancel runs in between debugging, use</p> <pre><code>kubectl delete job evaluation-metrics -n &lt;your-namespace&gt;\n</code></pre>"},{"location":"ai-workloads-manifests/llm-evaluation-metrics/helm/#making-changes","title":"Making changes","text":"<p>The templated kubernetes manifest, where you can make the main changes to the workload, is <code>workloads/llm-evaluation-metrics/helm/templates/metrics_evaluation_template_with_download.yaml</code></p> <p>Parameters of the job are stored in</p> <p><code>workloads/llm-evaluation-metrics/helm/values.yaml</code></p> <p>And can be overridden by</p> <p><code>workloads/llm-evaluation-metrics/helm/overrides/bertscore_llama-3.1-8B_cnn-dailymail_values.yaml</code></p> <p>Finally, to change the docker image, refer to dockerfiles in the docker directory, e.g. <code>docker/evaluation/metrics/evaluation_by_metrics.Dockerfile</code></p> <p>This directory may also contain some scripts, like compute_bertscore.py, that will probably be built into the docker images. The docker images can be uploaded to our private GCS registry, here: https://console.cloud.google.com/artifacts/docker/silogen-dev/europe-west4/silogen-dev?inv=1&amp;invt=AbqFEQ&amp;project=silogen-dev</p>"},{"location":"ai-workloads-manifests/llm-finetune-axolotl/helm/","title":"Finetuning with Axolotl","text":"<p>This is a Helm Chart for running an Axolotl finetuning job.</p> <p>Currently the base model and input data are assumed to be from HuggingFace, or some other source directly supported by Axolotl. The output is saved with MinIO in the directory specified by <code>checkpointsRemote</code>. If any checkpoints already exist in the directory, the training can be resumed from there (by setting <code>auto_resume_from_checkpoints</code>)</p> <p>The provided example task is based on the Llama-3.2-1B LoRA config from the Axolotl repo. The only changes are:</p> <ul> <li>set the optimizer to <code>adamw_torch</code> in order to avoid using bitsandbytes</li> <li>turn on <code>auto_resume_from_checkpoints</code></li> <li>not specifying an output directory (the chart takes care of uploading the checkpoints)</li> </ul>"},{"location":"ai-workloads-manifests/llm-finetune-axolotl/helm/#limitations","title":"Limitations","text":"<ul> <li>bitsandbytes does not work. It is built and installed so that axolotl launch works, but currently has some unsolved issues. Avoid functionality that actually uses bitsandbytes.</li> <li>ray also does not work as expected, so multi-node training is not supported.</li> </ul>"},{"location":"ai-workloads-manifests/llm-finetune-axolotl/helm/#configuration","title":"Configuration","text":"<p>Create an Axolotl config file in the <code>mount/</code> directory, and set <code>configFile</code> in the overrides to point to your config file. See the Axolotl docs for how to specify a config file.</p>"},{"location":"ai-workloads-manifests/llm-finetune-axolotl/helm/#running-the-workload","title":"Running the workload","text":"<p>Then the simplest is to run <code>helm template</code> and pipe the result to <code>kubectl create</code>.</p> <p>Example command:</p> <pre><code>helm template workloads/llm-finetune-axolotl/helm \\\n  --values workloads/llm-finetune-axolotl/helm/overrides/finetune-lora.yaml \\\n  --name-template finetune-lora-axolotl \\\n  | kubectl create -f -\n</code></pre>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/","title":"Finetuning with the SiloGen finetuning engine","text":"<p>This is a Helm Chart for finetuning Jobs based on the SiloGen finetuning engine. The chart integrates the finetuning config as part of the <code>values.yaml</code> input.</p> <p>See the <code>values.yaml</code> file for the general structure (more documentation coming soon).</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/#running-the-workload","title":"Running the workload","text":"<p>Since the <code>helm install</code> semantics are centered around on-going installs, not jobs that run once, it's best to just run <code>helm template</code> and pipe the result to <code>kubectl create</code>.</p> <p>Example command: <pre><code>helm template . \\\n  -f overrides/llama-31-tiny-random-deepspeed-values.yaml \\\n  --name-template llama-31-tiny-random-deepspeed-alpha \\\n  | kubectl create -f -\n</code></pre></p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/#multiple-overlays-simplified-interface","title":"Multiple overlays, simplified interface","text":"<p>This chart supports two ways of specifying certain inputs, one on the top level and one as part of the finetuning_config: - Training data can be provided as <code>trainingData</code> or <code>finetuning_config.data_conf.training_data.datasets</code> - The total batch size target can be provided as <code>batchSize</code> or <code>finetuning_config.batchsize_conf.total_train_batch_size</code> - The number of epochs to run for can be provided as <code>numberOfEpochs</code> or <code>finetuning_config.training_args.num_train_epochs</code></p> <p>The top level inputs provide a simpler interface to run finetuning. However, they're not enough alone to fully specify a sensible training setup. The expectation is that these top-level inputs are used in conjuction with a set of override files that specify most arguments. This is the expected way that the chart is used in conjuction with the so called Silogen developer console. An example of such use is: <pre><code>helm template . \\\n  -f overrides/models/meta-llama_llama-3.1-8b.yaml \\\n  -f overrides/dev-console/default.yaml \\\n  --name-template llama-31-8b-argilla-alpha \\\n  | kubectl create -f -\n</code></pre></p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/#multiple-overlays-general-case","title":"Multiple overlays, general case","text":"<p>Multiple overlays can be useful for a CLI user as well. Here's an example that reproduces the output of tutorial-01-finetune-full-param.yaml: <pre><code>helm template . \\\n  -f overrides/models/tiny-llama_tinyllama-1.1b-chat-v1.0.yaml \\\n  -f overrides/additional-example-files/repro-tutorial-01-user-inputs.yaml \\\n  --name-template tiny-llama-argilla-alpha \\\n  | kubectl create -f -\n</code></pre> To check that the manifests match, we can run a diff and see that it is empty: <pre><code>diff \\\n  &lt;( \\\n    helm template . \\\n    -f overrides/models/tiny-llama_tinyllama-1.1b-chat-v1.0.yaml \\\n    -f overrides/additional-example-files/repro-tutorial-01-user-inputs.yaml \\\n    --name-template tiny-llama-argilla-alpha \\\n  ) \\\n  &lt;( \\\n    helm template . \\\n    -f overrides/tutorial-01-finetune-full-param.yaml \\\n    --name-template tiny-llama-argilla-alpha \\\n  )\n</code></pre></p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/#tensorboard","title":"Tensorboard","text":"<p>Specifying <code>runTensorboard: true</code> and <code>finetuning_config.trainings_args.report_to: [\"tensorboard\"]</code> logs the training progress to tensorboard and serves the Tensorboard web UI from the training container. The tensorboard logs are also uploaded to the bucket storage for later use.</p> <p>To connect to the Tensorboard web UI on the container, start a port-forward: <pre><code>kubectl port-forward --namespace YOUR_NAMESPACE pods/YOUR_POD_NAME 6006:6006\n</code></pre> Then browse to localhost:6006.</p> <p>Note that the logging frequency is set by the HuggingFace Transformers logging options.</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/#best-known-configuration-model-overrides","title":"Best-known-configuration model overrides","text":"<p>The directory <code>overrides/models</code> hosts finetuning recipes for various models. The files are named according to model canonical names, which is the huggingface pattern of <code>organization/model-name</code> just changed into <code>organization_model-name</code>. These configurations have been shown to work well in experiments, but that does not guarantee that these exact parameters are always optimal. The best parameters still depend on the data, too.</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/","title":"Finetuning config structure and parameters","text":"<p>This document describes the structure of the finetuning configuration, and the parameters and values that can be defined there.</p> <p>See the finetuning config section this config file for an example of a valid configuration. See the various sub-configs for their options. Additional properties are not allowed.</p> <p>Top-level properties:</p> Property Type Required Possible values Default Description method <code>const</code> <code>sft</code> <code>\"sft\"</code> data_conf <code>object</code> \u2705 ChatTrainValidConfig The data input config training_args <code>object</code> \u2705 SilogenTrainingArguments Transformer TrainingArguments with some restrictions overrides <code>object</code> Overrides <code>{\"num_train_epochs\": null, \"lr_multiplier\": 1.0, \"lr_batch_size_scaling\": \"none\"}</code> Override options to simplify the config interface batchsize_conf <code>object</code> \u2705 BatchsizeConfig Batch size configuration peft_conf <code>object</code> \u2705 NoPeftConfig or PretrainedPeftConfig or GenericPeftConfig Adapter configuration run_conf <code>object</code> \u2705 RunConfig Model related configuration tracking <code>object</code> or <code>null</code> FinetuningTrackingConfig MLFlow tracking configuration quant_conf <code>object</code> BnBQuantizationConfig or NoQuantizationConfig <code>{\"quantization_type\": \"no-quantization\"}</code> Quantization configuration sft_args <code>object</code> \u2705 SFTArguments SFT specific arguments"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#definitions","title":"Definitions","text":""},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#autosplitdatainput","title":"AutoSplitDataInput","text":"<p>Automatic validation split from the training data</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description type <code>const</code> \u2705 <code>AUTO_SPLIT</code> data_type <code>string</code> string <code>\"ChatConversation\"</code> generally, the data_type is automatically set based on the experiment config method ratio <code>number</code> number <code>0.2</code> Ratio of the training data to use for validation seed <code>integer</code> integer <code>1289525893</code> Seed for the random number generator for splitting"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#batchsizeconfig","title":"BatchsizeConfig","text":"<p>Config for determining the total batch size</p> <p>Total batch size is the effective batch size for the complete training run. It is equal to number of processes * per-device batch size * accumulation.</p> <p>The maximum batch size per device is the maximum batch size that can be accommodated on a single device. This mostly limited by the memory capacity of the device.</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_1","title":"Type: <code>object</code>","text":"Property Type Required Possible values Description total_train_batch_size <code>integer</code> \u2705 integer The total batch size for the training run max_per_device_train_batch_size <code>integer</code> \u2705 integer The maximum training batch size per device per_device_eval_batch_size <code>integer</code> or <code>null</code> integer The maximum eval batch size per device, if not given, will use same as training batch size"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#bnbquantizationconfig","title":"BnBQuantizationConfig","text":"<p>Bits and Bytes configuration</p> <p>The options are from the BitsAndBytes config, see: https://huggingface.co/docs/transformers/en/main_classes/quantization#transformers.BitsAndBytesConfig</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_2","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description quantization_type <code>const</code> <code>bits-and-bytes</code> <code>\"bits-and-bytes\"</code> load_in_8bit <code>boolean</code> boolean <code>False</code> load_in_4bit <code>boolean</code> boolean <code>False</code> llm_int8_threshold <code>number</code> number <code>6.0</code> llm_int8_skip_modules <code>array</code> or <code>null</code> string llm_int8_enable_fp32_cpu_offload <code>boolean</code> boolean <code>False</code> llm_int8_has_fp16_weight <code>boolean</code> boolean <code>False</code> bnb_4bit_compute_dtype <code>string</code> or <code>null</code> string bnb_4bit_quant_type <code>const</code> <code>fp4</code> and/or <code>nf4</code> <code>\"fp4\"</code> bnb_4bit_use_double_quant <code>boolean</code> boolean <code>False</code> bnb_4bit_quant_storage <code>string</code> or <code>null</code> string"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#chattrainvalidconfig","title":"ChatTrainValidConfig","text":"<p>Training time data configuration.</p> <p>Always defines some DataInput for training data and can include validation DataInput, though a trivial NoneDataInput is also allowed for the validation side.</p> <p>Additionally includes chat template and padding configurations, as those are part of the data input pipeline.</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_3","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description training_data <code>object</code> \u2705 ConcatenationDataInput or WeightedMixDataInput validation_data <code>object</code> \u2705 AutoSplitDataInput or ConcatenationDataInput or NoneDataInput chat_template_name <code>string</code> <code>mistral-with-system</code> or <code>chat-ml</code> or <code>poro</code> or <code>keep-original</code> or <code>simplified-llama31</code> <code>\"mistral-with-system\"</code> padding_side <code>string</code> string <code>\"right\"</code> Padding side, right is usually right. missing_pad_token_strategy <code>string</code> MissingPadTokenStrategy <code>\"bos-repurpose\"</code> See the MissingPadTokenStrategys for descriptions of the options"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#concatenationdatainput","title":"ConcatenationDataInput","text":"<p>A simple list of datasets</p> <p>These are simply concatenated, the same as sampling all with equal weight.</p> <p>The datasets themselves need to be in the finetuning supported JSONL formats. For SFT this means lines:</p> <pre><code>{\"messages\": {\"content\": \"string\", \"role\": \"string\"}}\n</code></pre> <p>For DPO this means lines of:</p> <pre><code>{\"prompt_messages\": {\"content\": \"string\", \"role\": \"string\"}, \"chosen_messages\": {\"content\": \"string\", \"role\": \"string\"}, \"rejected_messages\": {\"content\": \"string\", \"role\": \"string\"}}\n</code></pre>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_4","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description type <code>const</code> \u2705 <code>CONCATENATION</code> datasets <code>array</code> \u2705 DatasetDefinition data_type <code>string</code> string <code>\"ChatConversation\"</code> generally, the data_type is automatically set based on the experiment config method"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#datasetdefinition","title":"DatasetDefinition","text":"<p>Define how to load a dataset</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_5","title":"Type: <code>object</code>","text":"Property Type Required Possible values Description path <code>string</code> \u2705 string Local path to a JSONL file in the finetuning data format"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#finetuningtrackingconfig","title":"FinetuningTrackingConfig","text":"<p>Settings that define how run details are logged</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_6","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description mlflow_server_uri <code>string</code> \u2705 string MLflow server URI. Can be local path experiment_name <code>string</code> \u2705 string Experiment name that is used for MLFlow tracking run_id <code>string</code> or <code>null</code> string Run id, to resume logging to previousely started run run_name <code>string</code> or <code>null</code> string Run name, to give meaningful name to the run to be displayed in MLFlow UI. Used only when run_id is unspecified hf_mlflow_log_artifacts <code>string</code> string <code>\"False\"</code> Whether to store model artifacts in MLFlow"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#genericpeftconfig","title":"GenericPeftConfig","text":"<p>Config for any new initialized PEFT Adapter</p> <p>See https://huggingface.co/docs/peft/tutorial/peft_model_config for the possible kwargs and https://github.com/huggingface/peft/blob/v0.7.1/src/peft/utils/peft_types.py for the types.</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#example","title":"Example","text":"<pre><code>&gt;&gt;&gt; loaded_data = {'peft_type':'LORA', 'task_type': 'CAUSAL_LM',\n...         'peft_kwargs': {'r': 32, 'target_modules': ['v_proj']}}\n&gt;&gt;&gt; generic_conf = GenericPeftConfig(**loaded_data)\n&gt;&gt;&gt; # Then later in the code something like:\n&gt;&gt;&gt; model = transformers.AutoModel.from_pretrained('hf-internal-testing/tiny-random-MistralModel')\n&gt;&gt;&gt; peft.get_peft_model(model, generic_conf.get_peft_config())\nPeftModelForCausalLM(\n  (base_model): LoraModel(\n    ...\n  )\n)\n</code></pre>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_7","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description peft_type <code>string</code> \u2705 PeftType task_type <code>string</code> TaskType <code>\"CAUSAL_LM\"</code> peft_kwargs <code>object</code> object"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#missingpadtokenstrategy","title":"MissingPadTokenStrategy","text":"<p>Specifies the available missing pad token strategies.</p> <p>We've shown in a small set of experiments that repurposing EOS can start to hurt performance while the other options seem to work equally well.</p> <p>Repurposing EOS is the default in many online sources, but it is actually a bad idea if we want to predict EOS, as all the pad_token_ids get ignored in loss computation, and thus the model does not learn to predict the end of the text. However, for models that have additional tokens for end of message, end of turn, etc. this is not so dangerous.</p> <p>Repurposing BOS is similar to repurposing EOS, but since we do not need to predict BOS, this may be more sensible.</p> <p>Repurposing UNK can work with tokenizers that never produce UNKs in normal data (e.g. Mistral tokenizers should have a byte fall-back so that everything can be tokenized).</p> <p>UNK_CONVERT_TO_EOS uses a hack where the unk_token_id is initially used for padding, but in the collation phase the input-side UNKs (padding) gets set to EOS, so that the input-side padding looks like EOS. On the output-side, the UNKs (padding) still gets ignored. NOTE: This will leave the tokenizer's pad_token_id set to the unk_token_id; so any subsequent use of the model where padding is involved should somehow explicitly set the pad_token_id again.</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-string","title":"Type: <code>string</code>","text":"<p>Possible Values: <code>eos-repurpose</code> or <code>bos-repurpose</code> or <code>unk-repurpose</code> or <code>unk-convert-to-eos</code></p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#modelarguments","title":"ModelArguments","text":"<p>These are passed to AutoModelForCausalLM.from_pretrained</p> <p>See parameter docstrings and help at: https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained See below in \"Parameters for big model inference\" too, it affects training too. Also note that this link takes you to the transformers main branch version - be sure to compare with the installed version of transformers (that keeps changing over time, and it is difficult to keep this doctstring up to date, so we wanted to link to the latest here).</p> <p>Some important parameters to consider are: - device_map :     A map that specifies where each submodule should go. It doesn\u2019t need to be refined to each parameter/buffer     name, once a given module name is inside, every submodule of it will be sent to the same device. If we only pass     the device (e.g., \"cpu\", \"cuda:1\", \"mps\", or a GPU ordinal rank like 1) on which the model will be allocated,     the device map will map the entire model to this device. Passing device_map = 0 means put the whole model on GPU     0. - attn_implementation :     The attention implementation to use in the model (if relevant). Can be any of \"eager\" (manual implementation of     the attention), \"sdpa\" (using F.scaled_dot_product_attention), or \"flash_attention_2\" (using     Dao-AILab/flash-attention). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is     otherwise the manual \"eager\" implementation.</p> <p>NOTE:     This does not include quantization_config. Quantization config is specified separately.</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_8","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description torch_dtype <code>const</code> <code>auto</code> <code>\"auto\"</code> device_map <code>object</code> or <code>string</code> or <code>null</code> object and/or string Custom device map so that you can manually override the choices that HuggingFace would make. This can also be a string to specify \"auto\", \"balanced_low_0\", or \"sequential\" max_memory <code>object</code> or <code>null</code> object low_cpu_mem_usage <code>boolean</code> boolean <code>False</code> attn_implementation <code>string</code> or <code>null</code> string Note: this can be set to \"sdpa\", \"flash_attention_2\", \"eager\" offload_folder <code>string</code> or <code>null</code> string offload_state_dict <code>boolean</code> or <code>null</code> boolean Default is True if offloading (otherwise no effect) offload_buffers <code>boolean</code> or <code>null</code> boolean use_cache <code>boolean</code> boolean <code>True</code> Saves generated hidden states to speed up generation. See: https://discuss.huggingface.co/t/what-is-the-purpose-of-use-cache-in-decoder/958 use_cache is mutually exclusive with gradient_checkpointing cache_dir <code>string</code> or <code>null</code> string force_download <code>boolean</code> boolean <code>False</code> local_files_only <code>boolean</code> boolean <code>False</code> proxies <code>object</code> or <code>null</code> object resume_download <code>boolean</code> boolean <code>False</code> revision <code>string</code> string <code>\"main\"</code> code_revision <code>string</code> string <code>\"main\"</code> subfolder <code>string</code> or <code>null</code> string token <code>string</code> or <code>null</code> string use_safetensors <code>boolean</code> or <code>null</code> boolean variant <code>string</code> or <code>null</code> string trust_remote_code <code>boolean</code> boolean <code>False</code> Warning: if set to <code>True</code>, allows execution of downloaded remote code"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#nopeftconfig","title":"NoPeftConfig","text":"<p>A trivial config specifying that no peft is used</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_9","title":"Type: <code>object</code>","text":"Property Type Required Possible values Description peft_type <code>const</code> \u2705 <code>NO_PEFT</code>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#noquantizationconfig","title":"NoQuantizationConfig","text":"<p>A marker not to use quantization</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_10","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description quantization_type <code>const</code> <code>no-quantization</code> <code>\"no-quantization\"</code>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#nonedatainput","title":"NoneDataInput","text":"<p>A special type for not using data e.g. in validation</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_11","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description type <code>const</code> \u2705 <code>NONE</code> data_type <code>string</code> string <code>\"ChatConversation\"</code> generally, the data_type is automatically set based on the experiment config method"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#overrides","title":"Overrides","text":"<p>Override options that allow simple interfaces for charts using these configs</p> <p>This is particularly useful for a helm chart interface where we include the finetuning package config as a part of the values.yaml file. These a more flexible helm interface with certain keys brought to the top level.</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_12","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description num_train_epochs <code>integer</code> or <code>number</code> or <code>null</code> number Overrides the number of epochs in the training_args lr_multiplier <code>number</code> number <code>1.0</code> Multiplier applied to the learning rate in the training_args lr_batch_size_scaling <code>string</code> <code>none</code> <code>sqrt</code> <code>linear</code> <code>\"none\"</code> Scales the learning rate in the training_args by a factor derived from the total training batch size. <code>none</code>: No scaling. <code>sqrt</code>: Multiplies learning rate by square root of batch size (a classic scaling rule). <code>linear</code>: Multiplies learning rate by the batch size (a more modern scaling rule)."},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#pefttype","title":"PeftType","text":"<p>Enum class for the different types of adapters in PEFT.</p> <p>Supported PEFT types: - PROMPT_TUNING - MULTITASK_PROMPT_TUNING - P_TUNING - PREFIX_TUNING - LORA - ADALORA - BOFT - ADAPTION_PROMPT - IA3 - LOHA - LOKR - OFT - XLORA - POLY - LN_TUNING - VERA - FOURIERFT - HRA</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-string_1","title":"Type: <code>string</code>","text":"<p>Possible Values: <code>PROMPT_TUNING</code> or <code>MULTITASK_PROMPT_TUNING</code> or <code>P_TUNING</code> or <code>PREFIX_TUNING</code> or <code>LORA</code> or <code>ADALORA</code> or <code>BOFT</code> or <code>ADAPTION_PROMPT</code> or <code>IA3</code> or <code>LOHA</code> or <code>LOKR</code> or <code>OFT</code> or <code>POLY</code> or <code>LN_TUNING</code> or <code>VERA</code> or <code>FOURIERFT</code> or <code>XLORA</code> or <code>HRA</code> or <code>VBLORA</code></p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#pretrainedpeftconfig","title":"PretrainedPeftConfig","text":"<p>PEFT adapter uses the config and initialisation from a pretrained adapter</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_13","title":"Type: <code>object</code>","text":"Property Type Required Possible values Description peft_type <code>const</code> \u2705 <code>PRETRAINED_PEFT</code> name_or_path <code>string</code> \u2705 string HF ID or path to the pretrained peft"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#runconfig","title":"RunConfig","text":"<p>Experiment running configuration</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_14","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description model <code>string</code> string <code>\"/local_resources/basemodel\"</code> Local path to model to be fine-tuned. Normally this should be <code>/local_resources/basemodel</code> model_args <code>object</code> ModelArguments <code>{\"torch_dtype\": \"auto\", \"device_map\": \"auto\", \"max_memory\": null, \"low_cpu_mem_usage\": false, \"attn_implementation\": null, \"offload_folder\": null, \"offload_state_dict\": null, \"offload_buffers\": null, \"use_cache\": true, \"cache_dir\": null, \"force_download\": false, \"local_files_only\": false, \"proxies\": null, \"resume_download\": false, \"revision\": \"main\", \"code_revision\": \"main\", \"subfolder\": null, \"token\": null, \"use_safetensors\": null, \"variant\": null, \"trust_remote_code\": false}</code> tokenizer <code>string</code> or <code>null</code> string Model HuggingFace ID, or path, or None to use the one associated with the model use_fast_tokenizer <code>boolean</code> boolean <code>True</code> Use the Fast version of the tokenizer. The 'slow' version may be compatible with more features. resume_from_checkpoint <code>boolean</code> or <code>string</code> boolean and/or string <code>False</code> Normally should be set to 'auto' to continue if a checkpoint exists. Can set to <code>True</code> to always try to continue, <code>False</code> to never try, or a path to load from a specific path. final_checkpoint_name <code>string</code> string <code>\"checkpoint-final\"</code> Name of final checkpoint. Should be left as default"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#sftarguments","title":"SFTArguments","text":"<p>Supervised fine-tuning arguments</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_15","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description max_seq_length <code>integer</code> integer <code>2048</code> Maximum length input sequence length. Longer sequences will be filtered out. save_name_if_new_basemodel <code>string</code> string <code>\"checkpoint-new-basemodel\"</code> If a new basemodel is saved, it will be saved with this name train_on_completions_only <code>boolean</code> boolean <code>False</code> Only compute loss on the assistant's turns."},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#silogentrainingarguments","title":"SilogenTrainingArguments","text":"<p>HuggingFace TrainingArguments as Config with additional SiloGen conventions</p> <p>The list of training arguments is best available online (the version might not be up-to-date here): https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments</p> <p>The TrainingArguments object does a lot of things besides specifying the training configuaration options (e.g. it has computed properties like true training batch size etc.)</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#tasktype","title":"TaskType","text":"<p>Enum class for the different types of tasks supported by PEFT.</p> <p>Overview of the supported task types: - SEQ_CLS: Text classification. - SEQ_2_SEQ_LM: Sequence-to-sequence language modeling. - CAUSAL_LM: Causal language modeling. - TOKEN_CLS: Token classification. - QUESTION_ANS: Question answering. - FEATURE_EXTRACTION: Feature extraction. Provides the hidden states which can be used as embeddings or features   for downstream tasks.</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-string_2","title":"Type: <code>string</code>","text":"<p>Possible Values: <code>SEQ_CLS</code> or <code>SEQ_2_SEQ_LM</code> or <code>CAUSAL_LM</code> or <code>TOKEN_CLS</code> or <code>QUESTION_ANS</code> or <code>FEATURE_EXTRACTION</code></p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#weighteddatasetdefinition","title":"WeightedDatasetDefinition","text":"<p>Define a dataset, with a weight for sampling</p>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_16","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description path <code>string</code> \u2705 string Local path to a JSONL file in the finetuning data format sampling_weight <code>number</code> number <code>1.0</code>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#weightedmixdatainput","title":"WeightedMixDataInput","text":"<p>A list of datasets where each is sampled by a certain weight</p> <p>These datasets are interleaved based on the sampling weights. The resulting dataset is fully precomputed, upto the point where every single sample in every dataset gets picked. This means that with small sampling weights, it can take a lot of draws to see every sample from a dataset and so the resulting dataset can be very large.</p> <p>The datasets themselves need to be in the finetuning supported JSONL formats. For SFT this means lines:</p> <pre><code>{\"messages\": {\"content\": \"string\", \"role\": \"string\"}}\n</code></pre> <p>For DPO this means lines of:</p> <pre><code>{\"prompt_messages\": {\"content\": \"string\", \"role\": \"string\"}, \"chosen_messages\": {\"content\": \"string\", \"role\": \"string\"}, \"rejected_messages\": {\"content\": \"string\", \"role\": \"string\"}}\n</code></pre>"},{"location":"ai-workloads-manifests/llm-finetune-silogen-engine/helm/silogen_finetuning_config_readme/#type-object_17","title":"Type: <code>object</code>","text":"Property Type Required Possible values Default Description type <code>const</code> \u2705 <code>PRECOMPUTE_WEIGHTED_MIX</code> datasets <code>array</code> \u2705 WeightedDatasetDefinition data_type <code>string</code> string <code>\"ChatConversation\"</code> generally, the data_type is automatically set based on the experiment config method seed <code>integer</code> integer <code>19851243</code> Seed for the random number generator for interleaving draws"},{"location":"ai-workloads-manifests/llm-inference-llamacpp-mi300x/helm/","title":"LLM Inference Service with Llama.cpp","text":"<p>This Helm chart deploys a LLM inference service workload via llama.cpp</p>"},{"location":"ai-workloads-manifests/llm-inference-llamacpp-mi300x/helm/#prerequisites","title":"Prerequisites","text":"<ol> <li>Helm: Install <code>helm</code>. Refer to the Helm documentation for instructions.</li> <li>Secrets: (Optional) Create the secrets <code>minio-credentials</code> with keys <code>minio-access-key</code> and <code>minio-secret-key</code> in the namespace if you want to download pre-built executables and models from MinIO.</li> </ol>"},{"location":"ai-workloads-manifests/llm-inference-llamacpp-mi300x/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>Basic configurations are defined in the <code>values.yaml</code> file.</p> <p>The default model is 1.73-bit quantized DeepSeek-R1-UD-IQ1_M, which  fits in one MI300X GPU and can serve with a context length of 4K.</p> <p>For example: run the following command within the <code>helm/</code> folder to deploy the service:</p> <pre><code>helm template . --set env_vars.TEMP=\"0.8\" | kubectl apply -f -\n</code></pre> <p>Note: Compiling llama.cpp executables and downloading/merging the GGUF files of DeepSeek R1 (~200GB) from HuggingFace can take a significant amount of time. The deployment process may take over 30 minutes before the LLM inference service is ready.</p>"},{"location":"ai-workloads-manifests/llm-inference-llamacpp-mi300x/helm/#interacting-with-the-deployed-model","title":"Interacting with the Deployed Model","text":""},{"location":"ai-workloads-manifests/llm-inference-llamacpp-mi300x/helm/#verify-deployment","title":"Verify Deployment","text":"<p>Check the deployment and service status:</p> <pre><code>kubectl get deployment\nkubectl get service\n</code></pre>"},{"location":"ai-workloads-manifests/llm-inference-llamacpp-mi300x/helm/#port-forwarding","title":"Port Forwarding","text":"<p>To access the service locally, forward the port using the following commands. This assumes the service name is <code>llm-inference-llamacpp</code>:</p> <pre><code>kubectl port-forward services/llm-inference-llamacpp 8080:80\n</code></pre> <p>You can access the Llama.cpp server's WebUI at <code>http://localhost:8080</code> using a web browser.</p> <p>Additionally, an OpenAI-compatible API endpoint is available at <code>http://localhost:8080/v1</code></p>"},{"location":"ai-workloads-manifests/llm-inference-ollama/helm/","title":"LLM Inference Service with Llama.cpp","text":"<p>This Helm chart deploys a LLM inference service workload via Ollama</p>"},{"location":"ai-workloads-manifests/llm-inference-ollama/helm/#prerequisites","title":"Prerequisites","text":"<p>Install <code>helm</code>. Refer to the Helm documentation for instructions.</p>"},{"location":"ai-workloads-manifests/llm-inference-ollama/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>Basic configurations for the deployment are specified in the <code>values.yaml</code> file. By default, the service uses the quantized <code>Gemma3:4b</code> model. For a comprehensive list of available models, visit the Ollama Model Library.</p> <p>For example: run the following command within the <code>helm/</code> folder to deploy the service:</p> <pre><code>helm template . --set env_vars.MODEL=\"gemma3:27b-it-fp16\" | kubectl apply -f -\n</code></pre> <p>Note: Compiling Ollama executables and downloading models can take a significant amount of time. The deployment process may take over 10 minutes before the LLM inference service is ready.</p>"},{"location":"ai-workloads-manifests/llm-inference-ollama/helm/#interacting-with-the-deployed-model","title":"Interacting with the Deployed Model","text":""},{"location":"ai-workloads-manifests/llm-inference-ollama/helm/#verify-deployment","title":"Verify Deployment","text":"<p>Check the deployment and service status:</p> <pre><code>kubectl get deployment\nkubectl get service\n</code></pre>"},{"location":"ai-workloads-manifests/llm-inference-ollama/helm/#port-forwarding","title":"Port Forwarding","text":"<p>To access the service locally, forward the port using the following commands. This assumes the service name is <code>llm-inference-ollama</code>:</p> <pre><code>kubectl port-forward services/llm-inference-ollama 8080:80\n</code></pre> <p>Note Ollama server provides both Ollama API <code>http://localhost:8080/api</code> and OpenAI-compatible API <code>http://localhost:8080/v1</code></p>"},{"location":"ai-workloads-manifests/llm-inference-openai-benchmark-guidellm/helm/","title":"OpenAI-compatible Endpoint Benchmarking using GuideLLM","text":""},{"location":"ai-workloads-manifests/llm-inference-openai-benchmark-guidellm/helm/#overview","title":"Overview","text":"<p>This Helm chart deploys a batch job that benchmarks LLM performance against an OpenAI-compatible API endpoint using GuideLLM.</p>"},{"location":"ai-workloads-manifests/llm-inference-openai-benchmark-guidellm/helm/#prerequisites","title":"Prerequisites","text":"<ul> <li>Helm: Helm installed on your system</li> </ul>"},{"location":"ai-workloads-manifests/llm-inference-openai-benchmark-guidellm/helm/#configuration","title":"Configuration","text":"Parameter Description Default <code>env_vars.OPENAI_API_BASE_URL</code> OpenAI-compatible API endpoint URL <code>\"http://example-open-ai-api-server.com/v1\"</code> <code>env_vars.TOKENIZER</code> HuggingFace model repository for token calculations <code>deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</code> <code>env_vars.HF_TOKEN</code> HuggingFace access token (required for gated models like Mistral or Llama) not set"},{"location":"ai-workloads-manifests/llm-inference-openai-benchmark-guidellm/helm/#deployment","title":"Deployment","text":"<p>Deploy the benchmark job with the following command:</p> <pre><code>helm template . --set env_vars.OPENAI_API_BASE_URL=\"http://your-api-endpoint.com/v1/\" | kubectl apply -f -\n</code></pre> <p>Monitor job progress with <code>kubectl logs</code>, and review the benchmark results in the final output of the completed job.</p> <p>Example output:</p> <pre><code>\u256d\u2500 GuideLLM Benchmarks Report (stdout) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Benchmark Report 1 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Backend(type=openai_server,                                              \u2502 \u2502\n\u2502 \u2502 target=http://llm-inference-vllm-llama-4-maverick/v1,                    \u2502 \u2502\n\u2502 \u2502 model=meta-llama/Llama-4-Maverick-17B-128E-Instruct)                     \u2502 \u2502\n\u2502 \u2502 Data(type=emulated, source=prompt_tokens=512,generated_tokens=128,       \u2502 \u2502\n\u2502 \u2502 tokenizer=meta-llama/Llama-4-Maverick-17B-128E-Instruct)                 \u2502 \u2502\n\u2502 \u2502 Rate(type=sweep, rate=None)                                              \u2502 \u2502\n\u2502 \u2502 Limits(max_number=None requests, max_duration=120 sec)                   \u2502 \u2502\n\u2502 \u2502                                                                          \u2502 \u2502\n\u2502 \u2502                                                                          \u2502 \u2502\n\u2502 \u2502 Requests Data by Benchmark                                               \u2502 \u2502\n\u2502 \u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2502 \u2502\n\u2502 \u2502 \u2503           \u2503 Requests  \u2503 Request   \u2503           \u2503 Start     \u2503          \u2503 \u2502 \u2502\n\u2502 \u2502 \u2503 Benchmark \u2503 Completed \u2503 Failed    \u2503 Duration  \u2503 Time      \u2503 End Time \u2503 \u2502 \u2502\n\u2502 \u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 \u2502\n\u2502 \u2502 \u2502 synchron\u2026 \u2502 101/101   \u2502 0/101     \u2502 119.00    \u2502 08:21:17  \u2502 08:23:16 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502           \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 354/354   \u2502 0/354     \u2502 119.80    \u2502 08:23:43  \u2502 08:25:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 607/607   \u2502 0/607     \u2502 119.99    \u2502 08:25:43  \u2502 08:27:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 854/854   \u2502 0/854     \u2502 119.93    \u2502 08:27:43  \u2502 08:29:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 1099/1099 \u2502 0/1099    \u2502 119.96    \u2502 08:29:43  \u2502 08:31:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 1331/1331 \u2502 0/1331    \u2502 119.98    \u2502 08:31:43  \u2502 08:33:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 1578/1578 \u2502 0/1578    \u2502 119.49    \u2502 08:33:43  \u2502 08:35:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 1798/1798 \u2502 0/1798    \u2502 119.90    \u2502 08:35:43  \u2502 08:37:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 1885/1885 \u2502 0/1885    \u2502 119.92    \u2502 08:37:43  \u2502 08:39:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 2026/2026 \u2502 0/2026    \u2502 119.35    \u2502 08:39:44  \u2502 08:41:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502           \u2502           \u2502 sec       \u2502           \u2502          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 throughp\u2026 \u2502 505/505   \u2502 0/505     \u2502 25.14 sec \u2502 08:23:18  \u2502 08:23:43 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502 \u2502                                                                          \u2502 \u2502\n\u2502 \u2502 Tokens Data by Benchmark                                                 \u2502 \u2502\n\u2502 \u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2502 \u2502\n\u2502 \u2502 \u2503                 \u2503        \u2503 Prompt (1%,    \u2503        \u2503                 \u2503 \u2502 \u2502\n\u2502 \u2502 \u2503                 \u2503        \u2503 5%, 50%, 95%,  \u2503        \u2503 Output (1%, 5%, \u2503 \u2502 \u2502\n\u2502 \u2502 \u2503 Benchmark       \u2503 Prompt \u2503 99%)           \u2503 Output \u2503 50%, 95%, 99%)  \u2503 \u2502 \u2502\n\u2502 \u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 \u2502\n\u2502 \u2502 \u2502 synchronous     \u2502 512.49 \u2502 512.0, 512.0,  \u2502 127.84 \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@2\u2026 \u2502 512.33 \u2502 512.0, 512.0,  \u2502 127.70 \u2502 114.7, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@5\u2026 \u2502 512.37 \u2502 512.0, 512.0,  \u2502 127.61 \u2502 112.3, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@7\u2026 \u2502 512.37 \u2502 512.0, 512.0,  \u2502 127.73 \u2502 115.1, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@9\u2026 \u2502 512.34 \u2502 512.0, 512.0,  \u2502 127.86 \u2502 125.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@1\u2026 \u2502 512.33 \u2502 512.0, 512.0,  \u2502 127.75 \u2502 119.3, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@1\u2026 \u2502 512.34 \u2502 512.0, 512.0,  \u2502 127.75 \u2502 118.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@1\u2026 \u2502 512.36 \u2502 512.0, 512.0,  \u2502 127.78 \u2502 119.9, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@1\u2026 \u2502 512.36 \u2502 512.0, 512.0,  \u2502 127.79 \u2502 118.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@2\u2026 \u2502 512.36 \u2502 512.0, 512.0,  \u2502 127.79 \u2502 123.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 throughput      \u2502 512.38 \u2502 512.0, 512.0,  \u2502 127.72 \u2502 116.3, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 512.0, 514.0,  \u2502        \u2502 128.0, 128.0,   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502        \u2502 514.0          \u2502        \u2502 128.0           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502 \u2502                                                                          \u2502 \u2502\n\u2502 \u2502 Performance Stats by Benchmark                                           \u2502 \u2502\n\u2502 \u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2502 \u2502\n\u2502 \u2502 \u2503                 \u2503                 \u2503                \u2503 Inter Token     \u2503 \u2502 \u2502\n\u2502 \u2502 \u2503                 \u2503 Request Latency \u2503 Time to First  \u2503 Latency [1%,    \u2503 \u2502 \u2502\n\u2502 \u2502 \u2503                 \u2503 [1%, 5%, 10%,   \u2503 Token [1%, 5%, \u2503 5%, 10%, 50%,   \u2503 \u2502 \u2502\n\u2502 \u2502 \u2503                 \u2503 50%, 90%, 95%,  \u2503 10%, 50%, 90%, \u2503 90% 95%, 99%]   \u2503 \u2502 \u2502\n\u2502 \u2502 \u2503 Benchmark       \u2503 99%] (sec)      \u2503 95%, 99%] (ms) \u2503 (ms)            \u2503 \u2502 \u2502\n\u2502 \u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 \u2502\n\u2502 \u2502 \u2502 synchronous     \u2502 1.17, 1.17,     \u2502 60.2, 60.7,    \u2502 8.0, 8.6, 8.7,  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 1.17, 1.18,     \u2502 61.0, 62.1,    \u2502 8.7, 8.9, 9.0,  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 1.19, 1.19,     \u2502 63.6, 64.2,    \u2502 9.4             \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 1.20            \u2502 69.3           \u2502                 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@2\u2026 \u2502 1.46, 1.59,     \u2502 57.7, 58.7,    \u2502 0.2, 10.2,      \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502 1.59, 1.60,     \u2502 59.5, 63.1,    \u2502 10.3, 10.9,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 1.61, 1.62,     \u2502 68.5, 71.7,    \u2502 12.0, 15.4,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 1.65            \u2502 117.9          \u2502 48.1            \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@5\u2026 \u2502 1.71, 1.91,     \u2502 57.3, 58.2,    \u2502 0.2, 10.9,      \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502 1.91, 1.93,     \u2502 59.1, 64.2,    \u2502 11.4, 11.8,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 1.96, 2.00,     \u2502 72.1, 115.5,   \u2502 16.8, 46.3,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 2.11            \u2502 138.1          \u2502 49.3            \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@7\u2026 \u2502 2.17, 2.39,     \u2502 58.4, 59.4,    \u2502 0.2, 11.8,      \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502 2.42, 2.44,     \u2502 60.2, 64.3,    \u2502 12.7, 13.4,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 2.50, 2.57,     \u2502 74.6, 124.3,   \u2502 47.0, 48.9,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 2.96            \u2502 145.3          \u2502 50.5            \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@9\u2026 \u2502 2.78, 3.05,     \u2502 62.3, 63.4,    \u2502 0.2, 13.9,      \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502 3.09, 3.16,     \u2502 64.3, 67.1,    \u2502 14.4, 18.5,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 3.21, 3.25,     \u2502 81.6, 130.7,   \u2502 48.8, 50.6,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 3.32            \u2502 201.6          \u2502 53.2            \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@1\u2026 \u2502 3.56, 4.29,     \u2502 70.1, 71.5,    \u2502 0.2, 13.1,      \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502 4.36, 4.66,     \u2502 72.2, 78.6,    \u2502 19.8, 27.7,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 4.83, 5.02,     \u2502 104.1, 176.4,  \u2502 53.8, 54.7,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 5.39            \u2502 356.6          \u2502 70.4            \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@1\u2026 \u2502 4.25, 4.53,     \u2502 61.9, 74.5,    \u2502 0.2, 14.3,      \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502 4.61, 4.75,     \u2502 81.1, 149.6,   \u2502 17.4, 25.6,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 5.04, 5.13,     \u2502 460.8, 538.6,  \u2502 59.9, 70.1,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 5.30            \u2502 712.5          \u2502 122.4           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@1\u2026 \u2502 4.11, 4.27,     \u2502 92.2, 217.3,   \u2502 0.2, 13.8,      \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502 4.35, 4.72,     \u2502 345.8, 710.8,  \u2502 15.9, 24.4,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 5.41, 6.05,     \u2502 1255.9,        \u2502 54.8, 57.7,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 6.28            \u2502 1559.3, 2121.3 \u2502 84.8            \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@1\u2026 \u2502 4.11, 4.40,     \u2502 129.0, 666.0,  \u2502 0.1, 0.2, 0.2,  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502 4.71, 8.49,     \u2502 1044.1,        \u2502 22.2, 55.1,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 13.42, 14.13,   \u2502 4703.2,        \u2502 61.2, 112.8     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 14.62           \u2502 9479.4,        \u2502                 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502                 \u2502 10238.6,       \u2502                 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502                 \u2502 10723.4        \u2502                 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchronous@2\u2026 \u2502 3.85, 4.35,     \u2502 137.9, 920.0,  \u2502 0.1, 0.2, 0.2,  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec         \u2502 4.86, 10.38,    \u2502 1499.8,        \u2502 19.8, 54.4,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 17.68, 18.69,   \u2502 6826.9,        \u2502 64.0, 149.5     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 19.42           \u2502 14005.3,       \u2502                 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502                 \u2502 14994.2,       \u2502                 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502                 \u2502 15808.3        \u2502                 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 throughput      \u2502 3.02, 3.24,     \u2502 295.8, 497.8,  \u2502 0.1, 0.2, 0.2,  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 3.35, 6.70,     \u2502 692.8, 4160.1, \u2502 18.3, 44.3,     \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 9.33, 9.59,     \u2502 6786.1,        \u2502 48.8, 56.4      \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                 \u2502 10.19           \u2502 7115.2, 7739.8 \u2502                 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502 \u2502                                                                          \u2502 \u2502\n\u2502 \u2502 Performance Summary by Benchmark                                         \u2502 \u2502\n\u2502 \u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2502 \u2502\n\u2502 \u2502 \u2503           \u2503 Requests  \u2503           \u2503 Time to   \u2503 Inter     \u2503 Output   \u2503 \u2502 \u2502\n\u2502 \u2502 \u2503           \u2503 per       \u2503 Request   \u2503 First     \u2503 Token     \u2503 Token    \u2503 \u2502 \u2502\n\u2502 \u2502 \u2503 Benchmark \u2503 Second    \u2503 Latency   \u2503 Token     \u2503 Latency   \u2503 Through\u2026 \u2503 \u2502 \u2502\n\u2502 \u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 \u2502\n\u2502 \u2502 \u2502 synchron\u2026 \u2502 0.85      \u2502 1.18 sec  \u2502 62.29 ms  \u2502 8.72 ms   \u2502 108.50   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502           \u2502 req/sec   \u2502           \u2502           \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 2.96      \u2502 1.60 sec  \u2502 65.41 ms  \u2502 12.02 ms  \u2502 377.37   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502 req/sec   \u2502           \u2502           \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 5.06      \u2502 1.93 sec  \u2502 69.15 ms  \u2502 14.61 ms  \u2502 645.58   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502 req/sec   \u2502           \u2502           \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 7.12      \u2502 2.46 sec  \u2502 71.39 ms  \u2502 18.69 ms  \u2502 909.53   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502 req/sec   \u2502           \u2502           \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 9.16      \u2502 3.15 sec  \u2502 76.51 ms  \u2502 23.99 ms  \u2502 1171.39  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502 req/sec   \u2502           \u2502           \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 11.09     \u2502 4.61 sec  \u2502 91.56 ms  \u2502 35.39 ms  \u2502 1417.25  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502 req/sec   \u2502           \u2502           \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 13.21     \u2502 4.78 sec  \u2502 212.79 ms \u2502 35.77 ms  \u2502 1687.03  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502 req/sec   \u2502           \u2502           \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 15.00     \u2502 4.81 sec  \u2502 762.19 ms \u2502 31.71 ms  \u2502 1916.20  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502 req/sec   \u2502           \u2502           \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 15.72     \u2502 8.74 sec  \u2502 4904.15   \u2502 30.04 ms  \u2502 2008.65  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502 req/sec   \u2502           \u2502 ms        \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 asynchro\u2026 \u2502 16.98     \u2502 10.77 sec \u2502 7203.12   \u2502 27.90 ms  \u2502 2169.26  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 req/sec   \u2502 req/sec   \u2502           \u2502 ms        \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 throughp\u2026 \u2502 20.09     \u2502 6.79 sec  \u2502 4279.14   \u2502 19.54 ms  \u2502 2565.91  \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502           \u2502 req/sec   \u2502           \u2502 ms        \u2502           \u2502 tokens/\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"ai-workloads-manifests/llm-inference-openai-benchmark-rocmblog/helm/","title":"OpenAI-compatible Endpoint Benchmarking","text":"<p>This Helm chart defines a batch job to benchmark LLM performance using vLLM's benchmarking script against OpenAI-compatible API endpoints. It follows the best practices for optimized inference on AMD Instinct GPUs.</p>"},{"location":"ai-workloads-manifests/llm-inference-openai-benchmark-rocmblog/helm/#prerequisites-and-configuration","title":"Prerequisites and Configuration","text":"<ol> <li> <p>Helm: Ensure <code>helm</code> is installed. Refer to the Helm documentation for installation instructions.</p> </li> <li> <p>MinIO Storage: Required for saving benchmark results. Configure the following environment variables in <code>values.yaml</code>:</p> <ul> <li><code>BUCKET_STORAGE_HOST</code></li> <li><code>BUCKET_STORAGE_ACCESS_KEY</code></li> <li><code>BUCKET_STORAGE_SECRET_KEY</code></li> <li><code>BUCKET_RESULT_PATH</code></li> </ul> </li> <li> <p>API Endpoint: An OpenAI-compatible API endpoint is required. Configure this in <code>values.yaml</code> as <code>env_vars.OPENAI_API_BASE_URL</code> or override using the <code>--set</code> option with Helm.</p> </li> <li> <p>Tokenizer: Required for token calculations. Specify a HuggingFace model repository in <code>values.yaml</code> by setting <code>env_vars.TOKENIZER</code>. The default is <code>deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</code>.</p> </li> <li> <p>HuggingFace Token (optional): Set the <code>env_vars.HF_TOKEN</code> environment variable if using gated tokenizers (e.g., Mistral and Llama models) from HuggingFace.</p> </li> </ol>"},{"location":"ai-workloads-manifests/llm-inference-openai-benchmark-rocmblog/helm/#deployment-example","title":"Deployment Example","text":"<p>To deploy the chart, run the following command in the <code>helm/</code> directory:</p> <pre><code>helm template . \\\n    --set env_vars.OPENAI_API_BASE_URL=\"http://example-open-ai-api-server.com/v1/\" \\\n    --set env_vars.TOKENIZER=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\" | \\\n    kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads-manifests/llm-inference-sglang/helm/","title":"LLM Inference with SGLang","text":"<p>This Helm Chart deploys the LLM Inference SGLang workload.</p>"},{"location":"ai-workloads-manifests/llm-inference-sglang/helm/#prerequisites","title":"Prerequisites","text":"<p>Ensure the following prerequisites are met before deploying any workloads:</p> <ol> <li>Helm: Install <code>helm</code>. Refer to the Helm documentation for instructions.</li> <li>Secrets: Create the following secrets in the namespace:<ul> <li><code>minio-credentials</code> with keys <code>minio-access-key</code> and <code>minio-secret-key</code>.</li> <li><code>hf-token</code> with key <code>hf-token</code>.</li> </ul> </li> </ol>"},{"location":"ai-workloads-manifests/llm-inference-sglang/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>It is recommended to use <code>helm template</code> and pipe the result to <code>kubectl apply</code> , rather than using <code>helm install</code>. Generally, a command looks as follows <pre><code>helm template [optional-release-name] &lt;helm-dir&gt; -f &lt;overrides/xyz.yaml&gt; --set &lt;name&gt;=&lt;value&gt; | kubectl apply -n &lt;namespace&gt; -f -\n</code></pre></p> <p>The chart provides three main ways to deploy models, detailed below.</p>"},{"location":"ai-workloads-manifests/llm-inference-sglang/helm/#alternative-1-deploy-a-specific-model-configuration","title":"Alternative 1: Deploy a Specific Model Configuration","text":"<p>To deploy a specific model along with its settings, use the following command from the <code>helm</code> directory:</p> <pre><code>helm template tiny-llama . -f overrides/models/tinyllama_tinyllama-1.1b-chat-v1.0.yaml | kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads-manifests/llm-inference-sglang/helm/#alternative-2-override-the-model","title":"Alternative 2: Override the Model","text":"<p>You can also override the model on the command line:</p> <pre><code>helm template qwen2-0-5b . --set model=Qwen/Qwen2-0.5B-Instruct | kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads-manifests/llm-inference-sglang/helm/#alternative-3-deploy-a-model-from-bucket-storage","title":"Alternative 3: Deploy a Model from Bucket Storage","text":"<p>If you have downloaded your model to bucket storage, use:</p> <pre><code>helm template qwen2-0-5b . --set model=s3://models/Qwen/Qwen2-0.5B-Instruct | kubectl apply -f -\n</code></pre> <p>The model will be automatically downloaded before starting the inference server.</p>"},{"location":"ai-workloads-manifests/llm-inference-sglang/helm/#user-input-values","title":"User Input Values","text":"<p>Refer to the <code>values.yaml</code> file for the user input values you can provide, along with instructions.</p>"},{"location":"ai-workloads-manifests/llm-inference-sglang/helm/#interacting-with-deployed-model","title":"Interacting with Deployed Model","text":""},{"location":"ai-workloads-manifests/llm-inference-sglang/helm/#verify-deployment","title":"Verify Deployment","text":"<p>Check the deployment status:</p> <pre><code>kubectl get deployment\n</code></pre>"},{"location":"ai-workloads-manifests/llm-inference-sglang/helm/#port-forwarding","title":"Port Forwarding","text":"<p>Forward the port to access the service (assuming the service is named <code>llm-inference-sglang-tiny-llama</code> ):</p> <pre><code>kubectl port-forward deployments/llm-inference-sglang-tiny-llama 8080:80\n</code></pre>"},{"location":"ai-workloads-manifests/llm-inference-sglang/helm/#test-the-deployment","title":"Test the Deployment","text":"<p>Send a test request to verify the service, assuming <code>TinyLlama/TinyLlama-1.1B-Chat-v1.0</code> model:</p> <pre><code>curl http://localhost:8080/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n        ]\n    }'\n</code></pre>"},{"location":"ai-workloads-manifests/llm-inference-tgi/helm/","title":"LLM Inference with TGI","text":"<p>This Helm Chart deploys the LLM Inference vLLM workload.</p>"},{"location":"ai-workloads-manifests/llm-inference-tgi/helm/#prerequisites","title":"Prerequisites","text":"<p>Ensure the following prerequisites are met before deploying any workloads:</p> <ol> <li>Helm: Install <code>helm</code>. Refer to the Helm documentation for instructions.</li> <li>Secrets: Create the following secrets in the namespace:<ul> <li><code>minio-credentials</code> with keys <code>minio-access-key</code> and <code>minio-secret-key</code>.</li> <li><code>hf-token</code> with key <code>hf-token</code>.</li> </ul> </li> </ol>"},{"location":"ai-workloads-manifests/llm-inference-tgi/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>It is recommended to use <code>helm template</code> and pipe the result to <code>kubectl create</code>, rather than using <code>helm install</code>. Generally, a command looks as follows <pre><code>helm template [optional-release-name] &lt;helm-dir&gt; -f &lt;overrides/xyz.yaml&gt; --set &lt;name&gt;=&lt;value&gt; | kubectl apply -f -\n</code></pre></p> <p>The chart provides three main ways to deploy models, detailed below.</p>"},{"location":"ai-workloads-manifests/llm-inference-tgi/helm/#alternative-1-deploy-a-specific-model-configuration","title":"Alternative 1: Deploy a Specific Model Configuration","text":"<p>To deploy a specific model along with its settings, use the following command from the <code>helm</code> directory:</p> <pre><code>helm template tiny-llama . -f overrides/models/tinyllama_tinyllama-1.1b-chat-v1.0.yaml | kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads-manifests/llm-inference-tgi/helm/#alternative-2-override-the-model","title":"Alternative 2: Override the Model","text":"<p>You can also override the model on the command line:</p> <pre><code>helm template qwen2-0-5b . --set model=Qwen/Qwen2-0.5B-Instruct | kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads-manifests/llm-inference-tgi/helm/#alternative-3-deploy-a-model-from-bucket-storage","title":"Alternative 3: Deploy a Model from Bucket Storage","text":"<p>If you have downloaded your model to bucket storage, use:</p> <pre><code>helm template qwen2-0-5b . --set model=s3://models/Qwen/Qwen2-0.5B-Instruct | kubectl apply -f -\n</code></pre> <p>The model will be automatically downloaded before starting the inference server.</p>"},{"location":"ai-workloads-manifests/llm-inference-tgi/helm/#user-input-values","title":"User Input Values","text":"<p>Refer to the <code>values.yaml</code> file for the user input values you can provide, along with instructions.</p>"},{"location":"ai-workloads-manifests/llm-inference-tgi/helm/#interacting-with-deployed-model","title":"Interacting with Deployed Model","text":""},{"location":"ai-workloads-manifests/llm-inference-tgi/helm/#verify-deployment","title":"Verify Deployment","text":"<p>Check the deployment status:</p> <pre><code>kubectl get deployment\n</code></pre>"},{"location":"ai-workloads-manifests/llm-inference-tgi/helm/#port-forwarding","title":"Port Forwarding","text":"<p>Forward the port to access the service (assuming the deployment is named <code>llm-inference-vllm-tiny-llama</code> ):</p> <pre><code>kubectl port-forward deployments/llm-inference-vllm-tiny-llama 8080:8080\n</code></pre>"},{"location":"ai-workloads-manifests/llm-inference-tgi/helm/#test-the-deployment","title":"Test the Deployment","text":"<p>Send a test request to verify the service, assuming <code>TinyLlama/TinyLlama-1.1B-Chat-v1.0</code> model:</p> <pre><code>curl http://localhost:8080/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -X POST \\\n    -d '{\n        \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n        ]\n    }'\n</code></pre>"},{"location":"ai-workloads-manifests/llm-inference-vllm/helm/","title":"LLM Inference with vLLM","text":"<p>This Helm Chart deploys the LLM Inference vLLM workload.</p>"},{"location":"ai-workloads-manifests/llm-inference-vllm/helm/#prerequisites","title":"Prerequisites","text":"<p>Ensure the following prerequisites are met before deploying any workloads:</p> <ol> <li>Helm: Install <code>helm</code>. Refer to the Helm documentation for instructions.</li> <li>Secrets: Create the following secrets in the namespace:<ul> <li><code>minio-credentials</code> with keys <code>minio-access-key</code> and <code>minio-secret-key</code>.</li> <li><code>hf-token</code> with key <code>hf-token</code>.</li> </ul> </li> </ol>"},{"location":"ai-workloads-manifests/llm-inference-vllm/helm/#deploying-the-workload","title":"Deploying the Workload","text":"<p>It is recommended to use <code>helm template</code> and pipe the result to <code>kubectl create</code> , rather than using <code>helm install</code>. Generally, a command looks as follows</p> <pre><code>helm template [optional-release-name] &lt;helm-dir&gt; -f &lt;overrides/xyz.yaml&gt; --set &lt;name&gt;=&lt;value&gt; | kubectl apply -f -\n</code></pre> <p>The chart provides three main ways to deploy models, detailed below.</p>"},{"location":"ai-workloads-manifests/llm-inference-vllm/helm/#alternative-1-deploy-a-specific-model-configuration","title":"Alternative 1: Deploy a Specific Model Configuration","text":"<p>To deploy a specific model along with its settings, use the following command from the <code>helm</code> directory:</p> <pre><code>helm template tiny-llama . -f overrides/models/tinyllama_tinyllama-1.1b-chat-v1.0.yaml | kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads-manifests/llm-inference-vllm/helm/#alternative-2-override-the-model","title":"Alternative 2: Override the Model","text":"<p>You can also override the model on the command line:</p> <pre><code>helm template qwen2-0-5b . --set model=Qwen/Qwen2-0.5B-Instruct | kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads-manifests/llm-inference-vllm/helm/#alternative-3-deploy-a-model-from-bucket-storage","title":"Alternative 3: Deploy a Model from Bucket Storage","text":"<p>If you have downloaded your model to bucket storage, use:</p> <pre><code>helm template qwen2-0-5b . --set model=s3://models/Qwen/Qwen2-0.5B-Instruct | kubectl apply -f -\n</code></pre> <p>The model will be automatically downloaded before starting the inference server.</p>"},{"location":"ai-workloads-manifests/llm-inference-vllm/helm/#user-input-values","title":"User Input Values","text":"<p>Refer to the <code>values.yaml</code> file for the user input values you can provide, along with instructions.</p>"},{"location":"ai-workloads-manifests/llm-inference-vllm/helm/#interacting-with-deployed-model","title":"Interacting with Deployed Model","text":""},{"location":"ai-workloads-manifests/llm-inference-vllm/helm/#verify-deployment","title":"Verify Deployment","text":"<p>Check the deployment status:</p> <pre><code>kubectl get deployment\n</code></pre>"},{"location":"ai-workloads-manifests/llm-inference-vllm/helm/#port-forwarding","title":"Port Forwarding","text":"<p>Forward the port to access the service (assuming the deployment is named <code>llm-inference-vllm-tiny-llama</code> ):</p> <pre><code>kubectl port-forward deployments/llm-inference-vllm-tiny-llama 8080:8080\n</code></pre>"},{"location":"ai-workloads-manifests/llm-inference-vllm/helm/#test-the-deployment","title":"Test the Deployment","text":"<p>Send a test request to verify the service, assuming <code>TinyLlama/TinyLlama-1.1B-Chat-v1.0</code> model:</p> <pre><code>curl http://localhost:8080/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n        ]\n    }'\n</code></pre>"},{"location":"ai-workloads-manifests/llm-inference-vllm-benchmark-mad/helm/","title":"LLM Inference Benchmarking Workload","text":"<p>This Helm chart submits a job to benchmark the performance of vLLM running a model in the same container.</p>"},{"location":"ai-workloads-manifests/llm-inference-vllm-benchmark-mad/helm/#prerequisites","title":"Prerequisites","text":"<ol> <li>Helm: Install <code>helm</code>. Refer to the Helm documentation for instructions.</li> <li> <p>MinIO Storage (optional): To use pre-downloaded model weights from MinIO storage, the following environment variables must be set, otherwise models will be downloaded from HuggingFace. MinIO storage is also used for saving benchmark results.</p> <ul> <li><code>BUCKET_STORAGE_HOST</code></li> <li><code>BUCKET_STORAGE_ACCESS_KEY</code></li> <li><code>BUCKET_STORAGE_SECRET_KEY</code></li> <li><code>BUCKET_MODEL_PATH</code></li> </ul> </li> <li> <p>HF Token (optional): If you need to download gated models from HuggingFace (e.g., Mistral and LLaMA 3.x) that are not available locally, ensure a secret named <code>hf-token</code> exists in the namespace.</p> </li> </ol>"},{"location":"ai-workloads-manifests/llm-inference-vllm-benchmark-mad/helm/#implementation","title":"Implementation","text":"<p>Basic configurations are defined in the <code>values.yaml</code> file, with key settings:</p> <ul> <li><code>env_vars.TESTOPT</code>: Must be set to either \"latency\" or \"throughput\"</li> <li><code>env_vars.USE_MAD</code>: Controls whether to apply the MAD approach (see below)</li> </ul> <p>Note: If the specified model cannot be found locally, the workload will attempt to download it from HuggingFace.</p>"},{"location":"ai-workloads-manifests/llm-inference-vllm-benchmark-mad/helm/#a-scenario-specific-approach","title":"A. Scenario-specific approach","text":"<p>In this approach (<code>env_vars.USE_SCENARIO</code> is not \"false\"), scenarios are defined in the <code>mount/scenarios_{$TESTOPT}.csv</code> file. Modify this file to specify models, parameters, and environment variables for benchmarking. Each column defines a parameter or variable, and each row represents a unique scenario to benchmark.</p> <p>The default configuration benchmarks latency using benchmark_latency.py from vLLM. Setting <code>env_vars.TESTOPT</code> to \"throughput\" will use benchmark_throughput.py instead.</p> <p>Example 1: Benchmark latency scenarios (default) <pre><code>helm template . | kubectl apply -f -\n</code></pre></p> <p>Example 2: Benchmark throughput scenarios <pre><code>helm template . --set env_vars.TESTOPT=\"throughput\" | kubectl apply -f -\n</code></pre></p>"},{"location":"ai-workloads-manifests/llm-inference-vllm-benchmark-mad/helm/#b-rocmmad-standalone-approach","title":"B. ROCm/MAD standalone approach","text":"<p>When <code>env_vars.USE_MAD</code> is not \"false\", the ROCm/MAD repository will be cloned. The specified model (<code>env_vars.MAD_MODEL</code>) will be benchmarked according to preset scripts.</p> <p>Example 3: Benchmark using MAD standalone approach with override settings <pre><code>helm template . -f overrides/methods/MAD-Qwen2.5_0.5B.yaml | kubectl apply -f -\n</code></pre></p>"},{"location":"ai-workloads-manifests/llm-inference-vllm-benchmark-rocmblog/helm/","title":"LLM Inference Benchmarking Workload (ROCm Best Practices)","text":"<p>This Helm chart deploys a job to benchmark the performance of vLLM running a model within the same container. It follows the best practices for optimized inference on AMD Instinct GPUs.</p>"},{"location":"ai-workloads-manifests/llm-inference-vllm-benchmark-rocmblog/helm/#prerequisites","title":"Prerequisites","text":"<ol> <li>Helm: Ensure <code>helm</code> is installed. Refer to the Helm documentation for installation instructions.</li> <li> <p>MinIO Storage (optional): To use pre-downloaded model weights from MinIO storage, set the following environment variables. If not set, models will be downloaded from HuggingFace. MinIO storage is also used for saving benchmark results:</p> <ul> <li><code>BUCKET_STORAGE_HOST</code></li> <li><code>BUCKET_STORAGE_ACCESS_KEY</code></li> <li><code>BUCKET_STORAGE_SECRET_KEY</code></li> <li><code>BUCKET_MODEL_PATH</code></li> </ul> </li> <li> <p>HuggingFace Token (optional): Required for downloading gated models (e.g., Mistral and LLaMA 3.x) from HuggingFace if they are not available locally.</p> </li> </ol>"},{"location":"ai-workloads-manifests/llm-inference-vllm-benchmark-rocmblog/helm/#implementation","title":"Implementation","text":"<p>Basic configurations are defined in the <code>values.yaml</code> file. YAML files in the <code>overrides/models/</code> directory can be used to reproduce benchmarks for specific scenarios, such as models, tensor parallelism, data types, quantization, etc.</p>"},{"location":"ai-workloads-manifests/llm-inference-vllm-benchmark-rocmblog/helm/#example-benchmarking-a-specific-model-configuration","title":"Example: Benchmarking a Specific Model Configuration","text":"<p>To benchmark a specific model (e.g., Mistral-7B-Instruct-v0.3-FP8) with its settings, run the following command from the <code>helm</code> directory:</p> <pre><code>helm template . -f overrides/models/Mistral-7B-Instruct-v0.3-FP8.TP2.yaml | kubectl apply -f -\n</code></pre> <p>The benchmark results will be displayed at the end of the job log. An example result as the following:</p> <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     256\nBenchmark duration (s):                  63.26\nTotal input tokens:                      524288\nTotal generated tokens:                  524288\nRequest throughput (req/s):              4.05\nOutput token throughput (tok/s):         8287.48\nTotal Token throughput (tok/s):          16574.96\n---------------Time to First Token----------------\nMean TTFT (ms):                          5749.02\nMedian TTFT (ms):                        5569.11\nP99 TTFT (ms):                           10835.37\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          28.06\nMedian TPOT (ms):                        28.15\nP99 TPOT (ms):                           30.52\n---------------Inter-token Latency----------------\nMean ITL (ms):                           28.06\nMedian ITL (ms):                         25.17\nP99 ITL (ms):                            40.60\n----------------End-to-end Latency----------------\nMean E2EL (ms):                          63192.59\nMedian E2EL (ms):                        63190.51\nP99 E2EL (ms):                           63229.48\n==================================================\n</code></pre>"},{"location":"ai-workloads-manifests/llm-pretraining-megatron-lm/","title":"Megatron-LM CPT templates","text":""},{"location":"ai-workloads-manifests/llm-pretraining-megatron-lm/#helm","title":"Helm","text":"<p>To generate manifests and print them in standard output using the default <code>values.yaml</code>, for example, run: <pre><code>helm template ./ --name-template 8b\n</code></pre> This will generate a kubernetes manifest with a job and a configmap both named <code>llm-pretraining-megatron-lm-job-8b</code> in the user's active namespace.</p> <p>To override the default values, a specific file can be passed using <code>-f</code> flag, for example: <pre><code>helm template ./ --name-template 70b -f overrides/values-llama-70b-cpt.yaml\n</code></pre></p> <p>Multiple overriding yaml files can be used e.g.: <pre><code>helm template ./ --name-template 70b \\\n    -f overrides/values-llama-70b-cpt.yaml \\\n    -f overrides/labels/kaiwo-managed-true.yaml\n</code></pre></p> <p>Note: Anything overlapping with the default <code>values.yaml</code> file can be omitted from the specific files passed with the <code>-f</code> flag.</p>"},{"location":"ai-workloads-manifests/llm-pretraining-megatron-lm/#running","title":"Running","text":"<p>To run the workload, simply pipe the generated manifests to a <code>kubectl</code> command, like:</p> <pre><code>helm template ./ --name-template 8b |\u00a0kubectl apply -f -\n</code></pre>"},{"location":"ai-workloads-manifests/llm-pretraining-megatron-lm/#docker-image","title":"Docker image","text":"<p>We recommend using the <code>rocm/megatron-lm:v25.4</code> image or later versions of it.</p> <p>Note: this workload has been tested with images: - <code>rocm/megatron-lm:v25.4</code></p>"},{"location":"ai-workloads-manifests/llm-pretraining-megatron-lm/#assumptions","title":"Assumptions","text":"<p>Some assumptions for running the pretraining jobs are as follows: The initial model checkpoint and data, both in Megatron format, are located in an S3-compatible storage. Additionally, it is assumed that a secret containing the S3 storage provider's HMAC credentials (access key and secret key) is present in the namespace where the jobs are executed. The defaults (as viewed from the Kubernetes manifest's perspective) are:</p> <pre><code>- name: BUCKET_STORAGE_ACCESS_KEY\n    valueFrom:\n    secretKeyRef:\n        name: minio-credentials\n        key: minio-access-key\n- name: BUCKET_STORAGE_SECRET_KEY\n    valueFrom:\n    secretKeyRef:\n        name: minio-credentials\n        key: minio-secret-key\n</code></pre>"},{"location":"ai-workloads-manifests/llm-pretraining-megatron-lm/#cleanup","title":"Cleanup","text":"<p>Note that this chart, when applied with <code>kubectl</code>, will create Job and ConfigMap objects. After the Job has finished, there is a 3600-second grace period to remove the Job object from the namespace. However, the cleanup of the ConfigMap must be done manually. For example, to delete the ConfigMap <code>llm-pretraining-megatron-lm-job-8b</code> from the user's active namespace:</p> <pre><code>kubectl delete configmap llm-pretraining-megatron-lm-job-8b\n</code></pre> <p>Alternatively one can use the garbage collection utility script <code>../utils/gc.sh</code> to manage cleanup automatically. After the workload is submitted to the kubernetes cluster run <code>../utils/gc.sh &lt;job-name&gt; &lt;namespace&gt;</code> to attach configmap to the lifecycle of the job object, e.g., for the above example:</p> <pre><code>../utils/gc.sh llm-pretraining-megatron-lm-job-8b &lt;ACTIVE_NAMESPACE&gt;\n</code></pre>"},{"location":"ai-workloads-manifests/prepare-data-for-megatron-lm/helm/","title":"Workload helm template to download and preprocess data for Megatron-LM and upload it to bucket storage","text":"<p>This workload downloads data and a Huggingface tokenizer, preprocesses the data to the format accepted by Megatron-LM framework, and uploads it to bucket storage. To launch the workload, resolve the helm template with <code>helm template</code> and pipe the result to <code>kubectl apply</code>:</p> <p>Example: <pre><code>helm template workloads/prepare-data-for-megatron-lm/helm \\\n    -f workloads/prepare-data-for-megatron-lm/helm/overrides/fineweb-data-sample.yaml \\\n    --name-template prepare-fineweb-data \\\n    | kubectl apply -f -\n</code></pre></p>"},{"location":"ai-workloads-manifests/prepare-data-for-megatron-lm/helm/#user-inputs","title":"User inputs","text":"<p>See the <code>values.yaml</code> file for all user input values that you can provide, with instructions.</p> <p>The data download logic should be implemented in a user-provided script under the key <code>dataScript</code>. The script must accept <code>--target-dir</code> as an argument, and output a JSONL file in the directory specified by that argument. The only requirement for the JSONL file is that the same json key in each line contains text paragraphs to be trained on. The specific json key that is used for extracting text data is specified as parameter for the next preprocessing step. For example, for <code>HuggingFaceFW/fineweb-edu</code> dataset this json key name is <code>text</code>.</p> <p>Further preprocessing logic that transforms JSONL data to the format accepted by Megatron-LM framwework is implemented in the <code>mount/prepare_data.sh</code> script. This script accepts, in particular, <code>json_key</code> as its 4-th argument, that points to the json key that stores training text data in JSONL file from previous step.</p> <p>At the moment we choose to ask the user to provide the data download script instead of just a few parameters like dataset identifier, because HuggingFace datasets don't have a standard format that can be always directly applied for every use case.</p>"},{"location":"core/docs/airman/dashboard/","title":"Dashboard","text":"<p>The dashboard provides users with an overview of the cluster and the workloads running on it.</p>","tags":["airman","resource management","dashboard"]},{"location":"core/docs/airman/dashboard/#clusters-and-nodes","title":"Clusters and nodes","text":"<p>This section shows the number of onboarded clusters, the number of GPU nodes in the clusters, the total number of GPUs across all clusters, and the number of allocated GPUs via quotas.</p> <p></p>","tags":["airman","resource management","dashboard"]},{"location":"core/docs/airman/dashboard/#allocations-and-workloads","title":"Allocations and workloads","text":"<p>This section displays current statistics for workloads, such as GPU utilization, the number of running workloads, and the number of pending workloads. It also lists resource usage by project.</p> <p></p> <p>GPU memory and device utilization are shown in a graph over time. Users can change the time scale of these graphs using the buttons above the graph.</p> <p></p>","tags":["airman","resource management","dashboard"]},{"location":"core/docs/airman/overview/","title":"AI Resource Manager overview","text":"<p>The SiloGen platform provides administrators with tools to oversee and control the platform\u2019s computational resources and user access. These features are managed through a module called Airman, short for AI Resource Manager. Its key capabilities include cluster management, monitoring, and maintaining teams' access to computational resources.</p>","tags":["airman","overview","features"]},{"location":"core/docs/airman/overview/#airman-features","title":"Airman features","text":"<p>Airman is built around the basic usage pattern of maintaining compute resources, setting up teams and projects, and allowing individual users to utilize the resources for their compute needs.</p> <ul> <li>Cluster: The physical part of the platform installation, which can be managed in the Airman user interface.</li> <li>Organization: An organization is built from teams. Each team can have multiple users and multiple projects.</li> <li>Projects: A project contains users and a quota for their workloads. Multiple users can belong to multiple projects.</li> <li>Quota: A quota is a usage limit reserved for a project. Quotas are useful for ensuring everyone gets their fair share of compute resources.</li> <li>User: Users are individuals who require compute access for work purposes.</li> </ul>","tags":["airman","overview","features"]},{"location":"core/docs/airman/clusters/add-clusters-ui/","title":"Create a new cluster in the UI","text":"<p>This article explains how to create a new cluster in the Airman user interface.</p> <p>To add a new cluster:</p> <ol> <li>Click the \"Add Cluster\" button.</li> <li>Cluster identification - Set a unique identifier for your cluster.</li> <li>Set the connection parameters and click the \"Verify Connection\" button.</li> <li>Verify connection - Wait for the connection verification to complete.</li> <li>Complete the next steps in the process.</li> </ol> <p></p>","tags":["airman","cluster","add cluster"]},{"location":"core/docs/airman/clusters/add-clusters-ui/#delete-a-cluster","title":"Delete a cluster","text":"<ol> <li>Select the cluster you want to delete.</li> <li>Click \"Delete\" in the right menu of the selected cluster.</li> </ol>","tags":["airman","cluster","add cluster"]},{"location":"core/docs/airman/clusters/overview/","title":"Clusters overview","text":"<p>The Clusters page provides a quick and easy way to see the status of all your clusters.</p>","tags":["airman","cluster"]},{"location":"core/docs/airman/clusters/overview/#what-is-a-cluster","title":"What is a cluster?","text":"<p>A cluster is a set of nodes (interconnected computers) that work together to run containerized applications, including AI/ML models, data pipelines, and inference services.</p> <p>A cluster is the physical part of the platform installation, which can be managed in the Airman user interface.</p>","tags":["airman","cluster"]},{"location":"core/docs/airman/clusters/overview/#clusters-overview-page","title":"Clusters overview page","text":"<p>The clusters overview provides a list of the clusters added to the SiloGen platform, along with their status. You can see the following information about your clusters:</p> <p>Clusters: Clusters for your organization that have been onboarded to AI Resource Manager. Onboarding clusters allows you to manage resources, projects, and workloads on them.</p> <p>Available nodes: The total number of nodes available across all clusters. AI Resource Manager routinely pulls node information from clusters, including resources and health.</p> <p>Allocated GPUs: The total number of GPUs allocated to project quotas across all clusters. Users belonging to projects with GPU quotas are guaranteed allocated GPU resources for their workloads in the corresponding clusters.</p> <p>Running workloads: The total number of active workloads across all clusters. Workloads submitted via AI Resource Manager are optimally scheduled, tracked, and monitored for resource usage and health.</p> <p>Clusters table</p> Column Description Name The name of the cluster Status Status of the cluster Nodes The number of nodes associated with this cluster. Shows the number of healthy nodes and total nodes. GPU allocation The share of the total number of GPUs allocated to quotas in the cluster. Users belonging to projects with GPU quotas are guaranteed allocated GPU resources for their workloads. CPU allocation The share of the total number of CPUs allocated to quotas in the cluster. Users belonging to projects with CPU quotas are guaranteed allocated CPU resources for their workloads. Memory allocation The share of the total amount of memory allocated to quotas in the cluster. Users belonging to projects with memory quotas are guaranteed allocated memory resources for their workloads. <p></p>","tags":["airman","cluster"]},{"location":"core/docs/airman/clusters/overview/#single-cluster-status","title":"Single cluster status","text":"<p>The Cluster page provides a quick and easy way to see the status of your cluster.</p> <p>Available nodes: The total number of nodes available in the cluster. AI Resource Manager routinely pulls node information from the cluster, including resources and health.</p> <p>Assigned quotas: Quotas assigned to the cluster resources. Each project has a quota to limit the amount of resources they are guaranteed on the cluster.</p> <p>Allocated GPUs: The total number of GPUs allocated to quotas in the cluster. Users belonging to projects with GPU quotas are guaranteed allocated GPU resources for their workloads.</p> <p>Running workloads: The total number of active workloads running on the cluster. Workloads submitted via AI Resource Manager are optimally scheduled, tracked, and monitored for resource usage and health.</p> <p>Projects table</p> Column Description Name Unique identifier for the projects Status Status of the project GPU allocation Total number of GPUs allocated to this project CPU allocation Total number of GPUs allocated to this project <p>Nodes table</p> Column Description Name Name of the node Status Status of the node CPU cores Number of CPU cores available on the node CPU memory Amount of memory available on the node GPU type Type of GPU GPU devices Number of GPUs on the node GPU memory Amount of GPU memory available on the node <p></p>","tags":["airman","cluster"]},{"location":"core/docs/airman/projects/manage-projects/","title":"Manage projects","text":"<p>A project is a container for managing work. It has a specified compute quota, which is used to limit resources. Users can initiate workloads only in the projects they are assigned to. Only platform administrators can create new projects and assign users to them.</p> <p>The main projects view lists available projects, the number of assigned users, and a short description of each project. To add a new project, click the \"Create project\" button.</p> <p></p> <p>Clicking a project's name opens the edit view. This view has a form for editing project details. The project's name is permanent and cannot be changed. Click the \"Save changes\" button to apply updates.</p> <p>Below the form is an area called \"Danger zone\". It contains a button for deleting the project. Deleting a project is a permanent action and should not be done without proper consideration.</p> <p>The right pane lists users assigned to the project and provides an option to add new project members. Administrators can also invite new users directly to the project.</p>","tags":["projects","airman"]},{"location":"core/docs/airman/projects/project-dashboard/","title":"Project dashboard","text":"<p>The project dashboard breaks down all the attributes of a project into an easy to read user interface. This is also the page from which to edit the project details.</p> <p>The project dashboard can be set to display information for the past hour, 24 hours and 7 days. Manually refreshing the information with the Refresh button will fetch the latest information.</p>","tags":["projects","airman"]},{"location":"core/docs/airman/projects/project-dashboard/#workloads-panel","title":"Workloads panel","text":"<p>This panel shows the cluster name as well as the number of workloads in a particular state. More detailed information can then be found by looking at the workloads table.</p>","tags":["projects","airman"]},{"location":"core/docs/airman/projects/project-dashboard/#wait-time-panel","title":"Wait time panel","text":"<p>This panel shows the average time that workloads in the given project spent waiting to start (within the specified time range). This includes both the total time each workload was in a pending state, plus any additional time still pending if the workload is currently waiting to run.</p>","tags":["projects","airman"]},{"location":"core/docs/airman/projects/project-dashboard/#quota-utilization","title":"Quota utilization","text":"<p>The average ratio of utilized GPUs versus the allocated GPUs for all workloads running within the project (within the specified time range).</p>","tags":["projects","airman"]},{"location":"core/docs/airman/projects/project-dashboard/#gpu-idle-time","title":"GPU idle time","text":"<p>The GPU idle time panel shows the average time that GPUs allocated to the given project spent idle (not actively used) during the specified date range.</p> <p>For each point in time, the system looks at how many GPUs were allocated to the project and how many were actually being used. The difference between these two numbers represents the number of idle GPUs at that moment. This difference is then multiplied by the time interval (step size) to get the total idle time for that period.</p>","tags":["projects","airman"]},{"location":"core/docs/airman/projects/project-dashboard/#gpu-device-usage","title":"GPU device usage","text":"<p>The number of GPU devices utilized by workloads in this project versus the GPUs allocated to it (within the specified time range).</p>","tags":["projects","airman"]},{"location":"core/docs/airman/projects/project-dashboard/#gpu-vram-usage","title":"GPU VRAM usage","text":"<p>The amount of GPU VRAM GPU utilized by workloads in this project versus the VRAM allocated to them, via the allocated GPUs (within the specified time range).</p>","tags":["projects","airman"]},{"location":"core/docs/airman/projects/project-dashboard/#workloads-table","title":"Workloads table","text":"<p>The workloads table lists all the workloads associated with the current project. Additionally each entry in the table also includes the following information:</p> <ul> <li>type: the type of workload</li> <li>status: the status of the workload</li> <li>GPU and VRAM usage</li> <li>run time: the total execution time for the workload</li> <li>which project member created the workload</li> </ul>","tags":["projects","airman"]},{"location":"core/docs/airman/projects/project-dashboard/#users-table","title":"Users table","text":"<p>The users table lists every member of the project including the role and the last time the logged in.</p>","tags":["projects","airman"]},{"location":"core/docs/airman/projects/project-quotas/","title":"Project quotas overview","text":"<p>A quota is a usage limit set on a project. Teams use project quotas to accomplish their tasks by ensuring everyone gets their fair share of compute resources. Without quotas, some workloads might use all the resources, leaving other workloads waiting.</p> <p>Quotas define a minimum set of resources that are guaranteed. There is no maximum, so quotas can be exceeded if unused resources are available.</p> <p>In Airman, you can manage GPU, CPU, memory, and disk quotas for your projects. When a new project is created, the quota for all resources is zero. Only after the project is created can the quotas be updated.</p> <p>Note</p> <p>In systems with only one project, the quota settings aren't really needed. Only after having a second project does quota management become important.</p>","tags":["airman","quota","project"]},{"location":"core/docs/airman/projects/project-quotas/#quota-status","title":"Quota status","text":"<p>In Airman, a project is a way to separate whatever tasks a team is working on. The status of the project is the status of those tasks.</p> <p>For the most part, Airman works with AI-focused tasks, which are primarily resource-heavy workloads. In order for the workload to run, the cluster needs to be made aware of the quota requested by the project.</p> <p>Currently, the project status reflects the state of this application of the quota to the cluster.</p> <p></p>","tags":["airman","quota","project"]},{"location":"core/docs/airman/projects/project-quotas/#manage-quotas","title":"Manage quotas","text":"<p>This article explains how to define quotas for your projects. In Airman, you can set quotas for GPUs and CPUs for your projects. Users belonging to a project are guaranteed the allocated quota for their AI workloads.</p>","tags":["airman","quota","project"]},{"location":"core/docs/airman/projects/project-quotas/#guaranteed-quota-panel","title":"Guaranteed quota panel","text":"<p>Project quota can be adjusted using the sliders or by inputting the required allocations into the fields.</p> <p></p> <p>GPU allocation \u2013 Number of GPUs allocated for this project. Users belonging to this project are guaranteed the allocated GPU quota for their workloads.</p> <p>CPU allocation \u2013 Number of CPUs allocated for this project. Users belonging to this project are guaranteed the allocated CPU quota for their workloads.</p> <p>System memory \u2013 Memory allocation for this project. Users belonging to this project are guaranteed the allocated memory for their workloads.</p> <p>Ephemeral disk allocation \u2013 Disk storage allocation for this project. Users belonging to this project are guaranteed the allocated storage for their workloads.</p>","tags":["airman","quota","project"]},{"location":"core/docs/airman/users/manage-users/","title":"Manage users","text":"<p>Users enter the SiloGen platform with their email address after being granted access. User management takes place in the Access control section found in the navigation.</p>","tags":["user management","invite users","airman"]},{"location":"core/docs/airman/users/manage-users/#main-view","title":"Main view","text":"<p>The main user management view lists users with all relevant information. The user's name and email address identify who the user is, and the role indicates their capabilities on the platform. The projects column shows the project the user belongs to. If there is more than one project, hovering over the project name will show all of them. Above the list is a search field that can search by name and address. Below the user list is pagination for navigating the user list.</p> <p></p>","tags":["user management","invite users","airman"]},{"location":"core/docs/airman/users/manage-users/#user-view","title":"User view","text":"<p>Clicking a user's name in the list switches to the user view. It displays a form for updating the user's name. The user's email address cannot be changed in the platform. Click the \"Save changes\" button to apply the update.</p> <p>The right pane lists the user's assignments to different projects and their dedicated user role. Users with the appropriate role can adjust individual users' projects or change their role.</p> <p></p>","tags":["user management","invite users","airman"]},{"location":"core/docs/airman/users/manage-users/#invite-users","title":"Invite users","text":"<p>Inviting new users happens on a special page or via the action button in the User directory. The main view in the Invited users section lists all invited users before they have accepted and activated their accounts. The invitation flow asks for the user's email address, role, and a project they should be assigned to.</p> <p>Only users within allowed domains are accepted on the platform. Platform administrators can update the organization's allowed domains.</p> <p></p>","tags":["user management","invite users","airman"]},{"location":"core/docs/airman/users/manage-users/#organization-with-an-identity-provider","title":"Organization with an identity provider","text":"<p>If the organization has an identity provider (IDP) linked and active, the user invitation functionality is disabled, as user management is handled by the identity provider. Users can set up their system to use an identity provider with these instructions.</p>","tags":["user management","invite users","airman"]},{"location":"core/docs/airman/users/manage-users/#delete-users","title":"Delete users","text":"<p>At the bottom of the user edit page is an area called the Danger zone. It contains a button for deleting the user account. Deleting an account is a permanent action and should not be done without proper consideration.</p> <p>Only platform administrators can remove users.</p>","tags":["user management","invite users","airman"]},{"location":"core/docs/airman/users/overview/","title":"Users overview","text":"<p>The SiloGen platform has a robust user management system bundled with the authentication system Keycloak. The platform can also use an identity provider for user authentication.</p> <p>Tools for managing the organization and user access are found in the navigation under the titles Organization and Access control.</p>","tags":["user management","keycloak","roles","airman"]},{"location":"core/docs/airman/users/overview/#roles","title":"Roles","text":"<p>Airman and Developer Center have two different user roles: platform admin and team member.</p> <p>The platform admin has access to everything on the platform. When submitting workloads, they follow the regular project rules, meaning they can submit workloads only to clusters where they have a project. Administrators can change a user's role.</p> <p>The team member has several limitations in their workflows. A team member can:</p> <ul> <li>view clusters if any of their projects are in the cluster</li> <li>submit workloads to their projects</li> <li>view workloads in their projects</li> <li>delete workloads from their projects.</li> </ul> <p>The super administrator is a special role reserved for platform engineers, who perform maintenance and other background tasks, such as onboarding new organizations.</p>","tags":["user management","keycloak","roles","airman"]},{"location":"core/docs/airman/users/overview/#adjusting-user-roles","title":"Adjusting user roles","text":"<p>Roles are currently fixed and cannot be adjusted in individual SiloGen platform installations.</p>","tags":["user management","keycloak","roles","airman"]},{"location":"core/docs/developer-center/overview/","title":"AI Development overview (Developer Center)","text":"<p>The SiloGen Developer Center is an interface for developers to easily manage the lifecycle of their Generative AI stack. The Developer Center provides an easy-to-use no-code/low-code option for running and managing AI workloads. This article lays the foundation for how to develop, run, and manage AI workloads in the Developer Center.</p> <p></p>","tags":["developer center","model catalog","workloads"]},{"location":"core/docs/developer-center/overview/#developer-center-functionality","title":"Developer Center functionality","text":"<p>The Developer Center includes the following capabilities:</p>","tags":["developer center","model catalog","workloads"]},{"location":"core/docs/developer-center/overview/#developer-tooling","title":"Developer tooling","text":"<p>The Developer Center provides developers with tools and frameworks to accelerate AI development and experimentation, featuring a comprehensive catalog of optimized AI workloads and models for AMD compute. The workloads include the most common developer tools and frameworks, such as Jupyter Notebooks, Visual Studio Code, and popular frameworks like PyTorch and TensorFlow.</p>","tags":["developer center","model catalog","workloads"]},{"location":"core/docs/developer-center/overview/#chat-and-compare","title":"Chat and compare","text":"<p>The chat page allows developers to experiment with models they have access to\u2014developers can modify several retrieval and generation parameters to see how they affect the model's response. The model comparison view allows developers to compare the output of different models using the same set of settings.</p>","tags":["developer center","model catalog","workloads"]},{"location":"core/docs/developer-center/overview/#training-fine-tuning","title":"Training &amp; fine-tuning","text":"<p>Fine-tuning a model allows developers to customize it for their specific use case and data. SiloGen provides a certified list of base models that developers can fine-tune, and allows customization of certain hyperparameters to achieve the best results.</p>","tags":["developer center","model catalog","workloads"]},{"location":"core/docs/developer-center/overview/#gpu-as-a-service","title":"GPU-as-a-Service","text":"<p>The Developer Center provides developers with self-service access to workspaces with GPU resources. Platform admins can set project-based quotas for GPU usage so teams always have the right amount of resources available.</p>","tags":["developer center","model catalog","workloads"]},{"location":"core/docs/developer-center/overview/#ai-models","title":"AI models","text":"<p>The Developer Center offers a comprehensive catalog of curated open-source models, including state-of-the-art large language models (LLMs) and image/video generation models. Developers can easily discover, fine-tune, and deploy these models for their AI use cases.</p>","tags":["developer center","model catalog","workloads"]},{"location":"core/docs/developer-center/overview/#running-ai-workloads-on-the-command-line","title":"Running AI workloads on the command-line","text":"<p>Developers can also deploy and run AI workloads through the command-line interface using pre-validated and open-source workloads published as Kubernetes manifests and Helm charts. The list of AI workloads is continuously updated.</p> <p>See more details on how to run workloads on the command-line.</p>","tags":["developer center","model catalog","workloads"]},{"location":"core/docs/developer-center/workloads/","title":"Workloads","text":"<p>A workload is a batch job or a service running in the cluster with resources defined by the workload and limited by the quota that the workload owner has. This page shows all started workloads. The view shows all workload statuses by default except <code>Deleted</code>.</p> <p>If a user belongs to multiple projects, the project needs to be selected first from the top of the page.</p> <p></p> <p>The paginated table shows the name of the workload, the cluster the workload is assigned to, and how many resources the workload requires. The <code>Status</code> column shows the last known status of the workload and is not updated automatically. Filters can be used to find workloads by name, type, or status.</p> <p></p>","tags":["developer center","workloads","workspaces"]},{"location":"core/docs/developer-center/workloads/#actions","title":"Actions","text":"<p>Workload actions are available from the Actions column by pressing the three-dot button.</p> <p></p>","tags":["developer center","workloads","workspaces"]},{"location":"core/docs/developer-center/workloads/#show-details","title":"Show details","text":"<p>Shows details of the workload, such as when it was created and by whom.</p>","tags":["developer center","workloads","workspaces"]},{"location":"core/docs/developer-center/workloads/#delete","title":"Delete","text":"<p>Queues the workload for deletion.</p>","tags":["developer center","workloads","workspaces"]},{"location":"core/docs/developer-center/workloads/#open-workspace","title":"Open workspace","text":"<p>If the workload type is <code>Workspace</code>, selecting this action opens the workspace in a new browser tab.</p>","tags":["developer center","workloads","workspaces"]},{"location":"core/docs/developer-center/workloads/#workload-types","title":"Workload types","text":"Type Description Model Download Batch job for downloading a new AI model Inference Inference service Training Model training batch job which generates a new model Pre-Training Model pre-training batch job which generates a new base model Fine-Tuning Model fine-tuning batch job which generates a new model Evaluation Workload for evaluating model performance Workspace Workspace for model experiments","tags":["developer center","workloads","workspaces"]},{"location":"core/docs/developer-center/workloads/#workload-statuses","title":"Workload statuses","text":"Status Description Condition Added Workload has been created Workload component creation has not started Pending Waiting to start All components are in the <code>Pending</code> state Running Workload is being executed Any component is in the <code>Running</code> state Complete Workload has finished successfully All components are in the <code>Completed</code> state Failed An error has occurred and the workload did not complete Any component is in the <code>Failed</code> state Deleting Workload is queued for removal Delete started, but not all components are <code>Deleted</code> Deleted Workload successfully deleted All components are in the <code>Deleted</code> state Delete Failed Delete failed and manual cleanup might be needed Any component is in the <code>Delete Failed</code> state Terminated Execution has been terminated All components are in the <code>Completed</code> or <code>Deleted</code> state Unknown Status cannot be determined","tags":["developer center","workloads","workspaces"]},{"location":"core/docs/developer-center/workspaces/","title":"Workspaces","text":"<p>Workspaces provide different kinds of interactive environments optimized for AMD compute. For example, JupyterLab and Visual Studio Code workspaces allow users to leverage the power of the cluster with zero configuration on the client computer.</p>","tags":["developer center","workspaces","workloads"]},{"location":"core/docs/developer-center/workspaces/#deploy-a-workspace","title":"Deploy a workspace","text":"<p>You can deploy a workspace by clicking \"View and deploy\", which opens the workload deployment view.</p> <p></p> <p>From the deployment view, you can change the workspace name if you wish. The default resource allocation should be sufficient for most workspaces, but it is possible to customize these settings if needed.</p> <p>Once the values have been set, press \"Quick deploy\" to deploy the workspace. It may take a while for the workspace to start, but once it has started, the deployment overlay will show a \"Launch\" button, which can be used to access the workspace. The workspace can also be accessed later from the workloads page.</p> <p></p>","tags":["developer center","workspaces","workloads"]},{"location":"core/docs/developer-center/inference/chat/","title":"Chat","text":"<p>The chat page allows you to experiment with models you have access to. You can modify several parameters to see how they affect the model's response.</p>","tags":["developer center","chat","rag"]},{"location":"core/docs/developer-center/inference/chat/#accessing-the-chat","title":"Accessing the chat","text":"<p>Navigate to the AI Developer Center \u2192 <code>Chat and Compare</code> to access the chat.</p> <p>You will be able to choose from the list of models you have access to.</p> <p>Expand the settings toggle to view and modify the generation parameters.</p> <p></p>","tags":["developer center","chat","rag"]},{"location":"core/docs/developer-center/inference/chat/#inspecting-the-debug-output-of-the-model","title":"Inspecting the debug output of the model","text":"<p>Once you have a response from the model, you can inspect the messages sent to the model, the context retrieved as part of generation, and the consumed tokens by clicking the \"bug\" icon next to the response.</p> <p></p> <p></p>","tags":["developer center","chat","rag"]},{"location":"core/docs/developer-center/inference/compare/","title":"Comparison","text":"<p>The model comparison view allows you to compare the output of different models using the same set of settings.</p> <p>A common use case is to compare the response of your model before and after fine-tuning, or to test how different settings affect the model's output.</p>","tags":["developer center","chat","comparison"]},{"location":"core/docs/developer-center/inference/compare/#accessing-the-model-comparison","title":"Accessing the model comparison","text":"<p>Navigate to the corresponding page in the AI Developer Center \u2192 <code>Chat and Compare</code> \u2192 <code>Compare</code> to access the Comparison.</p> <p>You will be able to choose from the list of models you have access to.</p> <p>Similar to the chat, you can expand the settings toggle to view and modify the generation parameters.</p>","tags":["developer center","chat","comparison"]},{"location":"core/docs/developer-center/inference/compare/#inspecting-the-debug-output-of-the-model","title":"Inspecting the debug output of the model","text":"<p>Also similar to the chat, you can click the \"bug\" icon to inspect the messages sent to the model,the context retrieved as part of generation, and the consumed tokens.</p> <p></p>","tags":["developer center","chat","comparison"]},{"location":"core/docs/developer-center/inference/how-to-deploy-and-inference/","title":"How to deploy a model and run inference","text":"<p>Training AI models and developing your own models are essential parts of AI workflows, but eventually they lead to using the models in your business applications. Solving problems, making predictions, and processing data is called inference. This is the culmination of AI workflows, providing you with business value.</p> <p>In short, an AI model requires data for training it properly. The model applies learned patterns to analyze new inputs. With new data, the model generates predictions, classifications, or other types of responses. In the end, the results are used to make decisions and provide services.</p> <p>This tutorial takes you through the following steps. Utilizing AI models in business applications is essential in any AI development workflow. This simple tutorial guides you to pick a model in SiloGen, deploy it and trial its endpoints in Jupyter. This tutorial takes you through the following steps:</p> <ol> <li>Deploy a model from the model catalog.</li> <li>Find information about the model endpoints.</li> <li>Connect to a model endpoint for inference.</li> </ol>","tags":["developer center","deploy","models","inference","tutorial"]},{"location":"core/docs/developer-center/inference/how-to-deploy-and-inference/#1-deploy-a-model","title":"1. Deploy a model","text":"<p>Navigate to the <code>Models</code> page and find a model with the tag Instruction. Chat models are fun, but their responses can be too conversational and not very helpful for specific tasks. Instruction models are fine-tuned to be helpful and try to answer questions or act based on the given instructions.</p> <p>One of these models is Qwen 2.5 1.5B Instruct (note that it has the word \"Instruct\" in the name), but you can pick any of the available ones. If it isn't already downloaded, click the Download button. The download might take a while.</p> <p>Otherwise, proceed with deploying the model by clicking Deploy. No need to update any deployment parameters. This might take a while, so it's better to stretch your legs or do something else in the meantime. You can follow the deployment progress on the <code>Workloads</code> page or the <code>Active deployments</code> page.</p> <p></p> <p>The community models page provides you with a variety of AI models with different capabilities. You can use the filters to find interesting models for your inference needs.</p>","tags":["developer center","deploy","models","inference","tutorial"]},{"location":"core/docs/developer-center/inference/how-to-deploy-and-inference/#2-find-model-endpoints","title":"2. Find model endpoints","text":"<p>You can find model information on the <code>Community</code> page. Find the deployed model and it should have a \"Connect\" button. Clicking it reveals a dialog window giving you the required information for connecting to the model for inference.</p> <p></p> <p>External URL is the address you need for connections outside of your current platform.</p> <p>Internal URL is the address that is used for connections inside your current platform.</p> <p>The content box holds sample code for testing out the model. Click the copy button in the top right corner. The copy button looks like two overlaid squares.</p> <p>If you want to end the tutorial here, you can just click the \"Open in chat\" button and continue with testing your new AI model.</p>","tags":["developer center","deploy","models","inference","tutorial"]},{"location":"core/docs/developer-center/inference/how-to-deploy-and-inference/#3-connect-to-the-endpoint","title":"3. Connect to the endpoint","text":"<p>To test the model endpoints, the easiest way is to go to the <code>Workspaces</code> page. On the page, look for JupyterLab and click \"View and deploy\". To keep things simple, click \"Quick deploy\" at the bottom of the newly opened drawer. You can change the name or allocated resources, but those aren't necessary for testing the workspace.</p> <p></p> <p>After the short deployment process, you can click the \"Launch\" button either in the drawer or, if you closed it, on the Workspaces page. This will open a JupyterLab browser tab, where you can run Python code or do other types of development inside the platform.</p> <p></p> <p>Be sure to open the directory named with a UUID. This directory's contents shouldn't be wiped if something unexpected happens to the JupyterLab deployment. Click the \"Python 3\" square in the right side pane. This will create an empty Python file; in JupyterLab these are called Python notebooks. You can name the file by right-clicking the filename in the left file list or in the tab above the code window.</p> <p></p> <p>Paste the previously copied Python code into the code window. A few notes about the code: - BASE_URL is the internal URL mentioned previously in the connection parameter dialog window. - The JSON area has a few lines of note:   - \"model\" is the model's name prepended with a path to the model.   - \"messages\" \"content\" is a message sent to the chat model.</p> <p>Click the play/run button in the top area of the code window. The button looks like a right-facing triangle. This sends the message contents to the model and returns a response formatted in JSON format. In the response, you can spot a line titled 'content'. This is the chat response from the model.</p> <p>For a nice greeting \"Hello\", the chat model should answer you with something nice as a response. You can change the \"content\" field in the initial JSON, click the play/run button once more, and expect another kind of response.</p> <p>You should be able to use the same code in external applications. Just be sure to replace the BASE_URL variable with the external URL found in the connection parameters. This concludes the tutorial.</p>","tags":["developer center","deploy","models","inference","tutorial"]},{"location":"core/docs/developer-center/inference/overview/","title":"Model inference","text":"<p>Training AI models and developing your own models are essential parts of AI workflows. Solving problems, making predictions, and processing data is called inference. You can use the SiloGen platform for all of these use cases.</p> <p>In short, an AI model requires data for training it properly. The model applies learned patterns to analyze new inputs. With new data, the model generates predictions, classifications, or other types of responses. In the end, the results are used to make decisions and provide services.</p> <p>The SiloGen platform has user-friendly features for experimentation and deploying AI models.</p>","tags":["developer center","inference","chat"]},{"location":"core/docs/developer-center/inference/overview/#developer-center-chat","title":"Developer Center \u2013 Chat","text":"<p>The chat page allows you to experiment with models you have access to. You can modify several parameters to see how they affect the model's response. Users can test and compare chat models and quickly switch between views. For more details, see the Chat or Compare pages.</p>","tags":["developer center","inference","chat"]},{"location":"core/docs/developer-center/inference/overview/#model-deployment-and-inference","title":"Model deployment and inference","text":"<p>Read our tutorial on how to deploy a model and run inference to get a kickstart on these features on the SiloGen platform.</p>","tags":["developer center","inference","chat"]},{"location":"core/docs/developer-center/inference/overview/#other-tutorials-and-examples","title":"Other tutorials and examples","text":"<ul> <li>Model inference using SGLang</li> <li>Model inference using vLLM</li> <li>Using your own model and data</li> </ul>","tags":["developer center","inference","chat"]},{"location":"core/docs/developer-center/training/datasets/","title":"Datasets","text":"<p>SiloGen allows you to create and manage datasets on the platform for fine-tuning purposes. Datasets are collections of data points, where each data point represents a conversation, with or without the model's expected response, depending on the use case.</p>","tags":["developer center","datasets","fine-tuning"]},{"location":"core/docs/developer-center/training/datasets/#uploading-a-dataset","title":"Uploading a dataset","text":"<p>Navigate to the AI Developer Center \u2192 <code>Datasets</code> to upload a dataset. You can provide a name and description for the dataset, and upload a file in JSONL format.</p> <p>The type of dataset should be selected when uploading the file, and the format of each row should correspond to the selected dataset type.</p> <p></p>","tags":["developer center","datasets","fine-tuning"]},{"location":"core/docs/developer-center/training/datasets/#data-formats","title":"Data formats","text":"<p>Depending on the selected dataset type, each row in the JSONL file should be formatted as shown below.</p> <p>Info</p> <p>In a real JSONL file, each object must be on a single line. Here, objects are formatted for readability.</p>","tags":["developer center","datasets","fine-tuning"]},{"location":"core/docs/developer-center/training/datasets/#fine-tuning","title":"Fine-tuning","text":"<pre><code>{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a helpful Geography bot. Only answer questions about Geography.\"\n    },\n    { \"role\": \"user\", \"content\": \"What is the capital of France?\" },\n    { \"role\": \"assistant\", \"content\": \"Paris\" }\n  ]\n}\n</code></pre>","tags":["developer center","datasets","fine-tuning"]},{"location":"core/docs/developer-center/training/datasets/#direct-preference","title":"Direct preference","text":"<pre><code>{\n  \"prompt_messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a helpful Geography bot. Only answer questions about Geography.\"\n    },\n    { \"role\": \"user\", \"content\": \"What is the capital of France?\" }\n  ],\n  \"chosen_messages\": [{ \"role\": \"assistant\", \"content\": \"Paris\" }],\n  \"rejected_messages\": [{ \"role\": \"assistant\", \"content\": \"I do not know\" }]\n}\n</code></pre>","tags":["developer center","datasets","fine-tuning"]},{"location":"core/docs/developer-center/training/datasets/#archiving-datasets","title":"Archiving datasets","text":"<p>If you have identified a dataset that is no longer relevant to your use cases, you can choose to archive it by selecting the dataset(s) and clicking the \"Archive\" button in the \"Actions\" menu.</p> <p></p>","tags":["developer center","datasets","fine-tuning"]},{"location":"core/docs/developer-center/training/datasets/#unarchiving-datasets","title":"Unarchiving datasets","text":"<p>To unarchive a dataset, select the \"Archived\" filter option, choose the dataset(s) you would like to unarchive, and click the \"Unarchive\" button in the \"Actions\" menu.</p> <p></p>","tags":["developer center","datasets","fine-tuning"]},{"location":"core/docs/developer-center/training/fine-tuning/","title":"Fine-tuning","text":"<p>Fine-tuning a model allows you to customize it for your specific use case with your data. We provide a certified list of base models that you can fine-tune, and allow you to customize certain hyperparameters to achieve the best results.</p> <p>Fine-tuned models can be deployed and subsequently used for inference once the model weights have been computed.</p>","tags":["developer center","fine-tuning"]},{"location":"core/docs/developer-center/training/fine-tuning/#getting-ready-to-fine-tune-your-model","title":"Getting ready to fine-tune your model","text":"","tags":["developer center","fine-tuning"]},{"location":"core/docs/developer-center/training/fine-tuning/#uploading-training-data","title":"Uploading training data","text":"<p>One of the first steps toward fine-tuning your model is to upload training data to our platform. The training data should represent a wide range of conversations that you would like your model to respond to as part of inference.</p> <p>Once you have procured training data for your model, navigate to the \"Datasets\" page of the Developer Center to upload the dataset. We currently support uploading datasets in a JSONL format, where each row represents a separate chat conversation. The format of each row should correspond to what is defined in the specification for each datapoint.</p> <p>Click the \"Upload\" button and drop your JSONL file in, providing a name and description.</p> <p></p>","tags":["developer center","fine-tuning"]},{"location":"core/docs/developer-center/training/fine-tuning/#base-models-for-fine-tuning","title":"Base models for fine-tuning","text":"<p>We continuously work to certify base models for fine-tuning. It is recommended that you first identify a base model appropriate for your use case and use that for fine-tuning. If you are missing a specific base model, reach out to our customer success team so they can either provision access or fast-track certification.</p> <p>You can browse the list of accessible base models by navigating back to the AI Developer Center \u2192 <code>Models</code> and clicking the \"Create New\" button.</p>","tags":["developer center","fine-tuning"]},{"location":"core/docs/developer-center/training/fine-tuning/#creating-a-fine-tuned-model","title":"Creating a fine-tuned model","text":"<p>Navigate to the Models page. You can trigger the creation of your fine-tuned model by clicking the \"Fine-Tune Model\" button and selecting the appropriate entries.</p> <p>You must provide your model a name and can optionally specify a description and any of the three hyperparameters: batch size, learning rate multiplier, and number of epochs. If you are unsure of the values to use, leave the fields empty to auto-select the default certified values.</p> <p></p> <p>Once the fine-tuning process has been successfully triggered, you will be able to see your model in the \"Not-deployed\" tab of the page and the fine-tuning run itself in the \"Run status\" section. The fine-tuning run may take several hours to complete, so we recommend visiting the Developer Center occasionally while the run is in progress. If the run fails mid-way, please reach out to our customer success team, and we will help triage your issue.</p>","tags":["developer center","fine-tuning"]},{"location":"core/docs/developer-center/training/fine-tuning/#deploying-a-fine-tuned-model","title":"Deploying a fine-tuned model","text":"<p>Once your model has been successfully trained, the model status will reflect as \"Ready\". This means that the weights have been successfully computed for your model and it can be used for inferencing.</p> <p>Click the \"Deploy\" button corresponding to the model you would like to deploy to make it available for inferencing. Please note that a model, once deployed, can take up to 5 minutes before it can serve requests.</p> <p></p> <p>Once deployed, you can navigate to the \"Chat and Compare\" page to converse with the model.</p> <p>After you have verified that your model performs as expected, you can click the \"View Code\" menu item on the row in the Models page and use the code snippet for inference via the API.</p>","tags":["developer center","fine-tuning"]},{"location":"core/docs/developer-center/training/fine-tuning/#undeploying-a-fine-tuned-model","title":"Undeploying a fine-tuned model","text":"<p>Once you have deployed a version of a fine-tuned model, you might want to \"Undeploy\" it if it has been superseded by another model. You can do this by navigating to the AI Developer Center \u2192 <code>Models</code>, identifying the model in the list of \"Deployed\" models, and clicking the \"Undeploy\" button.</p> <p>This will make the model no longer accessible for inferencing.</p> <p></p>","tags":["developer center","fine-tuning"]},{"location":"core/docs/developer-center/training/fine-tuning/#adapter-merging","title":"Adapter merging","text":"<p>While adapters allow you to deploy multiple fine-tuned models in a cost-efficient way, this comes at a performance penalty. A deployment using an adapter will always be less performant than a deployment of a single model.</p> <p>To enable high-performance use of our fine-tuned models, we allow you to merge an adapter with the base model to create a new model with merged weights. This new model can then be deployed independently of the base model.</p> <p>To merge an adapter back into the base model, click the context menu of the adapter you would like to merge and click the \"Merge\" button. Provide a new unique name for the merged model and click \"Merge.\"</p> <p>Once the merge process is complete, the new model will be visible in the list of models.</p> <p></p> <p></p> <p>You can view the details of the merge process in the \"Run status\" section of the page.</p>","tags":["developer center","fine-tuning"]},{"location":"core/docs/developer-center/training/fine-tuning/#examples","title":"Examples","text":"<ul> <li>Run model fine-tuning workload through Kubernetes CLI</li> <li>Scalable fine-tuning</li> </ul>","tags":["developer center","fine-tuning"]},{"location":"core/docs/developer-center/training/models/","title":"Models","text":"<p>The Models view lists all available models that can be deployed or fine-tuned.</p> <p>The view is divided into three different subpages: community models, custom models, and active deployments. The Community page lists available industry-standard models, such as common large language models. The Custom models page lists all available custom and fine-tuned models. Active deployments lists deployed models and their statuses.</p>","tags":["developer center","model catalog","deployments"]},{"location":"core/docs/developer-center/training/models/#definitions","title":"Definitions","text":"<p>A model can be one of the following types:</p> <ul> <li>Base model: A foundation model trained with vast amounts of data.</li> <li>Merged model: A model that has been improved for specific use cases.</li> <li>Adapter: Small modules that alter the base model's behavior.</li> </ul> <p>Model can be in one of the following states:</p> <ul> <li>Pending: The model is being prepared.</li> <li>Ready: The model is ready for use.</li> <li>Failed: There was an issue loading the model, and it is not available for use.</li> </ul>","tags":["developer center","model catalog","deployments"]},{"location":"core/docs/developer-center/training/models/#subpage-community-models","title":"Subpage: Community models","text":"<p>All available ready-to-deploy open-source models are listed in this view. Users can filter the list using the search field and tags dropdown.</p> <p></p> <p>The model boxes contain helpful information about each model. Along with the name, users can click a link to the external model webpage. Each model also has a short description and tags.</p> <p>The models contain an action button, which can be one of the following:</p> <ul> <li>Download: Downloads a base model into the custom models catalog.</li> <li>Fine-tune: Fine-tune a model that is eligible for it. Read more about fine-tuning.</li> <li>Deploy: Prepares the model for use.</li> <li>Connect: Opens a set of parameters that can be used for connecting your applications to the model. For chattable models, this lets you use the model in the chat window.</li> </ul> <p>The platform's development team maintains the available community models.</p>","tags":["developer center","model catalog","deployments"]},{"location":"core/docs/developer-center/training/models/#subpage-custom-models","title":"Subpage: Custom models","text":"<p>The custom models page lists all available custom models activated from the community models page.</p> <p>Warning</p> <p>The custom models page does not currently allow the implementation of users' own models, such as custom business intelligence solutions.</p> <p></p> <p>The page has a prominent Fine-tune model button, which opens the fine-tuning dialog. Clicking each model's action menu shows a list of available functions: show details, fine-tune, deploy, or delete.</p> <ul> <li>Show details opens a dialog containing relevant information.</li> <li>Fine-tune opens the fine-tuning dialog.</li> <li>Deploy opens a drawer that instructs the user on deploying the model for use.</li> <li>Delete lets users delete their models. We advise caution when using this action, as it is irreversible.</li> </ul>","tags":["developer center","model catalog","deployments"]},{"location":"core/docs/developer-center/training/models/#subpage-active-deployments","title":"Subpage: Active deployments","text":"<p>This view lists active models that are deployed into the cluster. Each model is associated with a lifetime status.</p> <p>From the actions menu, users can view model details or delete models. The filter bar offers various ways to select a subset of models for viewing.</p> <p></p>","tags":["developer center","model catalog","deployments"]},{"location":"core/docs/developer-center/training/overview/","title":"Training and fine-tuning in the SiloGen platform","text":"<p>Note</p> <p>This is a placeholder page for the training and fine-tuning overview.</p>","tags":["developer center","training","fine-tuning"]},{"location":"core/docs/keycloak/manual-user-management/","title":"Manually managing and activating users in Airman","text":"<p>Airman is optimized to support user management where either users are federated via an identity provider (IdP) through SSO or the user management workflow is orchestrated by the Airman UI, relying on access to an SMTP server. If neither option is suitable, you can manage users using the Keycloak admin console and the Airman UI.</p>","tags":["keycloak","login","manual"]},{"location":"core/docs/keycloak/manual-user-management/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>The URL of the keycloak instance for your application, and corresponding admin credentials are known.</p> </li> <li> <p>The keycloak URL is typically https://kc.&lt;domain provided during installation&gt;/admin/master/console/</p> </li> <li> <p>The keycloak credentials should be provided to you by AMD's customer success team during installation.</p> </li> <li> <p>Once you have logged into Keycloak, the correct realm must be selected (typically <code>airm</code>) as all actions are realm-specific.</p> </li> </ul>","tags":["keycloak","login","manual"]},{"location":"core/docs/keycloak/manual-user-management/#updating-the-email-domains-for-organization","title":"Updating the email domain(s) for organization","text":"<p>If you are managing users for your installation, it is likely that the default email domain(s) configured for the organization will need to be updated. This can be done by navigating to the Organizations tab in Keycloak and selecting the pre-configured organization. Subsequently, adjust the Domain of the organization to reflect the email domain(s) associated with your organization (e.g. <code>myorg.com</code> and <code>myorg.org</code>). If you are intending to support users from multiple domains in the platform, add a row for each of the email domains.</p>","tags":["keycloak","login","manual"]},{"location":"core/docs/keycloak/manual-user-management/#manual-user-management","title":"Manual user management","text":"","tags":["keycloak","login","manual"]},{"location":"core/docs/keycloak/manual-user-management/#keycloak","title":"Keycloak","text":"<p>Navigate to the Users in the realm. Here you can click the Add user button to create a new user. You will need to fill in the following fields:</p> <ul> <li>Email verified: Set this to <code>true</code> since Airman requires email verification for users to log in.</li> <li>Email: The email address of the user. This will be the username for the user, and must match any of the domain(s) configured for the organization.</li> <li>First name: The first name of the user.</li> <li>Last name: The last name of the user.</li> </ul> <p>Click Create to create the user. After creating the user, you will be redirected to the user's details page.</p> <ul> <li>Navigate to the Credentials tab and set a password for the user. Make sure to check the Temporary checkbox to ensure that the user is required to change their password on first login.</li> <li>Navigate to the Organizations tab subsequently click the Join organization button and add them to the pre-existing organization (e.g. <code>demo</code>).</li> <li>Send the credentials to the user and ask to them login to the application via the Airman UI.   Note: At this point, while the user can login, they will not have access to any Projects or elevated permissions.</li> </ul>","tags":["keycloak","login","manual"]},{"location":"core/docs/keycloak/manual-user-management/#airman-ui","title":"Airman UI","text":"<p>Once the user has logged in, you can manage their access to projects and permissions via the Airman UI. As a platform administrator, you can find the new in user in the Access Control tab. Select the user and adjust their roles (if needed) and/or add them to the desired projects as described in the Airman user guide.</p> <p>If only the projects that the user has access to has changed, the user can reload the page and the new projects will be visible to them. If their roles have changed, they will need to log out and log back in to see the changes.</p>","tags":["keycloak","login","manual"]},{"location":"core/docs/keycloak/smtp-configuration/","title":"Inviting users and configuring SMTP in Keycloak","text":"<p>This is a guide for configuring an SMTP server in Keycloak, which is a prerequisite for inviting users to Silogen via the application.</p>","tags":["keycloak","login","invitation","smtp"]},{"location":"core/docs/keycloak/smtp-configuration/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>The URL of the keycloak instance for your application, and corresponding admin credentials are known.</p> </li> <li> <p>The keycloak URL is typically https://kc.&lt;domain provided during installation&gt;/admin/master/console/</p> </li> <li> <p>The keycloak credentials should be provided to you by AMD's customer success team during installation.</p> </li> <li> <p>Once you have logged into Keycloak, the correct realm must be selected (typically <code>airm</code>) as all actions are realm-specific.</p> </li> <li>The organization must have an SMTP server accessible that they can use to integrate with Keycloak and Airman.</li> </ul>","tags":["keycloak","login","invitation","smtp"]},{"location":"core/docs/keycloak/smtp-configuration/#updating-the-email-domains-for-organization","title":"Updating the email domain(s) for organization","text":"<p>If you are managing users for your installation, it is likely that the default email domain(s) configured for the organization will need to be updated. This can be done by navigating to the Organizations tab in Keycloak and selecting the pre-configured organization. Subsequently, adjust the Domain of the organization to reflect the email domain(s) associated with your organization (e.g. <code>myorg.com</code> and <code>myorg.org</code>). If you are intending to support users from multiple domains in the platform, add a row for each of the email domains.</p>","tags":["keycloak","login","invitation","smtp"]},{"location":"core/docs/keycloak/smtp-configuration/#configuring-the-smtp-server","title":"Configuring the SMTP server","text":"<p>Navigate to Realm settings and then to the Email tab. Here you can configure the SMTP server that Keycloak will use to send emails. The settings you need to fill in are:</p> <ul> <li>From: The email address that will appear as the sender of the emails sent by Keycloak.</li> <li>Host: The hostname of your SMTP server (e.g., <code>smtp.example.com</code>).</li> <li>Port: The port on which your SMTP server is running (usually <code>587</code> for TLS or <code>465</code> for SSL).</li> <li>Encryption: Choose the encryption method your SMTP server uses (<code>SSL</code> and/or <code>StartTLS</code>). If your SMTP server does not support encryption, you can select neither, but this is not recommended for production environments.</li> <li>Authentication: Enable this if your SMTP server requires authentication. You will need to provide the username and password for the SMTP server.</li> <li>Username: The username for the SMTP server.</li> <li>Password: The password for the SMTP server.</li> </ul> <p>Once this is done, you can test the configuration by sending a test email to ensure that Keycloak can connect to the SMTP server and send emails successfully. Prior to doing this, ensure that the user you have currently logged in as, in the master realm has an associated email that Keycloak can use to send the test email.</p> <p>Note: Depending on the strictness of the spam/junk filters, you may need to whitelist the Keycloak server's IP address or domain in your email server settings to ensure that emails are not marked as spam.</p>","tags":["keycloak","login","invitation","smtp"]},{"location":"core/docs/keycloak/smtp-configuration/#inviting-users","title":"Inviting users","text":"<p>Once the SMTP server is configured, you can invite users via Airman. This is done by logging into Airman and navigating to the Access Control tab and clicking on the Invite user button. You can read more about inviting users in the Airman user guide.</p>","tags":["keycloak","login","invitation","smtp"]},{"location":"core/docs/keycloak/sso/","title":"Adding an identity provider to enable single sign-on","text":"<p>This is a general guide for setting up single sign-on (SSO) with an identity provider. This has been tested with OpenID Connect (Okta), but the guide aims to be provider-agnostic.</p>","tags":["keycloak","login","single sign-on"]},{"location":"core/docs/keycloak/sso/#prerequisite","title":"Prerequisite","text":"<p>Keycloak is set up for Airman, so organizations, realms, etc. are already created. Correct realm is selected as all the actions are realm specific.</p>","tags":["keycloak","login","single sign-on"]},{"location":"core/docs/keycloak/sso/#add-an-identity-provider","title":"Add an identity provider","text":"<p>Select Identity provider under Configure and add a new provider. There are many identity providers, and this guide will not cover every provider supported by Keycloak in detail.</p> <p>The main things you need are:</p> <ul> <li>Redirect URI from Keycloak   Configure this in the identity provider so that when Keycloak directs authentication to the external identity provider, the provider can then redirect the user back to Keycloak.</li> <li>Client ID from the identity provider   The identity provider offers an ID so it can distinguish between different clients (applications) making requests to the provider.</li> <li>Client secret from the identity provider   A secret generated by the identity provider. Do not confuse this with the UI client secret, for example.</li> </ul> <p>The rest of the settings will vary between different types of identity providers.</p>","tags":["keycloak","login","single sign-on"]},{"location":"core/docs/keycloak/sso/#mapping-rights","title":"Mapping rights","text":"<p>When you control authentication with an identity provider, you can automatically assign minimum rights to every user (i.e., all users allowed to access the system can be assumed to have at least minimum rights). Do this by opening the configured identity provider and going to the Mappers tab. Select Add mapper and use the following settings:</p> <ul> <li>Name: <code>default-user-role</code> (or any other name you prefer)</li> <li>Mapper type: Hardcoded Role</li> <li>Role: Team Member</li> <li>Sync mode override: Inherit</li> </ul> <p>Additional rights can still be granted in Keycloak manually, even if the minimum rights are hard-coded. Also note that without any rights, users can log in but cannot use the system.</p> <p>It's also possible to map roles from the identity provider by configuring a mapper to the identity provider for automatic role assignment, which moves the authorization from Keycloak to the identity provider. Remember to set <code>Sync mode override</code> to <code>Force</code> to ensure that rights are also removed, not just added.</p>","tags":["keycloak","login","single sign-on"]},{"location":"core/docs/keycloak/sso/#link-identity-provider-to-organization","title":"Link identity provider to organization","text":"<p>Select the correct organization from Organizations and go to the Identity providers tab. Select Link identity provider and then select the newly created identity provider from the list. Set:</p> <ul> <li>Identity provider: [Your new identity provider]</li> <li>Domain: [Domain attached to the organization]</li> <li>Hide on login page: Off</li> <li>Redirect when email matches: Off</li> </ul>","tags":["keycloak","login","single sign-on"]},{"location":"core/docs/keycloak/sso/#add-an-authentication-flow","title":"Add an authentication flow","text":"<p>To automatically enroll a user to an organization, you must create a new flow under Authentication. This can be adapted to specific needs, but here is one option to create a simple automatic flow for an IDP. Add execution steps in this order with the specified Requirement type:</p> <ul> <li>Create User If Unique [Required]   Creates a new user in Keycloak if one doesn't already exist.</li> <li>Automatically Set Existing User [Alternative]   Sets user to existing one as an alternative to \"Create User If Unique\".</li> <li>Review Profile [Required]   Updates Keycloak profile data with the data the identity provider offers using attribute mappers.</li> <li>Organization Member Onboard [Required]   Adds the user to the organization that the identity provider is linked to.</li> <li>Allows Access [Required]   Permits the user to log in.</li> </ul> <p>After the flow has been created, navigate to your identity provider and change <code>First Login Flow</code> to that custom authentication flow.</p>","tags":["keycloak","login","single sign-on"]},{"location":"core/docs/using-the-api/mcp-integration/","title":"MCP (Model Context Protocol) Integration","text":"<p>The AIRM API supports the Model Context Protocol (MCP), allowing Large Language Models (LLMs) to interact with the API endpoints as tools. This enables LLMs to perform operations like managing clusters, deploying workloads, handling datasets, and more.</p>"},{"location":"core/docs/using-the-api/mcp-integration/#overview","title":"Overview","text":"<p>MCP is a protocol that allows AI assistants to connect to external systems and perform actions on behalf of users. The AIRM API's MCP integration exposes all 56 API endpoints as tools that can be discovered and used by MCP-compatible clients.</p> <p>Benefits of MCP Integration: - Natural Language Interface: Interact with AIRM using conversational AI - Automated Workflows: Let AI assistants handle complex multi-step operations - Error Handling: AI can interpret error responses and suggest fixes - Discovery: AI can explore available resources and capabilities dynamically</p>"},{"location":"core/docs/using-the-api/mcp-integration/#available-endpoints","title":"Available Endpoints","text":"<p>The MCP integration automatically exposes all AIRM API endpoints. Key endpoints include:</p> <ul> <li>Clusters: Create, list, and manage GPU clusters</li> <li>Projects: Manage AI/ML projects and their configurations</li> <li>Workloads: Deploy and monitor containerized workloads</li> <li>Datasets: Upload, manage, and download training datasets</li> <li>Models: Manage AI models and their deployments</li> <li>Users &amp; Organizations: Handle user management and access control</li> <li>Metrics: Access GPU utilization and performance metrics</li> </ul>"},{"location":"core/docs/using-the-api/mcp-integration/#mcp-endpoints","title":"MCP Endpoints","text":"<p>When the AIRM API is running, the following MCP-specific endpoints are available:</p> <ul> <li><code>GET /mcp</code> - MCP server information and capabilities</li> <li><code>POST /mcp/messages/</code> - MCP message handling for tool execution</li> </ul>"},{"location":"core/docs/using-the-api/mcp-integration/#configuration-examples","title":"Configuration Examples","text":""},{"location":"core/docs/using-the-api/mcp-integration/#claude-desktop-configuration","title":"Claude Desktop Configuration","text":"<p>Add the AIRM API as an MCP server in your Claude Desktop configuration:</p> <pre><code>{\n  \"mcpServers\": {\n    \"airm-api\": {\n      \"command\": \"node\",\n      \"args\": [\"-e\", \"\n        const { spawn } = require('child_process');\n        const proc = spawn('curl', [\n          '-X', 'POST',\n          '-H', 'Content-Type: application/json',\n          '-H', 'Authorization: Bearer YOUR_TOKEN_HERE',\n          '--data-binary', '@-',\n          'https://your-airm-api.example.com/mcp/messages/'\n        ], { stdio: ['pipe', 'inherit', 'inherit'] });\n        process.stdin.pipe(proc.stdin);\n      \"]\n    }\n  }\n}\n</code></pre>"},{"location":"core/docs/using-the-api/mcp-integration/#alternative-configuration-direct-http","title":"Alternative Configuration (Direct HTTP)","text":"<p>For development or testing, you can configure a direct HTTP connection:</p> <pre><code>{\n  \"mcpServers\": {\n    \"airm-local\": {\n      \"command\": \"mcp-client\",\n      \"args\": [\n        \"--transport\", \"http\",\n        \"--url\", \"http://localhost:8001/mcp\",\n        \"--auth-header\", \"Authorization: Bearer YOUR_DEV_TOKEN\"\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"core/docs/using-the-api/mcp-integration/#python-mcp-client-configuration","title":"Python MCP Client Configuration","text":"<p>If using a Python MCP client, you can configure it as follows:</p> <pre><code>from mcp import Client\nimport asyncio\n\nasync def setup_airm_client():\n    client = Client()\n\n    # Configure AIRM API connection\n    await client.connect(\n        transport_type=\"http\",\n        url=\"https://your-airm-api.example.com/mcp\",\n        headers={\n            \"Authorization\": \"Bearer YOUR_API_TOKEN\",\n            \"Content-Type\": \"application/json\"\n        }\n    )\n\n    # List available tools\n    tools = await client.list_tools()\n    print(\"Available AIRM tools:\", tools)\n\n    return client\n\n# Usage example\nasync def main():\n    client = await setup_airm_client()\n\n    # Example: List clusters\n    result = await client.call_tool(\n        \"get_clusters\",\n        arguments={}\n    )\n    print(\"Clusters:\", result)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"core/docs/using-the-api/mcp-integration/#authentication","title":"Authentication","text":"<p>The MCP integration respects the same authentication and authorization mechanisms as the regular API:</p>"},{"location":"core/docs/using-the-api/mcp-integration/#oauth2keycloak-authentication","title":"OAuth2/Keycloak Authentication","text":"<p>For production use, configure OAuth2 authentication:</p> <pre><code># Environment variables for OAuth2\nexport KEYCLOAK_SERVER_URL=\"https://your-keycloak.example.com\"\nexport KEYCLOAK_REALM=\"airm\"\nexport KEYCLOAK_CLIENT_ID=\"your-client-id\"\nexport KEYCLOAK_CLIENT_SECRET=\"your-client-secret\"\n</code></pre>"},{"location":"core/docs/using-the-api/mcp-integration/#development-authentication","title":"Development Authentication","text":"<p>For local development, you can use the default development credentials:</p> <ul> <li>Username: <code>devuser@amd.com</code></li> <li>Password: <code>password</code></li> <li>Client ID: <code>354a0fa1-35ac-4a6d-9c4d-d661129c2cd0</code></li> </ul>"},{"location":"core/docs/using-the-api/mcp-integration/#example-use-cases","title":"Example Use Cases","text":""},{"location":"core/docs/using-the-api/mcp-integration/#1-cluster-management","title":"1. Cluster Management","text":"<p>User Query:</p> <p>\"Can you list all available GPU clusters and their current utilization?\"</p> <p>AI Response Flow: 1. Calls <code>get_clusters</code> to retrieve cluster list 2. Calls <code>get_clusters_stats</code> for utilization metrics 3. Presents formatted summary with recommendations</p> <p>Sample AI Output: <pre><code>I found 3 GPU clusters in your organization:\n\n\ud83d\udda5\ufe0f **ml-production** (Healthy)\n   - 8x NVIDIA A100 GPUs (6 available, 2 in use)\n   - Memory: 640GB total, 480GB available\n   - Current utilization: 25%\n\n\ud83d\udda5\ufe0f **dev-cluster** (Healthy)\n   - 4x NVIDIA V100 GPUs (all available)\n   - Memory: 256GB total, 256GB available\n   - Current utilization: 0%\n\n\ud83d\udda5\ufe0f **training-cluster** (Maintenance)\n   - 16x NVIDIA H100 GPUs (unavailable)\n   - Status: Scheduled maintenance until 2PM\n\n**Recommendation:** For new training jobs, I suggest using 'dev-cluster' for small experiments or 'ml-production' for larger workloads.\n</code></pre></p>"},{"location":"core/docs/using-the-api/mcp-integration/#2-workload-deployment","title":"2. Workload Deployment","text":"<p>User Query:</p> <p>\"Deploy a new PyTorch training workload using the latest PyTorch image on the 'ml-production' cluster with 2 GPUs\"</p> <p>AI Response Flow: 1. Validates cluster exists and has capacity (<code>get_clusters</code>) 2. Checks user's submittable projects (<code>get_submittable_projects</code>) 3. Generates appropriate YAML manifest 4. Submits workload using <code>submit_workload</code> 5. Monitors deployment status</p> <p>AI-Generated Manifest: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pytorch-training-20240127\n  labels:\n    app: pytorch-training\nspec:\n  containers:\n  - name: trainer\n    image: pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel\n    resources:\n      requests:\n        nvidia.com/gpu: 2\n        memory: 16Gi\n        cpu: 4\n      limits:\n        nvidia.com/gpu: 2\n        memory: 32Gi\n        cpu: 8\n    env:\n    - name: CUDA_VISIBLE_DEVICES\n      value: \"0,1\"\n    volumeMounts:\n    - name: shared-storage\n      mountPath: /workspace\n  volumes:\n  - name: shared-storage\n    persistentVolumeClaim:\n      claimName: shared-pvc\n  restartPolicy: Never\n</code></pre></p>"},{"location":"core/docs/using-the-api/mcp-integration/#3-dataset-management-with-workflow","title":"3. Dataset Management with Workflow","text":"<p>User Query:</p> <p>\"Upload my training dataset from local files and create a new dataset called 'image-classification-v2'\"</p> <p>AI Response Flow: 1. Validates project access and cluster health 2. Guides user through file upload process 3. Creates dataset with proper metadata 4. Suggests next steps for model training</p> <p>Interactive Flow: <pre><code>AI: I'll help you upload your dataset. First, let me check your project access...\n\n\u2705 Project access confirmed: 'computer-vision-project'\n\u2705 Cluster 'ml-production' is healthy and ready\n\nPlease provide your JSONL dataset file. The file should contain your training examples in JSON Lines format.\n\nUser: [uploads file: image_data.jsonl]\n\nAI: Perfect! I'm uploading your dataset...\n\n\u2705 Dataset 'image-classification-v2' created successfully\n   - Size: 125.3 MB\n   - Type: training\n   - Location: computer-vision-project/datasets/image-classification-v2.jsonl\n   - Records: 50,000 training examples\n\n**Next Steps:**\n1. Create a model: Use 'create_model' to register your base model\n2. Start training: Submit a training workload with this dataset\n3. Monitor progress: Check workload status and metrics\n\nWould you like me to help with any of these next steps?\n</code></pre></p>"},{"location":"core/docs/using-the-api/mcp-integration/#4-advanced-project-administration","title":"4. Advanced Project Administration","text":"<p>User Query:</p> <p>\"Create a new project called 'computer-vision' with a dedicated GPU quota and add users alice@company.com and bob@company.com as members\"</p> <p>AI Response Flow: 1. Creates project with proper configuration 2. Sets up resource quotas based on requirements 3. Adds users with appropriate roles 4. Configures cluster access and permissions</p> <p>Detailed Workflow: <pre><code>AI: I'll create the 'computer-vision' project with proper setup. Let me handle this step by step:\n\n\ud83d\udccb **Step 1: Creating Project**\n\u2705 Project 'computer-vision' created successfully\n   - ID: proj_cv_20240127_001\n   - Cluster: ml-production\n   - Status: Active\n\n\ud83d\udccb **Step 2: Setting Up GPU Quota**\n\u2705 Dedicated quota allocated:\n   - GPUs: 4x NVIDIA A100 (reserved)\n   - Memory: 320GB\n   - Storage: 1TB\n   - CPU: 32 cores\n\n\ud83d\udccb **Step 3: Adding Team Members**\n\u2705 alice@company.com added as Project Member\n\u2705 bob@company.com added as Project Member\n\n\ud83d\udccb **Step 4: Configuring Access**\n\u2705 Cluster access permissions configured\n\u2705 Dataset storage buckets created\n\u2705 Monitoring and logging enabled\n\n**Project Summary:**\n- Name: computer-vision\n- Members: 3 (including you as admin)\n- Resources: 4 GPUs, 320GB RAM, 1TB storage\n- Ready for workload deployment\n\nThe project is now ready for AI/ML workloads. Team members can submit training jobs, manage datasets, and deploy models.\n\n**Suggested Next Steps:**\n1. Upload initial datasets for the team\n2. Set up shared model repositories\n3. Create development workspaces for team collaboration\n</code></pre></p>"},{"location":"core/docs/using-the-api/mcp-integration/#tool-capabilities","title":"Tool Capabilities","text":"<p>The MCP integration automatically exposes the following types of operations as tools:</p> Operation Type Examples CRUD Operations Create, read, update, delete resources File Operations Upload/download datasets and models Deployment Actions Deploy workloads, start/stop services Monitoring Fetch metrics, check resource utilization User Management Invite users, assign roles, manage permissions"},{"location":"core/docs/using-the-api/mcp-integration/#security-considerations","title":"Security Considerations","text":"<ol> <li>Token Security: Always use secure token storage and rotation</li> <li>Scope Limitation: Configure clients with minimal required permissions</li> <li>Network Security: Use HTTPS in production environments</li> <li>Rate Limiting: Be aware of API rate limits when using automated tools</li> </ol>"},{"location":"core/docs/using-the-api/mcp-integration/#best-practices-for-llm-integration","title":"Best Practices for LLM Integration","text":""},{"location":"core/docs/using-the-api/mcp-integration/#1-provide-context-in-conversations","title":"1. Provide Context in Conversations","text":"<p>When interacting with AI assistants, provide relevant context:</p> <p>Good:</p> <p>\"I need to deploy a PyTorch training job for image classification. The model needs 2 GPUs and will train for about 4 hours. I'm working on the 'computer-vision' project.\"</p> <p>Better:</p> <p>\"Deploy a PyTorch training workload for image classification: - Project: computer-vision - Dataset: image-classification-v2 (already uploaded) - Resources: 2x A100 GPUs, 16GB RAM - Estimated duration: 4 hours - Base image: pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel\"</p>"},{"location":"core/docs/using-the-api/mcp-integration/#2-use-iterative-workflows","title":"2. Use Iterative Workflows","text":"<p>Break complex operations into steps:</p> <pre><code>1. \"First, show me available clusters and their current utilization\"\n2. \"Create a new project called 'nlp-research' on the ml-production cluster\"\n3. \"Set up a GPU quota of 4 A100s for this project\"\n4. \"Add team members: researcher1@company.com, researcher2@company.com\"\n5. \"Upload the training dataset from my local file\"\n</code></pre>"},{"location":"core/docs/using-the-api/mcp-integration/#3-leverage-ai-for-error-resolution","title":"3. Leverage AI for Error Resolution","text":"<p>When errors occur, let the AI help diagnose and fix issues:</p> <pre><code>User: \"My workload submission failed\"\n\nAI: Let me check the error details and help troubleshoot:\n\n1. First, I'll check your project quotas and cluster health\n2. Then examine the workload manifest for issues\n3. Verify resource availability and constraints\n4. Suggest fixes or alternative approaches\n\n[AI performs diagnostic steps and provides specific recommendations]\n</code></pre>"},{"location":"core/docs/using-the-api/mcp-integration/#4-monitor-and-optimize","title":"4. Monitor and Optimize","text":"<p>Use AI for ongoing monitoring and optimization:</p> <pre><code>\"Check the status of all my running workloads and suggest any optimizations\"\n\"Analyze GPU utilization across projects and recommend resource reallocation\"\n\"Identify idle resources and suggest cost-saving measures\"\n</code></pre>"},{"location":"core/docs/using-the-api/mcp-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"core/docs/using-the-api/mcp-integration/#common-issues","title":"Common Issues","text":"<p>Connection Refused <pre><code># Check if AIRM API is running\ncurl http://localhost:8001/mcp\n\n# Expected response: MCP server information\n{\n  \"name\": \"airm-api\",\n  \"version\": \"1.0.0\",\n  \"capabilities\": {\n    \"tools\": true,\n    \"prompts\": false,\n    \"resources\": false\n  }\n}\n</code></pre></p> <p>Authentication Errors <pre><code># Verify token validity\ncurl -H \"Authorization: Bearer YOUR_TOKEN\" http://localhost:8001/v1/health\n\n# Check token permissions\ncurl -H \"Authorization: Bearer YOUR_TOKEN\" http://localhost:8001/v1/projects/submittable\n</code></pre></p> <p>MCP Endpoint Not Found - Ensure you're using AIRM API version with MCP support (check <code>/mcp</code> endpoint) - Verify fastapi-mcp package is installed: <code>pip list | grep fastapi-mcp</code> - Check application logs for MCP initialization errors</p> <p>Tool Discovery Issues <pre><code># Test tool listing via MCP endpoint\ncurl -X POST http://localhost:8001/mcp/messages/ \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer YOUR_TOKEN\" \\\n  -d '{\"method\": \"tools/list\", \"params\": {}}'\n</code></pre></p> <p>Quota and Permission Errors Common issues when AI assistants can't perform operations: - Verify project membership: Use <code>get_submittable_projects</code> to check access - Check cluster quotas: Use <code>get_clusters</code> to verify resource availability - Validate cluster health: Ensure target cluster status is \"healthy\" - Review user roles: Platform admin vs. project member permissions</p>"},{"location":"core/docs/using-the-api/mcp-integration/#debug-mode","title":"Debug Mode","text":"<p>Enable comprehensive debugging for troubleshooting:</p> <p>Python MCP Client: <pre><code>import logging\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\n# Also enable httpx debugging for HTTP requests\nlogging.getLogger(\"httpx\").setLevel(logging.DEBUG)\n</code></pre></p> <p>Claude Desktop Configuration with Debug: <pre><code>{\n  \"mcpServers\": {\n    \"airm-api-debug\": {\n      \"command\": \"node\",\n      \"args\": [\"-e\", \"\n        const { spawn } = require('child_process');\n        const proc = spawn('curl', [\n          '-v',  // Enable verbose output\n          '-X', 'POST',\n          '-H', 'Content-Type: application/json',\n          '-H', 'Authorization: Bearer YOUR_TOKEN_HERE',\n          '--data-binary', '@-',\n          'https://your-airm-api.example.com/mcp/messages/'\n        ], { stdio: ['pipe', 'inherit', 'inherit'] });\n        process.stdin.pipe(proc.stdin);\n      \"],\n      \"env\": {\n        \"DEBUG\": \"1\"\n      }\n    }\n  }\n}\n</code></pre></p>"},{"location":"core/docs/using-the-api/mcp-integration/#performance-optimization","title":"Performance Optimization","text":"<p>For Large-Scale Operations: - Use batch operations when available (e.g., bulk user management) - Cache cluster and project information to reduce API calls - Implement proper error handling and retries for transient failures</p> <p>For Real-Time Monitoring: - Use streaming endpoints where available - Implement proper polling intervals for status checks - Cache frequently accessed metadata (cluster specs, project lists)</p>"},{"location":"core/docs/using-the-api/mcp-integration/#api-documentation","title":"API Documentation","text":"<p>For complete API endpoint documentation, visit: - Swagger UI: <code>http://localhost:8001/docs</code> - ReDoc: <code>http://localhost:8001/redoc</code></p>"},{"location":"core/docs/using-the-api/mcp-integration/#support","title":"Support","text":"<p>For MCP integration support: 1. Check the AIRM API logs for error messages 2. Verify your authentication credentials 3. Ensure network connectivity to the API endpoint 4. Consult the MCP client documentation for your specific client implementation</p>"},{"location":"platform-infrastructure/digitalocean-installation/","title":"Install SiloGen on DigitalOcean cloud","text":"<p>This article explains how to install SiloGen in DigitalOcean cloud environment, covering the full stack from metal to application layer in a streamlined manner.</p>"},{"location":"platform-infrastructure/digitalocean-installation/#prerequisites","title":"Prerequisites","text":"<p>Suggested minimum configuration for DigitalOcean droplet:</p> <p><pre><code>AMD MI300X\n1 GPU - 192 GB VRAM - 20 vCPU - 240 GB RAM\nBoot disk: 720 GB NVMe- Scratch disk: 5 TB NVMe\n</code></pre> Quick start: <pre><code>ROCm\u2122 Software\nROCm 6.4.0.\nInstall any AI tools you need.\n</code></pre></p>"},{"location":"platform-infrastructure/digitalocean-installation/#installation-steps","title":"Installation steps","text":"<p>In order to install on a DigitalOcean droplet, copy the following text into a file bloom.yaml, replacing  with the ip address of the node. <pre><code>DOMAIN: &lt;your-ip-address&gt;.nip.io\nCERT_OPTION: generate\nCLUSTERFORGE_RELEASE: https://github.com/silogen/cluster-forge/releases/download/20250812-1-enterprise/release-enterprise-20250812-1.tar.gz\nFIRST_NODE: true\nGPU_NODE: true\nSKIP_DISK_CHECK: false\nUSE_CERT_MANAGER: false\nSELECTED_DISKS: /dev/vdc1\n</code></pre> <p>Download the installation tool (\"bloom\")</p> <p><pre><code>wget https://github.com/silogen/cluster-bloom/releases/latest/download/bloom\n</code></pre> Run bloom with the following command <pre><code>chmod +x bloom\n\nsudo ./bloom --config bloom.yaml\n</code></pre></p>"},{"location":"platform-infrastructure/on-premises-installation/","title":"Install SiloGen on-premises","text":"<p>This article explains how to install SiloGen in an on-premises environment, covering the full stack from metal to application layer in a streamlined manner.</p> <p>The SiloGen platform runs on top of Kubernetes orchestration platform and includes essential Kubernetes components for monitoring, secrets management, and certificate management.</p> <p>The installation process leverages helper tools called Cluster Bloom and Cluster Forge that deploy and configure all necessary platform components, preparing a Kubernetes cluster for executing AI workloads.</p>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-infrastructure/on-premises-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ubuntu (supported versions checked at runtime)</li> <li>Sufficient disk space (500GB+ recommended for root partition, 2TB+ for workloads)</li> <li>NVMe drives for optimal storage configuration</li> <li>ROCm-compatible AMD GPUs (for GPU nodes)</li> <li>Root/sudo access</li> </ul>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-infrastructure/on-premises-installation/#domain-and-ssl-certificate-prerequisites","title":"Domain and SSL certificate prerequisites","text":"<ul> <li>Before installing the Kubernetes services, you'll need a domain name (such as myapp.example.com) that points to your server's IP address.</li> <li>For production environments, this domain should point to a load balancer that distributes traffic across multiple servers.</li> <li>For smaller setups or demonstrations, the domain can point directly to a single server's IP address, and MetalLB will be configured to handle load balancing within the Kubernetes cluster.</li> <li>If you don't have a DNS-enabled domain available, you may use a .nip.io domain with your IP address. Example: <code>&lt;master-node-ip-address&gt;.nip.io</code>.</li> <li>Additionally, you'll need an SSL certificate to enable secure HTTPS connections to your services.</li> <li>You can either provide your own trusted SSL certificate purchased from a certificate authority, or use the free Let's Encrypt service to automatically generate one.</li> <li>If using Let's Encrypt, your setup must meet one of these requirements: either have port 80 accessible from the internet (allowing Let's Encrypt to verify domain ownership through your website), or have DNS management capabilities that allow automated domain validation (where Let's Encrypt can verify ownership by temporarily adding DNS records to your domain).</li> <li>Finally, ensure that any firewalls, security groups, or network routing configurations are updated to allow incoming connections from users who will be accessing these services.</li> </ul>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-infrastructure/on-premises-installation/#install-kubernetes-cluster-and-software","title":"Install Kubernetes cluster and software","text":"<p>You will use an installation script called Cluster Bloom to install and configure a Kubernetes cluster and install the SiloGen software. It installs the following features to prepare an AMD GPU node to be part of a Kubernetes cluster:</p> <ul> <li>Automated RKE2 Kubernetes cluster deployment</li> <li>ROCm setup and configuration for AMD GPU nodes</li> <li>Disk management and Longhorn storage integration</li> <li>Multi-node cluster support with easy node joining</li> <li>1Password integration for secrets management</li> <li>Install the SiloGen software using Cluster Forge tool</li> </ul>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-infrastructure/on-premises-installation/#1-ssh-to-node-as-root-user","title":"1. SSH to node as root user","text":"<p>Access the node using SSH as root user.</p>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-infrastructure/on-premises-installation/#2-download-the-latest-installation-script","title":"2. Download the latest installation script","text":"<p>Go to the working folder where you want to install the release.</p> <p>Download the latest installation script (adjust the URL to the release of your preference): <pre><code>wget https://github.com/silogen/cluster-bloom/releases/latest/download/bloom\n</code></pre></p>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-infrastructure/on-premises-installation/#3-make-file-executable","title":"3. Make file executable","text":"<pre><code>chmod +x bloom\n</code></pre>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-infrastructure/on-premises-installation/#4-create-the-installation-configuration","title":"4. Create the installation configuration","text":"<p>Before you can start the installation you need to create the installation configuration, which adapts the installation to your environment. You can use a Configuration wizard that facilitates the creation of the configuration (Option A) or add the values directly in the configuration file (Option B).</p>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-infrastructure/on-premises-installation/#option-a-create-installation-configuration-using-the-installation-wizard","title":"Option A - Create installation configuration using the Installation wizard","text":"<p>The Installation wizard is a helper tool that guides the user in creating the optimal configuration for the installation. For the standard installation you should select the default values, only exception is the <code>domain</code> and <code>cert option</code> where you should provide a specific value.</p> <p>To start the wizard: <pre><code>sudo ./bloom\n</code></pre></p> <p>The wizard includes the following steps:</p> <p>First node</p> <p>Specifies if this is the first node in the cluster. Set to <code>false</code> for additional nodes joining an existing cluster.</p> <p>GPU node</p> <p>Specifies whether the node has GPUs. Set to <code>false</code> for CPU-only nodes. When <code>true</code>, ROCm will be installed and configured.</p> <p>OIDC URL</p> <p>URL of the OIDC provider for authentication. Leave empty to skip OIDC configuration.</p> <p>Skip disk check</p> <p>Specifies if disk check should be performed. Set to <code>true</code> if you don't want automatic disk setup.</p> <p>Selected disks</p> <p>List of disk devices to use. Example: <code>dev/sdb</code>. Leave empty for automatic selection.</p> <p>Longhorn disks</p> <p>List of disk paths for Longhorn storage. Leave empty for automatic configuration.</p> <p>Cluster-Forge release</p> <p>The ClusterForge release <code>URL</code> or <code>none</code> to skip the SW installation.</p> <p>Domain</p> <p>Domain name for the cluster, e.g., <code>cluster.example.com</code>. The domain name is used for ingress configuration. If you don't have a DNS-enabled domain available, you may use a .nip.io domain with your IP address. Example: <code>&lt;master-node-ip-address&gt;.nip.io</code>.</p> <p>Use cert manager</p> <p>Set to <code>Yes</code> to use cert-manager with Let's encrypt for automatic TLS certificates. Set to <code>false</code> to provide your own certificates.</p> <p>Cert option</p> <p>Certificate option when <code>Use cert manager</code> is false. Choose <code>existing</code> to use existing certificate files, or <code>generate</code> to create a self-signed certificate.</p> <p>Configuration complete!</p> <p>Once the wizard has completed you can find the configuration file in <code>bloom.yaml</code>.</p> <p>Start the installation</p> <p>To run the actual installation select <code>y</code> in the following step <code>Would you like to run bloom with this configuration no? (y/n)</code></p>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-infrastructure/on-premises-installation/#option-b-specify-the-values-in-configuration-file","title":"Option B - Specify the values in configuration file","text":"<p>You can also specify the values in the configuration file directly and skip the installation wizard process.</p> <p>Below is an example configuration for the configuration file bloom.yaml: <pre><code>DOMAIN: &lt;your-ip-address&gt;.nip.io\nCERT_OPTION: generate\nCLUSTERFORGE_RELEASE: https://github.com/silogen/cluster-forge/releases/download/20250812-1-enterprise/release-enterprise-20250812-1.tar.gz\nFIRST_NODE: true\nGPU_NODE: true\nSKIP_DISK_CHECK: false\nUSE_CERT_MANAGER: false\nSELECTED_DISKS: /dev/vdc1\n</code></pre> To start the installation: <pre><code>sudo ./bloom --config bloom.yaml\n</code></pre></p>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-infrastructure/on-premises-installation/#5-complete-the-installation-progress","title":"5. Complete the installation progress","text":"<p>The installation will take roughly 15 minutes. You can now follow the installation progress through the user interface:</p> <p></p> <p>For systems with unmounted physical disks, a selection prompt will appear:</p> <p></p>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-infrastructure/on-premises-installation/#51-optional-step-adding-a-second-node-to-cluster","title":"5.1 Optional step: Adding a second node to cluster","text":"<p>After successful installation, Cluster Bloom generates <code>additional_node_command.txt</code>, which contains the command for installing additional nodes into the cluster.</p>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-infrastructure/on-premises-installation/#6-specify-huggingface-token","title":"6. Specify HuggingFace token","text":"<p>In order to download and access gated models from Hugging Face, you need to provide a Hugging Face token. Tokens contain sensitive information. To keep them secure and prevent unauthorized access, they should not be stored in plain text in your code or configuration files. Instead, they are stored as secrets, a secure way to manage sensitive data.</p>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-infrastructure/on-premises-installation/#how-to-get-a-hugging-face-token","title":"How to get a Hugging Face token","text":"<ol> <li> <p>Create or log in to your Hugging Face account:     https://huggingface.co.</p> </li> <li> <p>Navigate to your account settings.</p> </li> <li> <p>Under Access Tokens, generate a new token.</p> </li> <li> <p>Copy the token (keep it safe; don\\'t share it).</p> </li> </ol>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-infrastructure/on-premises-installation/#where-to-install-the-token","title":"Where to install the token","text":"<p>The token needs to be installed as a secret in your environment. Here are examples of how to do this in common platforms:</p> <p>On Kubernetes, save the token in an environment variable in your terminal:</p> <pre><code>kubectl create secret generic hf-token \\\n    --from-literal=hf-token=my_super_secret_token \\\n    -n my_namespace\n</code></pre>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-infrastructure/on-premises-installation/#7-login-to-silogen","title":"7. Login to SiloGen","text":"<p>To confirm that the installation was successful, ensure you are able to log in to the Developer Center.</p> <ol> <li>Access the SiloGen URL (your domain name).</li> <li>For nip.io domain: <code>https://airmui.&lt;master-node-ip-address&gt;.nip.io</code></li> <li>If using a registered domain, the web address of the service will be: <code>https://airmui.&lt;your-domain&gt;</code></li> <li>Login as <code>devuser@domain</code> user and use the default password.</li> </ol> <p>See more details about login here.</p>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-infrastructure/on-premises-installation/#install-only-software-into-an-existing-kubernetes-cluster","title":"Install only software into an existing Kubernetes cluster","text":"<p>To install SiloGen platform in an existing Kubernetes cluster, download a Cluster Forge release package and run <code>deploy.sh</code>. This assumes there is a working Kubernetes cluster to deploy into, and the current Kubeconfig context refers to that cluster.</p> <p>For the Cluster Forge <code>deploy</code> release:</p> <pre><code>wget https://github.com/silogen/cluster-forge/releases/download/deploy/deploy-release.tar.gz\ntar -xzvf deploy-release.tar.gz\nsudo bash clusterforge/deploy.sh\n</code></pre>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-infrastructure/on-premises-installation/#appendix","title":"Appendix","text":"<ul> <li>Cluster Forge: https://github.com/silogen/cluster-forge</li> <li>Cluster Bloom: https://github.com/silogen/cluster-bloom</li> </ul>","tags":["platform","installation","demo environment","kubernetes"]},{"location":"platform-infrastructure/supported-environments/","title":"Supported environments","text":"<p>SiloGen can be installed on the following environments:</p> <ul> <li> <p>On-premises environment: This includes setting up a one-node demo cluster with built-in storage and secrets management. The main use case is to quickly get started running AI workloads on the SiloGen platform. Access the installation article here</p> </li> <li> <p>DigitalOcean cloud: This article explains how to install SiloGen on DigitalOcean cloud. Access the installation article here</p> </li> <li> <p>Azure cloud with AKS: See instructions on how to set up a virtual machine with Instinct MI300X and the SiloGen platform on Azure cloud here</p> </li> </ul>","tags":["platform","installation","demo environment"]},{"location":"quick-start-guides/airman-quick-start/","title":"Quick start guide for AI resource managers","text":"<p>The SiloGen platform provides a robust environment for managing AI development teams' work on AMD compute. Whether teams are working on fine-tuning, inference, or any other aspect of AI development, the platform offers easy-to-use tools to maintain fair compute resource allocation. To work efficiently in the SiloGen platform, a resource manager should have some experience with AI terminology and a general understanding of AI development infrastructure.</p> <p>This quick start guide outlines the steps to set up essential AI compute resources for project work. By the end, you will have a connected compute cluster, a project with allocated compute resources, and an invited team member who can access the platform.</p> <p>Terminology reference</p> Word Explanation GPU A graphics processing unit. An essential part of compute clusters. Node A single data center computer that can contain multiple GPUs. Cluster A set of interconnected computational nodes. Project A container for AI development. The project is allocated resources through a quota. Quota Quotas define a minimum set of resources that are guaranteed. Quotas ensure that computational resources are appropriately shared between projects. Without quotas, some workloads might use all the resources, leaving other workloads waiting. SSO Single sign-on. A user login feature allowing easy traversal between multiple applications.","tags":["quick start","resource management","airman"]},{"location":"quick-start-guides/airman-quick-start/#prerequisites","title":"Prerequisites","text":"<p>This guide assumes you have access to an installed SiloGen platform. To assist you with onboarding, the following configurations have already been created during the installation process:</p> <ul> <li>Platform admin users have already been created for your organization as part of the installation process. These are named devuser@domain and silogen-admin. The platform admin user can manage the SiloGen platform and onboard more users.</li> <li>An organization entity has been created.</li> <li>A compute cluster has been onboarded and connected to SiloGen platform.</li> <li>A project with a resource quota has been created.</li> </ul>","tags":["quick start","resource management","airman"]},{"location":"quick-start-guides/airman-quick-start/#getting-started","title":"Getting started","text":"<p>This section outlines the steps to set up essential AI compute resources for project work.</p>","tags":["quick start","resource management","airman"]},{"location":"quick-start-guides/airman-quick-start/#onboard-users","title":"Onboard users","text":"<p>Note</p> <p>SiloGen provides multiple options for onboarding users into the platform. For small-scale trial deployments you can invite users through email or add them manually, but for large scale deployments we recommend using single sign-on (SSO).</p>","tags":["quick start","resource management","airman"]},{"location":"quick-start-guides/airman-quick-start/#invite-users-through-email","title":"Invite users through email","text":"<p>Follow the instructions in \"Inviting users and configuring SMTP in Keycloak\" to invite users through email.</p>","tags":["quick start","resource management","airman"]},{"location":"quick-start-guides/airman-quick-start/#onboard-users-through-single-sign-on-sso","title":"Onboard users through single sign-on (SSO)","text":"<p>Follow the instructions in \"Adding an identity provider to enable single sign-on\" to onboard users through SSO.</p>","tags":["quick start","resource management","airman"]},{"location":"quick-start-guides/airman-quick-start/#invite-users-manually","title":"Invite users manually","text":"<p>In case your organization does not have SSO or SMTP server set up you can add users manually.</p> <p>Follow the instructions in \"Manually managing and activating users in Airman\" to add users manually.</p>","tags":["quick start","resource management","airman"]},{"location":"quick-start-guides/airman-quick-start/#change-the-default-admin-user-passwords","title":"Change the default admin user passwords","text":"<p>It is recommended to change the initial admin users' passwords that were created as part of the installation process. - Super admin password (Keycloak admin user) - The super admin password is needed to login to the Keycloak admin panel. The password is stored in a Kubernetes secret, so in order to change the password you need to change the Kubernetes secret. - Platform admin password (devuser@domain) - How to change your <code>devuser</code> password depends on your user management setup. E.g., the <code>super admin</code> user can reset the user in Keycloak admin panel, if you have <code>Invite users through email</code> you can change your password yourself through \"Forget password\" link.</p>","tags":["quick start","resource management","airman"]},{"location":"quick-start-guides/airman-quick-start/#optional-create-new-projects-for-advanced-users","title":"Optional: Create new projects (for advanced users)","text":"<p>Note</p> <p>A default project with quota has already been created for your organization as part of the installation process. In case you want to create new projects follow the instructions below.</p> <p>To learn how to create a new project with guaranteed quota follow the instructions in \"Manage projects\"</p>","tags":["quick start","resource management","airman"]},{"location":"quick-start-guides/devcenter-quick-start/","title":"Quick start guide for AI practitioners","text":"<p>The SiloGen platform provides a stable starting point for AI model development, fine-tuning, and inference with the help of an easy-to-use model catalog and AI developer workspaces. To work efficiently in the SiloGen platform, an AI developer should have some experience with AI terminology and a general understanding of typical AI development workflows.</p>","tags":["quick start","ai development","developer center"]},{"location":"quick-start-guides/devcenter-quick-start/#prerequisites","title":"Prerequisites","text":"<p>This guide assumes the user has access to an installed SiloGen platform. Besides that, you should have access to compute cluster and a project with allocated compute resources.</p> <p>Your system needs a Hugging Face token to download models.</p>","tags":["quick start","ai development","developer center"]},{"location":"quick-start-guides/devcenter-quick-start/#getting-started","title":"Getting started","text":"<ol> <li>Activate your SiloGen account and login to SiloGen Developer Center.</li> <li>Complete the model fine-tuning tutorial to get hands-on with the Developer Center.</li> <li>Browse through the Developer Center end user documentation in order to familiarize yourself with the SiloGen Developer Center features.</li> </ol>","tags":["quick start","ai development","developer center"]},{"location":"quick-start-guides/devcenter-quick-start/#activate-your-silogen-account","title":"Activate your SiloGen account","text":"<ol> <li>You should have received a registration email with a link to activate your account. Click on the link in this email to start the registration process.</li> <li>In case you have not received an email, please contact your SiloGen platform admin.</li> <li>Login to SiloGen Developer Center through your organization's SiloGen URL (ask for the URL from your SiloGen platform admin).</li> <li>Access the Developer Center tab on the left panel once you have logged in to SiloGen.</li> </ol>","tags":["quick start","ai development","developer center"]},{"location":"quick-start-guides/devcenter-quick-start/#complete-a-tutorial","title":"Complete a tutorial","text":"<p>Based on your background you can choose either no-code or more advanced coding tutorials.</p>","tags":["quick start","ai development","developer center"]},{"location":"quick-start-guides/devcenter-quick-start/#model-deployment-and-inference","title":"Model deployment and inference","text":"<p>To learn how to deploy a model and run inference, please complete the tutorial \"How to deploy a model and run inference\".</p>","tags":["quick start","ai development","developer center"]},{"location":"quick-start-guides/devcenter-quick-start/#no-code-tutorial-fine-tune-a-model-with-no-code","title":"No-code tutorial: fine-tune a model with no code","text":"<p>To learn how to finetune a model without any code, please complete the tutorial \"Fine-tune a model with no code\".</p>","tags":["quick start","ai development","developer center"]},{"location":"quick-start-guides/devcenter-quick-start/#advanced-tutorial-hands-on-ai-development-using-ai-workspaces","title":"Advanced tutorial: hands-on AI development using AI workspaces","text":"<p>To learn how to develop AI models using workspaces, please complete the tutorial \"AI workspace tutorial\".</p>","tags":["quick start","ai development","developer center"]},{"location":"quick-start-guides/devcenter-quick-start/#further-discovery","title":"Further discovery","text":"<p>Browse through the SiloGen end-user documentation to familiarize yourself with the SiloGen Developer Center features.</p>","tags":["quick start","ai development","developer center"]},{"location":"references/overview/","title":"References","text":"<ul> <li>AI workloads: https://github.com/silogen/ai-workloads</li> <li>Cluster Bloom: https://github.com/silogen/cluster-bloom</li> <li>Cluster Forge: https://github.com/silogen/cluster-forge</li> <li>Efficient GPU utilization &amp; orchestration using Kaiwo.</li> </ul>"},{"location":"references/tags/","title":"Tags","text":"<p>Here are the tags used in the platform documentation.</p>"},{"location":"references/tags/#tag:add-cluster","title":"add cluster","text":"<ul> <li>            Add new cluster          </li> </ul>"},{"location":"references/tags/#tag:ai-development","title":"ai development","text":"<ul> <li>            Quick start guide          </li> </ul>"},{"location":"references/tags/#tag:airman","title":"airman","text":"<ul> <li>            Add new cluster          </li> <li>            Airman overview          </li> <li>            Clusters overview          </li> <li>            Dashboard          </li> <li>            Manage users          </li> <li>            Project dashboard          </li> <li>            Project quotas          </li> <li>            Projects overview          </li> <li>            Quick start guide          </li> <li>            Resource utilization          </li> <li>            Users overview          </li> </ul>"},{"location":"references/tags/#tag:chat","title":"chat","text":"<ul> <li>            Chat with models          </li> <li>            Compare models          </li> <li>            Inference overview          </li> </ul>"},{"location":"references/tags/#tag:cluster","title":"cluster","text":"<ul> <li>            Add new cluster          </li> <li>            Clusters overview          </li> </ul>"},{"location":"references/tags/#tag:color-theme","title":"color theme","text":"<ul> <li>            Personal preferences          </li> </ul>"},{"location":"references/tags/#tag:comparison","title":"comparison","text":"<ul> <li>            Compare models          </li> </ul>"},{"location":"references/tags/#tag:dashboard","title":"dashboard","text":"<ul> <li>            Dashboard          </li> </ul>"},{"location":"references/tags/#tag:datasets","title":"datasets","text":"<ul> <li>            Datasets          </li> </ul>"},{"location":"references/tags/#tag:demo-environment","title":"demo environment","text":"<ul> <li>            On-premises installation          </li> <li>            Supported environments          </li> </ul>"},{"location":"references/tags/#tag:deploy","title":"deploy","text":"<ul> <li>            Deploy a model and run inference          </li> </ul>"},{"location":"references/tags/#tag:deployments","title":"deployments","text":"<ul> <li>            Models          </li> </ul>"},{"location":"references/tags/#tag:developer-center","title":"developer center","text":"<ul> <li>            AI workspace tutorial          </li> <li>            CLI tutorials overview          </li> <li>            Chat with models          </li> <li>            Compare models          </li> <li>            Datasets          </li> <li>            Deploy a model and run inference          </li> <li>            Developer Center overview          </li> <li>            Fine-tuning          </li> <li>            Inference overview          </li> <li>            Models          </li> <li>            No-code tutorial          </li> <li>            Quick start guide          </li> <li>            Training and fine-tuning in the SiloGen platform          </li> <li>            Workloads          </li> <li>            Workspaces          </li> </ul>"},{"location":"references/tags/#tag:features","title":"features","text":"<ul> <li>            Airman overview          </li> <li>            Platform overview          </li> </ul>"},{"location":"references/tags/#tag:fine-tuning","title":"fine-tuning","text":"<ul> <li>            Datasets          </li> <li>            Fine-tuning          </li> <li>            No-code tutorial          </li> <li>            Training and fine-tuning in the SiloGen platform          </li> </ul>"},{"location":"references/tags/#tag:inference","title":"inference","text":"<ul> <li>            Deploy a model and run inference          </li> <li>            Inference overview          </li> </ul>"},{"location":"references/tags/#tag:installation","title":"installation","text":"<ul> <li>            On-premises installation          </li> <li>            Quick start guide          </li> <li>            Supported environments          </li> </ul>"},{"location":"references/tags/#tag:introduction","title":"introduction","text":"<ul> <li>            Platform overview          </li> <li>            Quick start guide          </li> <li>            Target audience          </li> </ul>"},{"location":"references/tags/#tag:invitation","title":"invitation","text":"<ul> <li>            Configure SMTP and invite users          </li> </ul>"},{"location":"references/tags/#tag:invite-users","title":"invite users","text":"<ul> <li>            Manage users          </li> </ul>"},{"location":"references/tags/#tag:keycloak","title":"keycloak","text":"<ul> <li>            Add users manually          </li> <li>            Configure SMTP and invite users          </li> <li>            Enable single sign-on          </li> <li>            Login to SiloGen          </li> <li>            Users overview          </li> </ul>"},{"location":"references/tags/#tag:kubernetes","title":"kubernetes","text":"<ul> <li>            On-premises installation          </li> </ul>"},{"location":"references/tags/#tag:login","title":"login","text":"<ul> <li>            Add users manually          </li> <li>            Configure SMTP and invite users          </li> <li>            Enable single sign-on          </li> <li>            Login to SiloGen          </li> </ul>"},{"location":"references/tags/#tag:manual","title":"manual","text":"<ul> <li>            Add users manually          </li> </ul>"},{"location":"references/tags/#tag:model-catalog","title":"model catalog","text":"<ul> <li>            Developer Center overview          </li> <li>            Models          </li> </ul>"},{"location":"references/tags/#tag:model-training","title":"model training","text":"<ul> <li>            CLI tutorials overview          </li> </ul>"},{"location":"references/tags/#tag:models","title":"models","text":"<ul> <li>            Deploy a model and run inference          </li> </ul>"},{"location":"references/tags/#tag:overview","title":"overview","text":"<ul> <li>            Airman overview          </li> </ul>"},{"location":"references/tags/#tag:platform","title":"platform","text":"<ul> <li>            On-premises installation          </li> <li>            Platform overview          </li> <li>            Supported environments          </li> </ul>"},{"location":"references/tags/#tag:preferences","title":"preferences","text":"<ul> <li>            Personal preferences          </li> </ul>"},{"location":"references/tags/#tag:project","title":"project","text":"<ul> <li>            Project quotas          </li> </ul>"},{"location":"references/tags/#tag:projects","title":"projects","text":"<ul> <li>            Project dashboard          </li> <li>            Projects overview          </li> </ul>"},{"location":"references/tags/#tag:quick-start","title":"quick start","text":"<ul> <li>            Quick start guide          </li> <li>            Quick start guide          </li> <li>            Quick start guide          </li> </ul>"},{"location":"references/tags/#tag:quota","title":"quota","text":"<ul> <li>            Project quotas          </li> </ul>"},{"location":"references/tags/#tag:rag","title":"rag","text":"<ul> <li>            Chat with models          </li> </ul>"},{"location":"references/tags/#tag:resource-management","title":"resource management","text":"<ul> <li>            Dashboard          </li> <li>            Quick start guide          </li> <li>            Resource utilization          </li> </ul>"},{"location":"references/tags/#tag:roles","title":"roles","text":"<ul> <li>            Users overview          </li> </ul>"},{"location":"references/tags/#tag:single-sign-on","title":"single sign-on","text":"<ul> <li>            Enable single sign-on          </li> </ul>"},{"location":"references/tags/#tag:smtp","title":"smtp","text":"<ul> <li>            Configure SMTP and invite users          </li> </ul>"},{"location":"references/tags/#tag:target-audience","title":"target audience","text":"<ul> <li>            Target audience          </li> </ul>"},{"location":"references/tags/#tag:training","title":"training","text":"<ul> <li>            Training and fine-tuning in the SiloGen platform          </li> </ul>"},{"location":"references/tags/#tag:tutorial","title":"tutorial","text":"<ul> <li>            AI workspace tutorial          </li> <li>            CLI tutorials overview          </li> <li>            Deploy a model and run inference          </li> <li>            No-code tutorial          </li> <li>            Resource utilization          </li> </ul>"},{"location":"references/tags/#tag:user-management","title":"user management","text":"<ul> <li>            Manage users          </li> <li>            Users overview          </li> </ul>"},{"location":"references/tags/#tag:workloads","title":"workloads","text":"<ul> <li>            Developer Center overview          </li> <li>            Workloads          </li> <li>            Workspaces          </li> </ul>"},{"location":"references/tags/#tag:workspaces","title":"workspaces","text":"<ul> <li>            AI workspace tutorial          </li> <li>            Workloads          </li> <li>            Workspaces          </li> </ul>"},{"location":"tutorials/ai-workspace-tutorial/","title":"Working in AI workspaces hands-on","text":"<p>This guide teaches you how to work in the SiloGen AI workspace using a Jupyter Lab notebook.</p>","tags":["developer center","workspaces","tutorial"]},{"location":"tutorials/ai-workspace-tutorial/#tutorial-fine-tune-llama-31-8b-with-torchtune","title":"Tutorial: fine-tune Llama-3.1 8B with torchtune","text":"<p>This tutorial demonstrates how to fine-tune the Llama-3.1 8B large language model (LLM) on AMD ROCm GPUs using torchtune. Torchtune is an easy-to-use PyTorch library for authoring, post-training, and experimenting with LLMs.</p> <p>Access the tutorial here.</p> <p>Note</p> <p>Skip steps 1-3 in the chapter <code>Prepare the training environment</code>, as these do not apply to the SiloGen (Kubernetes) environment.</p>","tags":["developer center","workspaces","tutorial"]},{"location":"tutorials/ai-workspace-tutorial/#tutorial-prepare-and-upload-a-dataset-to-silogen-developer-center","title":"Tutorial: Prepare and upload a dataset to SiloGen Developer Center","text":"","tags":["developer center","workspaces","tutorial"]},{"location":"tutorials/ai-workspace-tutorial/#1-run-the-data-preparation-script-in-the-jupyter-notebook","title":"1. Run the data preparation script in the Jupyter Notebook","text":"<pre><code>from huggingface_hub import hf_hub_download\nimport json\nimport os\nimport random\n\ndef convert_opi(input_file, output_file):\n    \"\"\"\n    Converts a JSON array of objects with 'instruction', 'input', and 'output' fields\n    into a JSONL file with the specified message format.\n    \"\"\"\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    with open(output_file, 'w') as f:\n        for row in data:\n            if all(k in row for k in (\"instruction\", \"input\", \"output\")):\n                line = {\n                    \"messages\": [\n                        {\"role\": \"user\", \"content\": f\"{row['instruction']} Sequence: {row['input']}\"},\n                        {\"role\": \"assistant\", \"content\": row[\"output\"]}\n                    ]\n                }\n                f.write(json.dumps(line) + \"\\n\")\n\ndef create_sample(input_jsonl, output_jsonl, n):\n    \"\"\"Create a random sample of n lines from input_jsonl and write to output_jsonl.\"\"\"\n    with open(input_jsonl, 'r') as f:\n        lines = f.readlines()\n    sample = random.sample(lines, min(n, len(lines)))\n    with open(output_jsonl, 'w') as f:\n        f.writelines(sample)\n\nrepo_id = \"BAAI/OPI\"\ntarget_dir = \"./datasets\"\noutput_dir = \"./datasets\"\ndata_in = [\n    \"OPI_DATA/OPI_updated_160k.json\",\n]\ncreate_sample_n = 1000  # Set to None to disable\n\nfor file in data_in:\n    hf_hub_download(repo_id=repo_id,\n                    filename=file,\n                    repo_type=\"dataset\",\n                    local_dir=target_dir)\n    print('Downloaded', file)\n    file_out = file.split('/')[1].replace(\".json\", \".jsonl\")\n    out_path = os.path.join(output_dir, file_out)\n    convert_opi(os.path.join(target_dir, file), out_path)\n    print('Converted', file, 'to', file_out)\n\n    if create_sample_n is not None:\n        sample_out = out_path.replace(\".jsonl\", f\".sample{create_sample_n}.jsonl\")\n        create_sample(out_path, sample_out, create_sample_n)\n        print(f'Created random sample of {create_sample_n} lines: {sample_out}')\n</code></pre>","tags":["developer center","workspaces","tutorial"]},{"location":"tutorials/ai-workspace-tutorial/#2-upload-the-dataset-to-the-developer-center-catalog","title":"2. Upload the dataset to the Developer Center catalog","text":"<p>You can upload a dataset to SiloGen using the SiloGen API. See the examples below.</p> <p>Example: API call using Curl</p> <pre><code>curl -X 'POST' \\\n  'https://api-demo.silogen.ai/v1/datasets/upload?project_id=YOUR_PROJECT_UUID_HERE' \\\n  -H 'accept: application/json' \\\n  -H 'Authorization: Bearer YOUR_TOKEN_HERE' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'name=string' \\\n  -F 'description=string' \\\n  -F 'type=Fine-tuning' \\\n  -F 'jsonl=@dataset.jsonl'\n</code></pre> <p>Example: API call using Python</p> <pre><code>from pathlib import Path\nimport requests, certifi\n\nBASE_URL = \"https://api-demo.silogen.ai/v1/datasets/upload?project_id=ADD_YOUR_PROJECT_ID\"\nfile_path = Path(\"path_to_your_dataset\")\nheaders = {\"accept\": \"application/json\", \"Authorization\": \"Bearer ADD_YOUR_TOKEN\"}\ndata = {\"name\": \"dataset_name\", \"description\": \"dataset_decription\", \"type\": \"Fine-tuning\"}\n\nwith file_path.open(\"rb\") as f:\n    response = requests.post(\n        url=BASE_URL,\n        headers=headers,\n        data=data,\n        files={\"jsonl\": f},\n        verify=certifi.where(),\n        timeout=300,\n    )\nprint(response.json())\n</code></pre>","tags":["developer center","workspaces","tutorial"]},{"location":"tutorials/no-code-fine-tuning-tutorial/","title":"Fine-tune a model with no code","text":"<p>This guide teaches you how to fine-tune a model through the Developer Center user interface without writing any code. Follow these steps to complete a fine-tuning job:</p> <ol> <li>Select a base model and a dataset for fine-tuning.</li> <li>Start the model fine-tuning workload and monitor its status.</li> <li>Deploy the fine-tuned model for inference.</li> <li>Chat with the fine-tuned model using the chat interface.</li> </ol>","tags":["developer center","fine-tuning","tutorial"]},{"location":"tutorials/no-code-fine-tuning-tutorial/#1-select-a-base-model-and-a-dataset-for-fine-tuning","title":"1. Select a base model and a dataset for fine-tuning","text":"<p>Go to the <code>Models</code> page and select the <code>Custom models</code> tab. Make sure you have a model available for fine-tuning; you can check model capabilities in the \"Capabilities\" column. If there are no fine-tunable models, download one from the <code>Models</code> page.</p> <p>Create a fine-tuned model by clicking the \"Fine-Tune Model\" button and choosing the appropriate options:</p> <ul> <li>In the <code>Base Model</code> field, select a preloaded model.</li> <li>In the <code>Training data</code> field, select a preloaded dataset.</li> <li>In the <code>Name</code> field, enter a unique name for your fine-tuned model.</li> <li>In the <code>Description</code> field, provide a description for your fine-tuned model.</li> <li>You can skip the <code>Advanced parameter</code> fields and use the default values.</li> <li>Finally, click <code>Start training</code> to start fine-tuning the model.</li> </ul> <p></p>","tags":["developer center","fine-tuning","tutorial"]},{"location":"tutorials/no-code-fine-tuning-tutorial/#2-observe-the-model-fine-tuning-workload","title":"2. Observe the model fine-tuning workload","text":"<p>Go to the <code>Workloads</code> page to view details of your running AI workloads. In the table, you can see the workload name, the assigned cluster, and the resources required. The <code>Status</code> column displays the last known status of each workload and is not updated automatically. You can use filters to find workloads by name, type, or status.</p> <p></p> <p>Find more details about workloads here.</p>","tags":["developer center","fine-tuning","tutorial"]},{"location":"tutorials/no-code-fine-tuning-tutorial/#3-deploy-the-model-for-inference","title":"3. Deploy the model for inference","text":"<p>Click the <code>Deploy</code> button next to the model you want to make available for inference. Please note that once deployed, a model may take up to 5 minutes before it can begin serving requests.</p> <p></p> <p>Once deployed, navigate to the <code>Chat and Compare</code> page to interact with the model.</p> <p>After verifying your model's performance, you can click the \"View Code\" menu item on the <code>Models</code> page and use the provided code snippet for inference via the API.</p>","tags":["developer center","fine-tuning","tutorial"]},{"location":"tutorials/no-code-fine-tuning-tutorial/#4-chat-with-the-model","title":"4. Chat with the model","text":"<p>On the <code>Chat</code> page, select your fine-tuned model from the drop-down list in the top-right corner.</p> <p>Type a question in the chat to interact with the model.</p> <p></p>","tags":["developer center","fine-tuning","tutorial"]},{"location":"tutorials/no-code-fine-tuning-tutorial/#inspecting-the-debug-output","title":"Inspecting the debug output","text":"<p>After receiving a response from the model, you can inspect the messages sent to the model, the context retrieved as part of RAG, and the tokens used by clicking the \"bug\" icon next to the response.</p> <p></p> <p></p>","tags":["developer center","fine-tuning","tutorial"]},{"location":"tutorials/resource-utilization/","title":"Resource utilization of AI workloads","text":"<p>This guide teaches you how you can observe and manage the amount of resources that your AI workloads are using. Follow these steps to complete find about resource utilization:</p> <ol> <li>Start a new AI workload to consume resources.</li> <li>Observe the amount of resources your AI workload is using by viewing the dashboards in SiloGen AI Resource Manager.</li> <li>Adjust resource quotas as needed.</li> </ol>","tags":["airman","resource management","tutorial"]},{"location":"tutorials/resource-utilization/#1-consume-resources-by-running-ai-workloads","title":"1. Consume resources by running AI workloads","text":"<p>To consume resources, start a new AI workload that uses GPUs. For example, you can run a model fine-tuning workload by following the instructions in \"Model fine-tuning\". Once the workload has started, navigate to the <code>Projects</code> page, select your project, and observe the number of GPUs and the amount of VRAM the workload is using.</p> <p>See the example below:</p> <p></p>","tags":["airman","resource management","tutorial"]},{"location":"tutorials/resource-utilization/#2-observe-the-change-in-resource-utilization","title":"2. Observe the change in resource utilization","text":"<p>Go to the <code>Dashboard</code> and select the \"1 hour\" time period to observe changes in resource consumption. You will see that resource utilization has increased due to the model fine-tuning workload.</p> <p></p> <p>You can view more details about resource utilization on the <code>Project dashboard</code>.</p> <p></p>","tags":["airman","resource management","tutorial"]},{"location":"tutorials/resource-utilization/#3-adjust-the-quota","title":"3. Adjust the quota","text":"<p>If your AI workloads are pending and unable to start, you may need to allocate more resources for them. This can be done by increasing the project quota in the project settings to make more resources available.</p>","tags":["airman","resource management","tutorial"]}]}