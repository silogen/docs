# Bucket storage inputs
bucketStorageHost: http://minio.minio-tenant-default.svc.cluster.local:80

# Bucket credentials from a secret:
bucketCredentialsSecret:
  name: minio-credentials
  accessKeyKey: minio-access-key
  secretKeyKey: minio-secret-key

# Resource configuration:
finetuningGpus: 1 # 1 or 8, values in between are not robust at the moment
storageClass: mlstorage

# Resources:
downloadsReservedSize: 50Gi
checkpointsReservedSize: 50Gi

# Run configuration:
mergeAdapter: false

### Model input and output, required args ###
checkpointsRemote: "default-bucket/experiments/tinyllama-1.1b-chat-v1.0_argilla_alpha"
basemodel: "default-bucket/models/tiny-llama/tinyllama-1.1b-chat-v1.0"

### Finetuning config section ###
finetuning_config:
  method: sft
  data_conf:
    training_data:
      type: CONCATENATION
      datasets:
        - path: "default-bucket/datasets/argilla-mistral-large-human-prompts.jsonl"
    validation_data:
      type: AUTO_SPLIT
      ratio: 0.1
    chat_template_name: "chat-ml"
    missing_pad_token_strategy: "bos-repurpose"
  training_args:
    learning_rate: 0.00005
    max_grad_norm: 7.0
    weight_decay: 0.000001
    optim: "adamw_torch"
    num_train_epochs: 1
    lr_scheduler_type: cosine
    warmup_ratio: 0.01
    logging_strategy: steps
    logging_steps: 0.01
    save_strategy: "best"
    seed: 42
    bf16: true
    report_to:
      - none
    push_to_hub: false
    gradient_checkpointing: false
    eval_steps: 0.2
    eval_strategy: "steps"
    metric_for_best_model: "loss"
    greater_is_better: false
    load_best_model_at_end: true
  batchsize_conf:
    max_per_device_train_batch_size: 1
  peft_conf:
    peft_type: "LORA"
    task_type: "CAUSAL_LM"
    peft_kwargs:
      r: 64
      lora_alpha: 16.0
      lora_dropout: 0.05
      target_modules:
        - q_proj
        - k_proj
        - v_proj
        - o_proj
        - up_proj
        - down_proj
        - gate_proj
  run_conf:
    model_args:
      torch_dtype: bfloat16
      use_cache: false
      attn_implementation: "flash_attention_2"
    resume_from_checkpoint: auto
  sft_args:
    max_seq_length: 2048
