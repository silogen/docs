# Use these to add e.g. kueue labels
#labels:
#  kueue.x-k8s.io/queue-name: kaiwo

# Bucket storage inputs
bucketStorageHost: http://minio.minio-tenant-default.svc.cluster.local:80

# Resource configuration:
finetuningGpus: 8 # 1 or 8, values in between are not robust at the moment
storageClass: mlstorage

# Resources:
downloadsReservedSize: 512Gi
checkpointsReservedSize: 720Gi

# Run configuration:
distributedType: "auto-deepspeed-stage1"
mergeAdapter: false

### Model input and output, required args ###
checkpointsRemote: "default-bucket/experiments/qwen_1.5_odia_7b_chat"
basemodel: "default-bucket/models/OdiaGenAI-LLM/qwen_1.5_odia_7b"

### Finetuning config section ###
finetuning_config:
  method: sft
  data_conf:
    training_data:
      type: CONCATENATION
      datasets:
        - path: "default-bucket/datasets/argilla-mistral-large-human-prompts.jsonl"
        - path: "default-bucket/datasets/dolly-odia-15k-msgfmt.jsonl"
        - path: "default-bucket/datasets/odia-domain-context-train-v1-msgfmt.jsonl"
        - path: "default-bucket/datasets/alpaca-odia-52k-msgfmt.jsonl"
        - path: "default-bucket/datasets/hardcode-odia-qa-105-msgfmt.jsonl"
        - path: "default-bucket/datasets/odiencorp-odia-25k-msgfmt.jsonl"
    validation_data:
      type: AUTO_SPLIT
      ratio: 0.05
    chat_template_name: "chat-ml"
    missing_pad_token_strategy: "bos-repurpose"
  training_args:
    learning_rate: 0.000025
    max_grad_norm: 3.0
    weight_decay: 0.001
    optim: "adamw_torch"
    num_train_epochs: 6
    lr_scheduler_type: cosine
    warmup_ratio: 0.05
    logging_strategy: steps
    logging_steps: 0.01
    save_strategy: "no"
    save_steps: 0.2
    seed: 42
    bf16: true
    report_to:
      - none
    push_to_hub: false
    gradient_checkpointing: false
    eval_steps: 0.2
    eval_strategy: "steps"
    metric_for_best_model: "loss"
    greater_is_better: false
  batchsize_conf:
    total_train_batch_size: 8
    max_per_device_train_batch_size: 1
  peft_conf:
    peft_type: "NO_PEFT"
  run_conf:
    model_args:
      torch_dtype: bfloat16
      use_cache: false
      attn_implementation: "flash_attention_2"
    resume_from_checkpoint: auto
  sft_args:
    max_seq_length: 4096
