# Resources:
downloadsReservedSize: 64Gi
checkpointsReservedSize: 512Gi
finetuningGpus: 1

# Runtime configuration:
distributedType: "auto-single-process"
mergeAdapter: true

### Finetuning config section ###
finetuning_config:
  method: sft
  data_conf:
    training_data:
      type: CONCATENATION
      datasets:
        - path: "PLACEHOLDER"
    validation_data:
      type: AUTO_SPLIT
      ratio: 0.1
    chat_template_name: "simplified-llama31"
    missing_pad_token_strategy: "bos-repurpose"
  training_args:
    learning_rate: 0.00005
    max_grad_norm: 7.0
    weight_decay: 0.000001
    optim: "adamw_torch"
    num_train_epochs: 1
    lr_scheduler_type: cosine
    warmup_ratio: 0.01
    logging_strategy: steps
    logging_steps: 0.01
    save_strategy: steps
    save_steps: 0.1
    seed: 42
    bf16: true
    report_to:
      - none
    push_to_hub: false
    gradient_checkpointing: true
    gradient_checkpointing_kwargs:
      use_reentrant: true
    eval_steps: 0.1
    eval_strategy: "steps"
    metric_for_best_model: "loss"
    greater_is_better: false
    load_best_model_at_end: true
  batchsize_conf:
    max_per_device_train_batch_size: 1
  peft_conf:
    peft_type: "LORA"
    task_type: "CAUSAL_LM"
    peft_kwargs:
      r: 64
      lora_alpha: 16.0
      lora_dropout: 0.05
      target_modules:
        - q_proj
        - k_proj
        - v_proj
        - o_proj
        - up_proj
        - down_proj
        - gate_proj
        - lm_head
        - embed_tokens
  run_conf:
    model_args:
      torch_dtype: bfloat16
      use_cache: false
      attn_implementation: "flash_attention_2"
    resume_from_checkpoint: auto
  sft_args:
    max_seq_length: 2048
