# Resources:
downloadsReservedSize: 512Gi
checkpointsReservedSize: 720Gi
finetuningGpus: 8 # 1 or 8, values in between are not robust at the moment

# Run configuration:
distributedType: "auto-deepspeed-stage1"
mergeAdapter: false

### Finetuning config section ###
finetuning_config:
  method: sft
  data_conf:
    validation_data:
      type: AUTO_SPLIT
      ratio: 0.05
    chat_template_name: "chat-ml"
    missing_pad_token_strategy: "bos-repurpose"
  training_args:
    learning_rate: 0.00005
    max_grad_norm: 2.0
    weight_decay: 0.0
    optim: "adamw_torch"
    num_train_epochs: 6
    lr_scheduler_type: cosine
    warmup_ratio: 0.05
    logging_strategy: steps
    logging_steps: 0.01
    save_strategy: "no"
    save_steps: 0.2
    seed: 42
    bf16: true
    report_to:
      - none
    push_to_hub: false
    gradient_checkpointing: false
    eval_steps: 0.2
    eval_strategy: "steps"
    metric_for_best_model: "loss"
    greater_is_better: false
  batchsize_conf:
    total_train_batch_size:
    max_per_device_train_batch_size: 1
  peft_conf:
    peft_type: "NO_PEFT"
  run_conf:
    model_args:
      torch_dtype: bfloat16
      use_cache: false
      attn_implementation: "flash_attention_2"
    resume_from_checkpoint: auto
  sft_args:
    max_seq_length: 4096
