# Poro-34b

Poro is a 34B parameter decoder-only transformer pretrained on Finnish, English and code. It has been trained on 1 trillion tokens.

Through the combination of English and Finnish training data we get a model that outperforms previous Finnish only models, while also being fluent in English and code, and capable of translation between English and Finnish.

More information can be found here: [LumiOpen/Poro-34B Â· Hugging Face](https://huggingface.co/LumiOpen/Poro-34B).
