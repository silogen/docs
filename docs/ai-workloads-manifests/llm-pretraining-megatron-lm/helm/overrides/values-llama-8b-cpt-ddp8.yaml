### Chart values for Llama-3.1-8B DDP=8 in-cluster storage workload ###
main_workload_image: rocm/megatron-lm:v25.4

remote_base_model_path: default-bucket/megatron-models/Meta/Llama-3.1-8B/
remote_checkpoints_path: default-bucket/experiments/megatron-lm/llama-3.1-8b-cpt-test/
remote_data_dir_path: default-bucket/datasets/fineweb-edu
remote_data_name_prefix: fineweb-edu-train_text_document
remote_tokenizer_path: default-bucket/tokenizers/NousResearch/Meta-Llama-3.1-8B

# Resources of main workload container
resources:
  gpu: 8 # 1 or 8, values in between are not robust at the moment
  cpu: 8
  memory: 300Gi

storage:
  ephemeral:
    quantity: 100Gi
    storageClassName: mlstorage
    accessModes:
      - ReadWriteOnce
  dshm:
    sizeLimit: 32Gi

# Setting job labels
metadata:
  labels: {}

# Logistics (IO) container setup
logistics:
  image: ghcr.io/silogen/logistics:v0.1r
  # Remote storage settings. Configure these to match the credentials in your cluster:
  env_vars:
    BUCKET_STORAGE_HOST: http://minio.minio-tenant-default.svc.cluster.local:80
    BUCKET_STORAGE_ACCESS_KEY:
      name: minio-credentials
      key: minio-access-key
    BUCKET_STORAGE_SECRET_KEY:
      name: minio-credentials
      key: minio-secret-key

pretraining_script: train_cpt.sh

pretraining_settings:
  MOCK_DATA: 0
  SEQ_LENGTH: 2048
  SEQ_PARALLEL: 0
  TP: 1
  PP: 1
  BS: 32
  MBS: 2
  GEMM_TUNING: 0
  TE_FP8: 0
  OPTIMIZER: adam
  TOTAL_ITERS: 20
  SAVE_INTERVAL: 20
  EVAL_INTERVAL: 10
  EVAL_ITERS: 10

# These can be any of the megatron native arguments from the list
# https://github.com/ROCm/Megatron-LM/blob/rocm_dev/megatron/training/arguments.py
megatron_arguments:
  - "--lr 1e-5"
  - "--min-lr 1e-6"
